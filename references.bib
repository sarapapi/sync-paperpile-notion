@ARTICLE{Wang2023-fe,
  title         = "Large-scale {Multi-Modal} Pre-trained Models: A
                   Comprehensive Survey",
  author        = "Wang, Xiao and Chen, Guangyao and Qian, Guangwu and Gao,
                   Pengcheng and Wei, Xiao-Yong and Wang, Yaowei and Tian,
                   Yonghong and Gao, Wen",
  abstract      = "With the urgent demand for generalized deep models, many
                   pre-trained big models are proposed, such as BERT, ViT, GPT,
                   etc. Inspired by the success of these models in single
                   domains (like computer vision and natural language
                   processing), the multi-modal pre-trained big models have
                   also drawn more and more attention in recent years. In this
                   work, we give a comprehensive survey of these models and
                   hope this paper could provide new insights and helps fresh
                   researchers to track the most cutting-edge works.
                   Specifically, we firstly introduce the background of
                   multi-modal pre-training by reviewing the conventional deep
                   learning, pre-training works in natural language process,
                   computer vision, and speech. Then, we introduce the task
                   definition, key challenges, and advantages of multi-modal
                   pre-training models (MM-PTMs), and discuss the MM-PTMs with
                   a focus on data, objectives, network architectures, and
                   knowledge enhanced pre-training. After that, we introduce
                   the downstream tasks used for the validation of large-scale
                   MM-PTMs, including generative, classification, and
                   regression tasks. We also give visualization and analysis of
                   the model parameters and results on representative
                   downstream tasks. Finally, we point out possible research
                   directions for this topic that may benefit future works. In
                   addition, we maintain a continuously updated paper list for
                   large-scale pre-trained multi-modal big models:
                   https://github.com/wangxiao5791509/MultiModal\_BigModels\_Survey",
  month         =  feb,
  year          =  2023,
  url           = "http://arxiv.org/abs/2302.10035",
  archivePrefix = "arXiv",
  eprint        = "2302.10035",
  primaryClass  = "cs.CV",
  arxivid       = "2302.10035"
}

@ARTICLE{Kusupati2022-nx,
  title         = "Matryoshka Representation Learning",
  author        = "Kusupati, Aditya and Bhatt, Gantavya and Rege, Aniket and
                   Wallingford, Matthew and Sinha, Aditya and Ramanujan, Vivek
                   and Howard-Snyder, William and Chen, Kaifeng and Kakade,
                   Sham and Jain, Prateek and Farhadi, Ali",
  abstract      = "Learned representations are a central component in modern ML
                   systems, serving a multitude of downstream tasks. When
                   training such representations, it is often the case that
                   computational and statistical constraints for each
                   downstream task are unknown. In this context rigid, fixed
                   capacity representations can be either over or
                   under-accommodating to the task at hand. This leads us to
                   ask: can we design a flexible representation that can adapt
                   to multiple downstream tasks with varying computational
                   resources? Our main contribution is Matryoshka
                   Representation Learning (MRL) which encodes information at
                   different granularities and allows a single embedding to
                   adapt to the computational constraints of downstream tasks.
                   MRL minimally modifies existing representation learning
                   pipelines and imposes no additional cost during inference
                   and deployment. MRL learns coarse-to-fine representations
                   that are at least as accurate and rich as independently
                   trained low-dimensional representations. The flexibility
                   within the learned Matryoshka Representations offer: (a) up
                   to 14x smaller embedding size for ImageNet-1K classification
                   at the same level of accuracy; (b) up to 14x real-world
                   speed-ups for large-scale retrieval on ImageNet-1K and 4K;
                   and (c) up to 2\% accuracy improvements for long-tail
                   few-shot classification, all while being as robust as the
                   original representations. Finally, we show that MRL extends
                   seamlessly to web-scale datasets (ImageNet, JFT) across
                   various modalities -- vision (ViT, ResNet), vision +
                   language (ALIGN) and language (BERT). MRL code and
                   pretrained models are open-sourced at
                   https://github.com/RAIVNLab/MRL.",
  month         =  may,
  year          =  2022,
  url           = "http://arxiv.org/abs/2205.13147",
  archivePrefix = "arXiv",
  eprint        = "2205.13147",
  primaryClass  = "cs.LG",
  arxivid       = "2205.13147"
}

@MISC{noauthor_undated-ej,
  title        = "Revisiting Feature Prediction for Learning Visual
                  Representations from Video",
  abstract     = "This paper explores feature prediction as a stand-alone
                  objective for unsupervised learning from video and introduces
                  V-JEPA, a collection of vision...",
  url          = "https://ai.meta.com/research/publications/revisiting-feature-prediction-for-learning-visual-representations-from-video/",
  howpublished = "\url{https://ai.meta.com/research/publications/revisiting-feature-prediction-for-learning-visual-representations-from-video/}",
  note         = "Accessed: 2024-2-19",
  language     = "en"
}

@MISC{noauthor_undated-xd,
  title        = "{V-JEPA}: The next step toward advanced machine intelligence",
  booktitle    = "Meta {AI}",
  abstract     = "We're releasing the Video Joint Embedding Predictive
                  Architecture (V-JEPA) model, a crucial step in advancing
                  machine intelligence with a more grounded understanding of
                  the world.",
  url          = "https://ai.meta.com/blog/v-jepa-yann-lecun-ai-model-video-joint-embedding-predictive-architecture/",
  howpublished = "\url{https://ai.meta.com/blog/v-jepa-yann-lecun-ai-model-video-joint-embedding-predictive-architecture/}",
  note         = "Accessed: 2024-2-19",
  keywords     = "Models",
  language     = "en"
}

@ARTICLE{Hassid2024-jy,
  title    = "Textually pretrained speech language models",
  author   = "Hassid, Michael and Remez, Tal and Nguyen, Tu Anh and Gat, Itai
              and Conneau, Alexis and Kreuk, Felix and Copet, Jade and
              Defossez, Alexandre and Synnaeve, Gabriel and Dupoux, Emmanuel
              and {Others}",
  journal  = "Adv. Neural Inf. Process. Syst.",
  volume   =  36,
  year     =  2024,
  url      = "https://proceedings.neurips.cc/paper_files/paper/2023/hash/c859b99b5d717c9035e79d43dfd69435-Abstract-Conference.html",
  keywords = "Models",
  issn     = "1049-5258"
}

@ARTICLE{Ustun2024-tz,
  title         = "Aya Model: An Instruction Finetuned {Open-Access}
                   Multilingual Language Model",
  author        = "{\"U}st{\"u}n, Ahmet and Aryabumi, Viraat and Yong,
                   Zheng-Xin and Ko, Wei-Yin and D'souza, Daniel and Onilude,
                   Gbemileke and Bhandari, Neel and Singh, Shivalika and Ooi,
                   Hui-Lee and Kayid, Amr and Vargus, Freddie and Blunsom, Phil
                   and Longpre, Shayne and Muennighoff, Niklas and Fadaee,
                   Marzieh and Kreutzer, Julia and Hooker, Sara",
  abstract      = "Recent breakthroughs in large language models (LLMs) have
                   centered around a handful of data-rich languages. What does
                   it take to broaden access to breakthroughs beyond
                   first-class citizen languages? Our work introduces Aya, a
                   massively multilingual generative language model that
                   follows instructions in 101 languages of which over 50\% are
                   considered as lower-resourced. Aya outperforms mT0 and
                   BLOOMZ on the majority of tasks while covering double the
                   number of languages. We introduce extensive new evaluation
                   suites that broaden the state-of-art for multilingual eval
                   across 99 languages -- including discriminative and
                   generative tasks, human evaluation, and simulated win rates
                   that cover both held-out tasks and in-distribution
                   performance. Furthermore, we conduct detailed investigations
                   on the optimal finetuning mixture composition, data pruning,
                   as well as the toxicity, bias, and safety of our models. We
                   open-source our instruction datasets and our model at
                   https://hf.co/CohereForAI/aya-101",
  month         =  feb,
  year          =  2024,
  url           = "http://arxiv.org/abs/2402.07827",
  keywords      = "Models",
  archivePrefix = "arXiv",
  eprint        = "2402.07827",
  primaryClass  = "cs.CL",
  arxivid       = "2402.07827"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@MISC{noauthor_undated-ev,
  title        = "{CohereForAI/aya\_collection} Â· Datasets at Hugging Face",
  abstract     = "We're on a journey to advance and democratize artificial
                  intelligence through open source and open science.",
  url          = "https://huggingface.co/datasets/CohereForAI/aya_collection",
  howpublished = "\url{https://huggingface.co/datasets/CohereForAI/aya_collection}",
  note         = "Accessed: 2024-2-21"
}
