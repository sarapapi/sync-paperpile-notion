@MISC{noauthor_undated-ru,
  title        = "{GPTZero}",
  booktitle    = "GPTZero",
  abstract     = "Covered by >100 media outlets, GPTZero is the most advanced AI
                  detector for ChatGPT, GPT-4, Bard. Check up to 50000
                  characters for AI plagiarism in seconds.",
  howpublished = "\url{https://gptzero.me/}",
  note         = "Accessed: 2024-2-28",
  language     = "en"
}

@ARTICLE{Kusupati2022-db,
  title         = "Matryoshka Representation Learning",
  author        = "Kusupati, Aditya and Bhatt, Gantavya and Rege, Aniket and
                   Wallingford, Matthew and Sinha, Aditya and Ramanujan, Vivek
                   and Howard-Snyder, William and Chen, Kaifeng and Kakade, Sham
                   and Jain, Prateek and Farhadi, Ali",
  journal       = "arXiv [cs.LG]",
  abstract      = "Learned representations are a central component in modern ML
                   systems, serving a multitude of downstream tasks. When
                   training such representations, it is often the case that
                   computational and statistical constraints for each downstream
                   task are unknown. In this context rigid, fixed capacity
                   representations can be either over or under-accommodating to
                   the task at hand. This leads us to ask: can we design a
                   flexible representation that can adapt to multiple downstream
                   tasks with varying computational resources? Our main
                   contribution is Matryoshka Representation Learning (MRL)
                   which encodes information at different granularities and
                   allows a single embedding to adapt to the computational
                   constraints of downstream tasks. MRL minimally modifies
                   existing representation learning pipelines and imposes no
                   additional cost during inference and deployment. MRL learns
                   coarse-to-fine representations that are at least as accurate
                   and rich as independently trained low-dimensional
                   representations. The flexibility within the learned
                   Matryoshka Representations offer: (a) up to 14x smaller
                   embedding size for ImageNet-1K classification at the same
                   level of accuracy; (b) up to 14x real-world speed-ups for
                   large-scale retrieval on ImageNet-1K and 4K; and (c) up to
                   2\% accuracy improvements for long-tail few-shot
                   classification, all while being as robust as the original
                   representations. Finally, we show that MRL extends seamlessly
                   to web-scale datasets (ImageNet, JFT) across various
                   modalities -- vision (ViT, ResNet), vision + language (ALIGN)
                   and language (BERT). MRL code and pretrained models are
                   open-sourced at https://github.com/RAIVNLab/MRL.",
  month         =  "26~" # may,
  year          =  2022,
  url           = "http://arxiv.org/abs/2205.13147",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2205.13147",
  keywords      = "WP4"
}

@ARTICLE{Moschella2022-uh,
  title         = "Relative representations enable zero-shot latent space
                   communication",
  author        = "Moschella, Luca and Maiorca, Valentino and Fumero, Marco and
                   Norelli, Antonio and Locatello, Francesco and Rodolà,
                   Emanuele",
  journal       = "arXiv [cs.LG]",
  abstract      = "Neural networks embed the geometric structure of a data
                   manifold lying in a high-dimensional space into latent
                   representations. Ideally, the distribution of the data points
                   in the latent space should depend only on the task, the data,
                   the loss, and other architecture-specific constraints.
                   However, factors such as the random weights initialization,
                   training hyperparameters, or other sources of randomness in
                   the training phase may induce incoherent latent spaces that
                   hinder any form of reuse. Nevertheless, we empirically
                   observe that, under the same data and modeling choices, the
                   angles between the encodings within distinct latent spaces do
                   not change. In this work, we propose the latent similarity
                   between each sample and a fixed set of anchors as an
                   alternative data representation, demonstrating that it can
                   enforce the desired invariances without any additional
                   training. We show how neural architectures can leverage these
                   relative representations to guarantee, in practice,
                   invariance to latent isometries and rescalings, effectively
                   enabling latent space communication: from zero-shot model
                   stitching to latent space comparison between diverse
                   settings. We extensively validate the generalization
                   capability of our approach on different datasets, spanning
                   various modalities (images, text, graphs), tasks (e.g.,
                   classification, reconstruction) and architectures (e.g.,
                   CNNs, GCNs, transformers).",
  month         =  "30~" # sep,
  year          =  2022,
  url           = "http://arxiv.org/abs/2209.15430",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2209.15430"
}

@ARTICLE{Tsiamas2022-id,
  title         = "{SegAugment}: Maximizing the Utility of Speech Translation
                   Data with Segmentation-based Augmentations",
  author        = "Tsiamas, Ioannis and Fonollosa, José A R and Costa-jussà,
                   Marta R",
  journal       = "arXiv [cs.CL]",
  abstract      = "End-to-end Speech Translation is hindered by a lack of
                   available data resources. While most of them are based on
                   documents, a sentence-level version is available, which is
                   however single and static, potentially impeding the
                   usefulness of the data. We propose a new data augmentation
                   strategy, SegAugment, to address this issue by generating
                   multiple alternative sentence-level versions of a dataset.
                   Our method utilizes an Audio Segmentation system, which
                   re-segments the speech of each document with different length
                   constraints, after which we obtain the target text via
                   alignment methods. Experiments demonstrate consistent gains
                   across eight language pairs in MuST-C, with an average
                   increase of 2.5 BLEU points, and up to 5 BLEU for
                   low-resource scenarios in mTEDx. Furthermore, when combined
                   with a strong system, SegAugment establishes new
                   state-of-the-art results in MuST-C. Finally, we show that the
                   proposed method can also successfully augment sentence-level
                   datasets, and that it enables Speech Translation models to
                   close the gap between the manual and automatic segmentation
                   at inference time.",
  month         =  "19~" # dec,
  year          =  2022,
  url           = "http://arxiv.org/abs/2212.09699",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2212.09699",
  keywords      = "Datasets;WP3",
  annote        = "[TAUS - Lisa Vasileva] Introduces data augmentation method
                   that improves Speech-to-Speech translation: automatic method
                   that creates additional sentence-level (synthetic) data
                   through re-segmentation of available parallel speech data."
}

@ARTICLE{Wang2023-gz,
  title         = "Large-scale Multi-Modal Pre-trained Models: A Comprehensive
                   Survey",
  author        = "Wang, Xiao and Chen, Guangyao and Qian, Guangwu and Gao,
                   Pengcheng and Wei, Xiao-Yong and Wang, Yaowei and Tian,
                   Yonghong and Gao, Wen",
  journal       = "arXiv [cs.CV]",
  abstract      = "With the urgent demand for generalized deep models, many
                   pre-trained big models are proposed, such as BERT, ViT, GPT,
                   etc. Inspired by the success of these models in single
                   domains (like computer vision and natural language
                   processing), the multi-modal pre-trained big models have also
                   drawn more and more attention in recent years. In this work,
                   we give a comprehensive survey of these models and hope this
                   paper could provide new insights and helps fresh researchers
                   to track the most cutting-edge works. Specifically, we
                   firstly introduce the background of multi-modal pre-training
                   by reviewing the conventional deep learning, pre-training
                   works in natural language process, computer vision, and
                   speech. Then, we introduce the task definition, key
                   challenges, and advantages of multi-modal pre-training models
                   (MM-PTMs), and discuss the MM-PTMs with a focus on data,
                   objectives, network architectures, and knowledge enhanced
                   pre-training. After that, we introduce the downstream tasks
                   used for the validation of large-scale MM-PTMs, including
                   generative, classification, and regression tasks. We also
                   give visualization and analysis of the model parameters and
                   results on representative downstream tasks. Finally, we point
                   out possible research directions for this topic that may
                   benefit future works. In addition, we maintain a continuously
                   updated paper list for large-scale pre-trained multi-modal
                   big models:
                   https://github.com/wangxiao5791509/MultiModal\_BigModels\_Survey",
  month         =  "20~" # feb,
  year          =  2023,
  url           = "http://arxiv.org/abs/2302.10035",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2302.10035",
  keywords      = "WP3"
}

@INPROCEEDINGS{Barattin2023-dv,
  title     = "Attribute-preserving face dataset anonymization via latent code
               optimization",
  author    = "Barattin, Simone and Tzelepis, Christos and Patras, Ioannis and
               Sebe, Nicu",
  booktitle = "2023 IEEE/CVF Conference on Computer Vision and Pattern
               Recognition (CVPR)",
  publisher = "IEEE",
  abstract  = "This work presents a task-agnostic anonymization procedure that
               directly optimizes the images' latent representation in the
               latent space of a pretrained GAN, ensuring both that the identity
               is of a desired distance away from the original (with an identity
               obfuscation loss), whilst preserving the facial attributes. This
               work addresses the problem of anonymizing the identity of faces
               in a dataset of images, such that the privacy of those depicted
               is not violated, while at the same time the dataset is useful for
               downstream task such as for training machine learning models. To
               the best of our knowledge, we are the first to explicitly address
               this issue and deal with two major drawbacks of the existing
               state-of-the-art approaches, namely that they (i) require the
               costly training of additional, purpose-trained neural networks,
               and/or (ii) fail to retain the facial attributes of the original
               images in the anonymized counterparts, the preservation of which
               is of paramount importance for their use in downstream tasks. We
               accordingly present a task-agnostic anonymization procedure that
               directly optimizes the images' latent representation in the
               latent space of a pretrained GAN. By optimizing the latent codes
               directly, we ensure both that the identity is of a desired
               distance away from the original (with an identity obfuscation
               loss), whilst preserving the facial attributes (using a novel
               feature-matching loss in FaRL's [48] deep feature space). We
               demonstrate through a series of both qualitative and quantitative
               experiments that our method is capable of anonymizing the
               identity of the images whilst-crucially-better-preserving the
               facial attributes. We make the code and the pretrained models
               publicly available at: https://github.com/chi0tzp/FALCO.",
  month     =  jun,
  year      =  2023,
  url       = "https://ieeexplore.ieee.org/document/10203624/",
  keywords  = "WP4",
  doi       = "10.1109/cvpr52729.2023.00773",
  language  = "en",
  annote    = "[ITU - Mustafa İzzet Muştu] They use a pre-trained GAN and
               proposed directly editing the latent space codes of the synthetic
               images to ensure a desired distance from the original identity
               while preserving facial attributes."
}

@ARTICLE{Huang2023-di,
  title         = "{AudioGPT}: Understanding and Generating Speech, Music,
                   Sound, and Talking Head",
  author        = "Huang, Rongjie and Li, Mingze and Yang, Dongchao and Shi,
                   Jiatong and Chang, Xuankai and Ye, Zhenhui and Wu, Yuning and
                   Hong, Zhiqing and Huang, Jiawei and Liu, Jinglin and Ren, Yi
                   and Zhao, Zhou and Watanabe, Shinji",
  journal       = "arXiv [cs.CL]",
  abstract      = "Large language models (LLMs) have exhibited remarkable
                   capabilities across a variety of domains and tasks,
                   challenging our understanding of learning and cognition.
                   Despite the recent success, current LLMs are not capable of
                   processing complex audio information or conducting spoken
                   conversations (like Siri or Alexa). In this work, we propose
                   a multi-modal AI system named AudioGPT, which complements
                   LLMs (i.e., ChatGPT) with 1) foundation models to process
                   complex audio information and solve numerous understanding
                   and generation tasks; and 2) the input/output interface (ASR,
                   TTS) to support spoken dialogue. With an increasing demand to
                   evaluate multi-modal LLMs of human intention understanding
                   and cooperation with foundation models, we outline the
                   principles and processes and test AudioGPT in terms of
                   consistency, capability, and robustness. Experimental results
                   demonstrate the capabilities of AudioGPT in solving AI tasks
                   with speech, music, sound, and talking head understanding and
                   generation in multi-round dialogues, which empower humans to
                   create rich and diverse audio content with unprecedented
                   ease. Our system is publicly available at
                   \url{https://github.com/AIGC-Audio/AudioGPT}.",
  month         =  "25~" # apr,
  year          =  2023,
  url           = "http://arxiv.org/abs/2304.12995",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2304.12995",
  keywords      = "Models;WP3"
}

@MISC{Girdhar2023-yz,
  title         = "{ImageBind}: One Embedding Space To Bind Them All",
  author        = "Girdhar, Rohit and El-Nouby, Alaaeldin and Liu, Zhuang and
                   Singh, Mannat and Alwala, Kalyan Vasudev and Joulin, Armand
                   and Misra, Ishan",
  journal       = "arXiv [id='cs.CV' full\_name='Computer Vision and Pattern
                   Recognition' is\_active=True alt\_name=None in\_archive='cs'
                   is\_general=False description='Covers image processing,
                   computer vision, pattern recognition, and scene
                   understanding. Roughly includes material in ACM Subject
                   Classes I.2.10, I.4, and I.5.']",
  year          =  2023,
  url           = "http://arxiv.org/abs/2305.05665",
  archivePrefix = "arXiv",
  primaryClass  = "id='cs.CV' full\_name='Computer Vision and Pattern
                   Recognition' is\_active=True alt\_name=None in\_archive='cs'
                   is\_general=False description='Covers image processing,
                   computer vision, pattern recognition, and scene
                   understanding. Roughly includes material in ACM Subject
                   Classes I.2.10, I.4, and I.5.'",
  eprint        = "2305.05665",
  keywords      = "WP3",
  annote        = "[KIT - Maike Züfle] This paper introduces a multi-modality
                   encoder for image, video and audio (among other modalities).
                   ImageBind has been used as a shared encoder for multiple
                   modalities for example in NextGPT."
}

@ARTICLE{Zhang2023-dh,
  title         = "{SpeechGPT}: Empowering Large Language Models with Intrinsic
                   Cross-Modal Conversational Abilities",
  author        = "Zhang, Dong and Li, Shimin and Zhang, Xin and Zhan, Jun and
                   Wang, Pengyu and Zhou, Yaqian and Qiu, Xipeng",
  journal       = "arXiv [cs.CL]",
  abstract      = "Multi-modal large language models are regarded as a crucial
                   step towards Artificial General Intelligence (AGI) and have
                   garnered significant interest with the emergence of ChatGPT.
                   However, current speech-language models typically adopt the
                   cascade paradigm, preventing inter-modal knowledge transfer.
                   In this paper, we propose SpeechGPT, a large language model
                   with intrinsic cross-modal conversational abilities, capable
                   of perceiving and generating multi-model content. With
                   discrete speech representations, we first construct
                   SpeechInstruct, a large-scale cross-modal speech instruction
                   dataset. Additionally, we employ a three-stage training
                   strategy that includes modality-adaptation pre-training,
                   cross-modal instruction fine-tuning, and chain-of-modality
                   instruction fine-tuning. The experimental results demonstrate
                   that SpeechGPT has an impressive capacity to follow
                   multi-modal human instructions and highlight the potential of
                   handling multiple modalities with one model. Demos are shown
                   in https://0nutation.github.io/SpeechGPT.github.io/.",
  month         =  "18~" # may,
  year          =  2023,
  url           = "http://arxiv.org/abs/2305.11000",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2305.11000",
  keywords      = "WP3;Models",
  annote        = "[TLT - Stefano Perna] a large language model with intrinsic
                   cross-modal conversational abilities, capable of perceiving
                   and generating multimodal content."
}

@ARTICLE{Hassid2023-uv,
  title         = "Textually Pretrained Speech Language Models",
  author        = "Hassid, Michael and Remez, Tal and Nguyen, Tu Anh and Gat,
                   Itai and Conneau, Alexis and Kreuk, Felix and Copet, Jade and
                   Defossez, Alexandre and Synnaeve, Gabriel and Dupoux,
                   Emmanuel and Schwartz, Roy and Adi, Yossi",
  journal       = "arXiv [cs.CL]",
  abstract      = "Speech language models (SpeechLMs) process and generate
                   acoustic data only, without textual supervision. In this
                   work, we propose TWIST, a method for training SpeechLMs using
                   a warm-start from a pretrained textual language models. We
                   show using both automatic and human evaluations that TWIST
                   outperforms a cold-start SpeechLM across the board. We
                   empirically analyze the effect of different model design
                   choices such as the speech tokenizer, the pretrained textual
                   model, and the dataset size. We find that model and dataset
                   scale both play an important role in constructing
                   better-performing SpeechLMs. Based on our observations, we
                   present the largest (to the best of our knowledge) SpeechLM
                   both in terms of number of parameters and training data. We
                   additionally introduce two spoken versions of the StoryCloze
                   textual benchmark to further improve model evaluation and
                   advance future research in the field. We make speech samples,
                   code and models publicly available:
                   https://pages.cs.huji.ac.il/adiyoss-lab/twist/ .",
  month         =  "22~" # may,
  year          =  2023,
  url           = "http://arxiv.org/abs/2305.13009",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2305.13009",
  keywords      = "WP3"
}

@ARTICLE{Zhang2023-nj,
  title         = "{LLaVAR}: Enhanced Visual Instruction Tuning for Text-Rich
                   Image Understanding",
  author        = "Zhang, Yanzhe and Zhang, Ruiyi and Gu, Jiuxiang and Zhou,
                   Yufan and Lipka, Nedim and Yang, Diyi and Sun, Tong",
  journal       = "arXiv [cs.CV]",
  abstract      = "Instruction tuning unlocks the superior capability of Large
                   Language Models (LLM) to interact with humans. Furthermore,
                   recent instruction-following datasets include images as
                   visual inputs, collecting responses for image-based
                   instructions. However, visual instruction-tuned models cannot
                   comprehend textual details within images well. This work
                   enhances the current visual instruction tuning pipeline with
                   text-rich images (e.g., movie posters, book covers, etc.).
                   Specifically, we first use publicly available OCR tools to
                   collect results on 422K text-rich images from the LAION
                   dataset. Moreover, we prompt text-only GPT-4 with recognized
                   texts and image captions to generate 16K conversations, each
                   containing question-answer pairs for text-rich images. By
                   combining our collected data with previous multi-modal
                   instruction-following data, our model, LLaVAR, substantially
                   improves the LLaVA model's capability on text-based VQA
                   datasets (up to 20\% accuracy improvement) while achieving an
                   accuracy of 91.42\% on ScienceQA. The GPT-4-based
                   instruction-following evaluation also demonstrates the
                   improvement of our model on both natural images and text-rich
                   images. Through qualitative analysis, LLaVAR shows promising
                   interaction (e.g., reasoning, writing, and elaboration)
                   skills with humans based on the latest real-world online
                   content that combines text and images. We make our
                   code/data/models publicly available at
                   https://llavar.github.io/.",
  month         =  "29~" # jun,
  year          =  2023,
  url           = "http://arxiv.org/abs/2306.17107",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2306.17107",
  annote        = "[ KIT - Supriti Sinhamahapatra ] This work enhances the
                   current visual instruction tuning pipeline with text-rich
                   images (e.g., movie posters, book covers, etc."
}

@ARTICLE{Sanchez2023-pd,
  title         = "Stay on topic with Classifier-Free Guidance",
  author        = "Sanchez, Guillaume and Fan, Honglu and Spangher, Alexander
                   and Levi, Elad and Ammanamanchi, Pawan Sasanka and Biderman,
                   Stella",
  journal       = "arXiv [cs.CL]",
  abstract      = "Classifier-Free Guidance (CFG) has recently emerged in
                   text-to-image generation as a lightweight technique to
                   encourage prompt-adherence in generations. In this work, we
                   demonstrate that CFG can be used broadly as an inference-time
                   technique in pure language modeling. We show that CFG (1)
                   improves the performance of Pythia, GPT-2 and LLaMA-family
                   models across an array of tasks: Q\&A, reasoning, code
                   generation, and machine translation, achieving SOTA on
                   LAMBADA with LLaMA-7B over PaLM-540B; (2) brings improvements
                   equivalent to a model with twice the parameter-count; (3) can
                   stack alongside other inference-time methods like
                   Chain-of-Thought and Self-Consistency, yielding further
                   improvements in difficult tasks; (4) can be used to increase
                   the faithfulness and coherence of assistants in challenging
                   form-driven and content-driven prompts: in a human evaluation
                   we show a 75\% preference for GPT4All using CFG over
                   baseline.",
  month         =  "30~" # jun,
  year          =  2023,
  url           = "http://arxiv.org/abs/2306.17806",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2306.17806",
  annote        = "[ KIT - Carlos Mullov ] Might help mitigate some issues in
                   models such as QWEN-Audio where it goes into transcription
                   mode, ignoring the prompt/instruction"
}

@ARTICLE{Von_Neumann2023-ta,
  title         = "{MeetEval}: A toolkit for computation of Word Error Rates for
                   meeting transcription systems",
  author        = "von Neumann, Thilo and Boeddeker, Christoph and Delcroix,
                   Marc and Haeb-Umbach, Reinhold",
  journal       = "arXiv [cs.CL]",
  abstract      = "MeetEval is an open-source toolkit to evaluate all kinds of
                   meeting transcription systems. It provides a unified
                   interface for the computation of commonly used Word Error
                   Rates (WERs), specifically cpWER, ORC-WER and MIMO-WER along
                   other WER definitions. We extend the cpWER computation by a
                   temporal constraint to ensure that only words are identified
                   as correct when the temporal alignment is plausible. This
                   leads to a better quality of the matching of the hypothesis
                   string to the reference string that more closely resembles
                   the actual transcription quality, and a system is penalized
                   if it provides poor time annotations. Since word-level timing
                   information is often not available, we present a way to
                   approximate exact word-level timings from segment-level
                   timings (e.g., a sentence) and show that the approximation
                   leads to a similar WER as a matching with exact word-level
                   annotations. At the same time, the time constraint leads to a
                   speedup of the matching algorithm, which outweighs the
                   additional overhead caused by processing the time stamps.",
  month         =  "21~" # jul,
  year          =  2023,
  url           = "https://github.com/usnistgov/SCTK",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2307.11394",
  keywords      = "WP5",
  annote        = "[KIT - Maike Züfle] MeetEval is an open-source toolkit to
                   evaluate all kinds of meeting transcription systems. It
                   provides a unified interface for the computation of commonly
                   used Word Error Rates (WERs), specifically cpWER, ORC-WER and
                   MIMO-WER along other WER definitions. Since the toolkit is
                   specialized for meetings, it would be really useful for
                   evaluating future versions of SpeechLMM."
}

@ARTICLE{Lian2023-oq,
  title         = "{LLM}-grounded Video Diffusion Models",
  author        = "Lian, Long and Shi, Baifeng and Yala, Adam and Darrell,
                   Trevor and Li, Boyi",
  journal       = "arXiv [cs.CV]",
  abstract      = "Text-conditioned diffusion models have emerged as a promising
                   tool for neural video generation. However, current models
                   still struggle with intricate spatiotemporal prompts and
                   often generate restricted or incorrect motion. To address
                   these limitations, we introduce LLM-grounded Video Diffusion
                   (LVD). Instead of directly generating videos from the text
                   inputs, LVD first leverages a large language model (LLM) to
                   generate dynamic scene layouts based on the text inputs and
                   subsequently uses the generated layouts to guide a diffusion
                   model for video generation. We show that LLMs are able to
                   understand complex spatiotemporal dynamics from text alone
                   and generate layouts that align closely with both the prompts
                   and the object motion patterns typically observed in the real
                   world. We then propose to guide video diffusion models with
                   these layouts by adjusting the attention maps. Our approach
                   is training-free and can be integrated into any video
                   diffusion model that admits classifier guidance. Our results
                   demonstrate that LVD significantly outperforms its base video
                   diffusion model and several strong baseline methods in
                   faithfully generating videos with the desired attributes and
                   motion patterns.",
  month         =  "29~" # sep,
  year          =  2023,
  url           = "http://arxiv.org/abs/2309.17444",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2309.17444",
  keywords      = "WP3",
  annote        = "[KIT - Irem Eyiokur] Two stage approach is proposed for video
                   generation. First LLM generates intermediate representation
                   called dynamic scene layout (DSL), then, these layouts are
                   used to generate videos with pretrained video diffusion
                   model. Published in ICLR 2024."
}

@ARTICLE{Zhu2023-bv,
  title         = "{LanguageBind}: Extending Video-Language Pretraining to
                   {N}-modality by Language-based Semantic Alignment",
  author        = "Zhu, Bin and Lin, Bin and Ning, Munan and Yan, Yang and Cui,
                   Jiaxi and Wang, Hongfa and Pang, Yatian and Jiang, Wenhao and
                   Zhang, Junwu and Li, Zongwei and Zhang, Wancai and Li,
                   Zhifeng and Liu, Wei and Yuan, Li",
  journal       = "arXiv [cs.CV]",
  abstract      = "The video-language (VL) pretraining has achieved remarkable
                   improvement in multiple downstream tasks. However, the
                   current VL pretraining framework is hard to extend to
                   multiple modalities (N modalities, N>=3) beyond vision and
                   language. We thus propose LanguageBind, taking the language
                   as the bind across different modalities because the language
                   modality is well-explored and contains rich semantics.
                   Specifically, we freeze the language encoder acquired by VL
                   pretraining, then train encoders for other modalities with
                   contrastive learning. As a result, all modalities are mapped
                   to a shared feature space, implementing multi-modal semantic
                   alignment. While LanguageBind ensures that we can extend VL
                   modalities to N modalities, we also need a high-quality
                   dataset with alignment data pairs centered on language. We
                   thus propose VIDAL-10M with Video, Infrared, Depth, Audio and
                   their corresponding Language, naming as VIDAL-10M. In our
                   VIDAL-10M, all videos are from short video platforms with
                   complete semantics rather than truncated segments from long
                   videos, and all the video, depth, infrared, and audio
                   modalities are aligned to their textual descriptions.
                   LanguageBind has achieved superior performance on a wide
                   range of 15 benchmarks covering video, audio, depth, and
                   infrared. Moreover, multiple experiments have provided
                   evidence for the effectiveness of LanguageBind in achieving
                   indirect alignment and complementarity among diverse
                   modalities. Code address:
                   https://github.com/PKU-YuanGroup/LanguageBind",
  month         =  "3~" # oct,
  year          =  2023,
  url           = "http://arxiv.org/abs/2310.01852",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2310.01852",
  keywords      = "WP3",
  annote        = "[FBK - Francesco Giuliari] Multiple Modalities encoders
                   aligned to a common Latent space based on the text modality."
}

@INPROCEEDINGS{Tang2024-vi,
  title     = "{SALMONN}: Towards Generic Hearing Abilities for Large Language
               Models",
  author    = "Tang, Changli and Yu, Wenyi and Sun, Guangzhi and Chen, Xianzhao
               and Tan, Tian and Li, Wei and Lu, Lu and Ma, Zejun and Zhang,
               Chao",
  booktitle = "The Twelfth International Conference on Learning Representations",
  abstract  = "Hearing is arguably an essential ability of artificial
               intelligence (AI) agents in the physical world, which refers to
               the perception and understanding of general auditory information
               consisting of at least three types of sounds: speech, audio
               events, and music. In this paper, we propose SALMONN, a speech
               audio language music open neural network, built by integrating a
               pre-trained text-based large language model (LLM) with speech and
               audio encoders into a single multimodal model. SALMONN enables
               the LLM to directly process and understand general audio inputs
               and achieve competitive performances on a number of speech and
               audio tasks used in training, such as automatic speech
               recognition and translation, auditory-information-based question
               answering, emotion recognition, speaker verification, and music
               and audio captioning \textit{etc.} SALMONN also has a diverse set
               of emergent abilities unseen in the training, which includes but
               is not limited to speech translation to untrained languages,
               speech-based slot filling, spoken-query-based question answering,
               audio-based storytelling, and speech audio co-reasoning
               \textit{etc}. The presence of the cross-modal emergent abilities
               is studied, and a novel few-shot activation tuning approach is
               proposed to activate such abilities of SALMONN. To our knowledge,
               SALMONN is the first model of its type and can be regarded as a
               step towards AI with generic hearing abilities. An interactive
               demo of SALMONN is available at
               \texttt{\url{https://github.com/bytedance/SALMONN}}, and the
               training code and model checkpoints will be released upon
               acceptance.",
  year      =  2024,
  url       = "https://openreview.net/forum?id=14rn7HpKVk",
  keywords  = "WP4;Models",
  annote    = "[FBK - Sara Papi] A model combining Speech Foundation Models
               (BEATs and Whisper) and LLM (Vicuna) is proposed. The modality
               matching is realized with a Window-level Q-former. Many
               audio-speech related tasks are performed, including ASR and ST,
               for which examples of templates can be found."
}

@ARTICLE{Mireshghallah2023-aw,
  title         = "Can {LLMs} keep a secret? Testing privacy implications of
                   language models via contextual integrity theory",
  author        = "Mireshghallah, Niloofar and Kim, Hyunwoo and Zhou, Xuhui and
                   Tsvetkov, Yulia and Sap, Maarten and Shokri, Reza and Choi,
                   Yejin",
  journal       = "arXiv [cs.AI]",
  abstract      = "The interactive use of large language models (LLMs) in AI
                   assistants (at work, home, etc.) introduces a new set of
                   inference-time privacy risks: LLMs are fed different types of
                   information from multiple sources in their inputs and are
                   expected to reason about what to share in their outputs, for
                   what purpose and with whom, within a given context. In this
                   work, we draw attention to the highly critical yet overlooked
                   notion of contextual privacy by proposing ConfAIde, a
                   benchmark designed to identify critical weaknesses in the
                   privacy reasoning capabilities of instruction-tuned LLMs. Our
                   experiments show that even the most capable models such as
                   GPT-4 and ChatGPT reveal private information in contexts that
                   humans would not, 39\% and 57\% of the time, respectively.
                   This leakage persists even when we employ privacy-inducing
                   prompts or chain-of-thought reasoning. Our work underscores
                   the immediate need to explore novel inference-time
                   privacy-preserving approaches, based on reasoning and theory
                   of mind.",
  month         =  "27~" # oct,
  year          =  2023,
  url           = "http://arxiv.org/abs/2310.17884",
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "2310.17884",
  keywords      = "WP5",
  annote        = "[KIT - Irem Eyiokur] Published in ICLR 2024. The paper
                   introduces CONFAIDE, a benchmark based on contextual
                   integrity theory, to evaluate how well LLMs handle privacy in
                   interactive settings. Results show that even advanced models
                   like GPT-4 frequently disclose sensitive information
                   inappropriately, revealing significant gaps in their
                   contextual privacy reasoning."
}

@ARTICLE{Gandhi2023-uk,
  title         = "Distil-Whisper: Robust Knowledge Distillation via Large-Scale
                   Pseudo Labelling",
  author        = "Gandhi, Sanchit and von Platen, Patrick and Rush, Alexander M",
  journal       = "arXiv [cs.CL]",
  abstract      = "As the size of pre-trained speech recognition models
                   increases, running these large models in low-latency or
                   resource-constrained environments becomes challenging. In
                   this work, we leverage pseudo-labelling to assemble a
                   large-scale open-source dataset which we use to distill the
                   Whisper model into a smaller variant, called Distil-Whisper.
                   Using a simple word error rate (WER) heuristic, we select
                   only the highest quality pseudo-labels for training. The
                   distilled model is 5.8 times faster with 51\% fewer
                   parameters, while performing to within 1\% WER on
                   out-of-distribution test data in a zero-shot transfer
                   setting. Distil-Whisper maintains the robustness of the
                   Whisper model to difficult acoustic conditions, while being
                   less prone to hallucination errors on long-form audio.
                   Distil-Whisper is designed to be paired with Whisper for
                   speculative decoding, yielding a 2 times speed-up while
                   mathematically ensuring the same outputs as the original
                   model. To facilitate further research in this domain, we make
                   our training code, inference code and models publicly
                   accessible.",
  month         =  "1~" # nov,
  year          =  2023,
  url           = "http://arxiv.org/abs/2311.00430",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2311.00430",
  keywords      = "WP3;WP4"
}

@ARTICLE{Chu2023-xp,
  title         = "Qwen-audio: Advancing universal audio understanding via
                   unified large-scale audio-language models",
  author        = "Chu, Yunfei and Xu, Jin and Zhou, Xiaohuan and Yang, Qian and
                   Zhang, Shiliang and Yan, Zhijie and Zhou, Chang and Zhou,
                   Jingren",
  journal       = "arXiv [eess.AS]",
  abstract      = "Recently, instruction-following audio-language models have
                   received broad attention for audio interaction with humans.
                   However, the absence of pre-trained audio models capable of
                   handling diverse audio types and tasks has hindered progress
                   in this field. Consequently, most existing works have only
                   been able to support a limited range of interaction
                   capabilities. In this paper, we develop the Qwen-Audio model
                   and address this limitation by scaling up audio-language
                   pre-training to cover over 30 tasks and various audio types,
                   such as human speech, natural sounds, music, and songs, to
                   facilitate universal audio understanding abilities. However,
                   directly co-training all tasks and datasets can lead to
                   interference issues, as the textual labels associated with
                   different datasets exhibit considerable variations due to
                   differences in task focus, language, granularity of
                   annotation, and text structure. To overcome the one-to-many
                   interference, we carefully design a multi-task training
                   framework by conditioning on a sequence of hierarchical tags
                   to the decoder for encouraging knowledge sharing and avoiding
                   interference through shared and specified tags respectively.
                   Remarkably, Qwen-Audio achieves impressive performance across
                   diverse benchmark tasks without requiring any task-specific
                   fine-tuning, surpassing its counterparts. Building upon the
                   capabilities of Qwen-Audio, we further develop
                   Qwen-Audio-Chat, which allows for input from various audios
                   and text inputs, enabling multi-turn dialogues and supporting
                   various audio-central scenarios.",
  month         =  "14~" # nov,
  year          =  2023,
  url           = "http://arxiv.org/abs/2311.07919",
  archivePrefix = "arXiv",
  primaryClass  = "eess.AS",
  eprint        = "2311.07919",
  keywords      = "WP4",
  annote        = "[KIT - Enes Ugan] Qwen-Audio: Advancing Universal Audio
                   Understanding via Unified Large-Scale Audio-Language Models
                   introduces an audio-language LLM that unifies diverse audio
                   tasks through a multi-task training framework. A key insight
                   is how they address the challenge of variation in textual
                   labels across datasets by introducing the Speech Recognition
                   with Transcription (SRWT) task, which helps align different
                   label formats. Their approach highlights the importance of
                   multi-task learning for robust generalization and task
                   transfer in audio understanding."
}

@ARTICLE{Han2023-oa,
  title         = "{OneLLM}: One Framework to Align All Modalities with Language",
  author        = "Han, Jiaming and Gong, Kaixiong and Zhang, Yiyuan and Wang,
                   Jiaqi and Zhang, Kaipeng and Lin, Dahua and Qiao, Yu and Gao,
                   Peng and Yue, Xiangyu",
  journal       = "arXiv [cs.CV]",
  abstract      = "Multimodal large language models (MLLMs) have gained
                   significant attention due to their strong multimodal
                   understanding capability. However, existing works rely
                   heavily on modality-specific encoders, which usually differ
                   in architecture and are limited to common modalities. In this
                   paper, we present OneLLM, an MLLM that aligns eight
                   modalities to language using a unified framework. We achieve
                   this through a unified multimodal encoder and a progressive
                   multimodal alignment pipeline. In detail, we first train an
                   image projection module to connect a vision encoder with LLM.
                   Then, we build a universal projection module (UPM) by mixing
                   multiple image projection modules and dynamic routing.
                   Finally, we progressively align more modalities to LLM with
                   the UPM. To fully leverage the potential of OneLLM in
                   following instructions, we also curated a comprehensive
                   multimodal instruction dataset, including 2M items from
                   image, audio, video, point cloud, depth/normal map, IMU and
                   fMRI brain activity. OneLLM is evaluated on 25 diverse
                   benchmarks, encompassing tasks such as multimodal captioning,
                   question answering and reasoning, where it delivers excellent
                   performance. Code, data, model and online demo are available
                   at https://github.com/csuhan/OneLLM",
  month         =  "6~" # dec,
  year          =  2023,
  url           = "http://arxiv.org/abs/2312.03700",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2312.03700",
  keywords      = "WP3;Models"
}

@ARTICLE{Thompson2024-gz,
  title         = "A Shocking Amount of the Web is Machine Translated: Insights
                   from Multi-Way Parallelism",
  author        = "Thompson, Brian and Dhaliwal, Mehak Preet and Frisch, Peter
                   and Domhan, Tobias and Federico, Marcello",
  journal       = "arXiv [cs.CL]",
  abstract      = "We show that content on the web is often translated into many
                   languages, and the low quality of these multi-way
                   translations indicates they were likely created using Machine
                   Translation (MT). Multi-way parallel, machine generated
                   content not only dominates the translations in lower resource
                   languages; it also constitutes a large fraction of the total
                   web content in those languages. We also find evidence of a
                   selection bias in the type of content which is translated
                   into many languages, consistent with low quality English
                   content being translated en masse into many lower resource
                   languages, via MT. Our work raises serious concerns about
                   training models such as multilingual large language models on
                   both monolingual and bilingual data scraped from the web.",
  month         =  "11~" # jan,
  year          =  2024,
  url           = "http://arxiv.org/abs/2401.05749",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2401.05749"
}

@ARTICLE{Bar-Tal2024-mx,
  title         = "Lumiere: A space-time diffusion model for video generation",
  author        = "Bar-Tal, Omer and Chefer, Hila and Tov, Omer and Herrmann,
                   Charles and Paiss, Roni and Zada, Shiran and Ephrat, Ariel
                   and Hur, Junhwa and Li, Yuanzhen and Michaeli, Tomer and
                   Wang, Oliver and Sun, Deqing and Dekel, Tali and Mosseri,
                   Inbar",
  journal       = "arXiv [cs.CV]",
  abstract      = "We introduce Lumiere -- a text-to-video diffusion model
                   designed for synthesizing videos that portray realistic,
                   diverse and coherent motion -- a pivotal challenge in video
                   synthesis. To this end, we introduce a Space-Time U-Net
                   architecture that generates the entire temporal duration of
                   the video at once, through a single pass in the model. This
                   is in contrast to existing video models which synthesize
                   distant keyframes followed by temporal super-resolution -- an
                   approach that inherently makes global temporal consistency
                   difficult to achieve. By deploying both spatial and
                   (importantly) temporal down- and up-sampling and leveraging a
                   pre-trained text-to-image diffusion model, our model learns
                   to directly generate a full-frame-rate, low-resolution video
                   by processing it in multiple space-time scales. We
                   demonstrate state-of-the-art text-to-video generation
                   results, and show that our design easily facilitates a wide
                   range of content creation tasks and video editing
                   applications, including image-to-video, video inpainting, and
                   stylized generation.",
  month         =  "23~" # jan,
  year          =  2024,
  url           = "http://arxiv.org/abs/2401.12945",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2401.12945",
  keywords      = "WP3",
  annote        = "[ITU - Izzet Mustu] They use Space-Time UNet (STUNet) that
                   down- and up-sample the video in both space and time. To
                   avoid temporal boundary artifacts, they use Multidiffusion."
}

@ARTICLE{Wadhawan2024-tf,
  title         = "{ConTextual}: Evaluating Context-Sensitive Text-Rich Visual
                   Reasoning in Large Multimodal Models",
  author        = "Wadhawan, Rohan and Bansal, Hritik and Chang, Kai-Wei and
                   Peng, Nanyun",
  journal       = "arXiv [cs.CV]",
  abstract      = "Recent advancements in AI have led to the development of
                   large multimodal models (LMMs) capable of processing complex
                   tasks involving joint reasoning over text and visual content
                   in the image (e.g., navigating maps in public places). This
                   paper introduces ConTextual, a novel benchmark comprising
                   instructions designed explicitly to evaluate LMMs' ability to
                   perform context-sensitive text-rich visual reasoning.
                   ConTextual emphasizes diverse real-world scenarios (e.g.,
                   time-reading, navigation, shopping and more) demanding a
                   deeper understanding of the interactions between textual and
                   visual elements. Our findings reveal a significant
                   performance gap of 30.8\% between the best-performing LMM,
                   GPT-4V(ision), and human capabilities using human evaluation
                   indicating substantial room for improvement in
                   context-sensitive text-rich visual reasoning. Notably, while
                   GPT-4V excelled in abstract categories like meme and quote
                   interpretation, its overall performance still lagged behind
                   humans. In addition to human evaluations, we also employed
                   automatic evaluation metrics using GPT-4, uncovering similar
                   trends in performance disparities. We also perform a
                   fine-grained evaluation across diverse visual contexts and
                   provide qualitative analysis which provides a robust
                   framework for future advancements in the LMM design.
                   https://con-textual.github.io/",
  month         =  "24~" # jan,
  year          =  2024,
  url           = "http://arxiv.org/abs/2401.13311",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2401.13311",
  keywords      = "WP3",
  annote        = "[KIT - Carlos Mullov*] A new Huggingface leaderboard for
                   multimodal (image/text) reasoning (*added by Sara Papi --
                   FBK)"
}

@ARTICLE{Zhang2024-eq,
  title         = "{MM}-{LLMs}: Recent Advances in {MultiModal} Large Language
                   Models",
  author        = "Zhang, Duzhen and Yu, Yahan and Li, Chenxing and Dong, Jiahua
                   and Su, Dan and Chu, Chenhui and Yu, Dong",
  journal       = "arXiv [cs.CL]",
  abstract      = "In the past year, MultiModal Large Language Models (MM-LLMs)
                   have undergone substantial advancements, augmenting
                   off-the-shelf LLMs to support MM inputs or outputs via
                   cost-effective training strategies. The resulting models not
                   only preserve the inherent reasoning and decision-making
                   capabilities of LLMs but also empower a diverse range of MM
                   tasks. In this paper, we provide a comprehensive survey aimed
                   at facilitating further research of MM-LLMs. Initially, we
                   outline general design formulations for model architecture
                   and training pipeline. Subsequently, we introduce a taxonomy
                   encompassing $122$ MM-LLMs, each characterized by its
                   specific formulations. Furthermore, we review the performance
                   of selected MM-LLMs on mainstream benchmarks and summarize
                   key training recipes to enhance the potency of MM-LLMs.
                   Finally, we explore promising directions for MM-LLMs while
                   concurrently maintaining a real-time tracking website for the
                   latest developments in the field. We hope that this survey
                   contributes to the ongoing advancement of the MM-LLMs domain.",
  month         =  "24~" # jan,
  year          =  2024,
  url           = "http://arxiv.org/abs/2401.13601",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2401.13601",
  keywords      = "WP3"
}

@ARTICLE{Fan2024-vn,
  title         = "{MouSi}: Poly-Visual-Expert Vision-Language Models",
  author        = "Fan, Xiaoran and Ji, Tao and Jiang, Changhao and Li, Shuo and
                   Jin, Senjie and Song, Sirui and Wang, Junke and Hong, Boyang
                   and Chen, Lu and Zheng, Guodong and Zhang, Ming and Huang,
                   Caishuang and Zheng, Rui and Xi, Zhiheng and Zhou, Yuhao and
                   Dou, Shihan and Ye, Junjie and Yan, Hang and Gui, Tao and
                   Zhang, Qi and Qiu, Xipeng and Huang, Xuanjing and Wu, Zuxuan
                   and Jiang, Yu-Gang",
  journal       = "arXiv [cs.CV]",
  abstract      = "Current large vision-language models (VLMs) often encounter
                   challenges such as insufficient capabilities of a single
                   visual component and excessively long visual tokens. These
                   issues can limit the model's effectiveness in accurately
                   interpreting complex visual information and over-lengthy
                   contextual information. Addressing these challenges is
                   crucial for enhancing the performance and applicability of
                   VLMs. This paper proposes the use of ensemble experts
                   technique to synergizes the capabilities of individual visual
                   encoders, including those skilled in image-text matching,
                   OCR, image segmentation, etc. This technique introduces a
                   fusion network to unify the processing of outputs from
                   different visual experts, while bridging the gap between
                   image encoders and pre-trained LLMs. In addition, we explore
                   different positional encoding schemes to alleviate the waste
                   of positional encoding caused by lengthy image feature
                   sequences, effectively addressing the issue of position
                   overflow and length limitations. For instance, in our
                   implementation, this technique significantly reduces the
                   positional occupancy in models like SAM, from a substantial
                   4096 to a more efficient and manageable 64 or even down to 1.
                   Experimental results demonstrate that VLMs with multiple
                   experts exhibit consistently superior performance over
                   isolated visual encoders and mark a significant performance
                   boost as more experts are integrated. We have open-sourced
                   the training code used in this report. All of these resources
                   can be found on our project website.",
  month         =  "30~" # jan,
  year          =  2024,
  url           = "http://arxiv.org/abs/2401.17221",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2401.17221",
  keywords      = "Models;WP3"
}

@ARTICLE{Soldaini2024-pg,
  title         = "Dolma: An open corpus of three trillion tokens for language
                   model pretraining research",
  author        = "Soldaini, Luca and Kinney, Rodney and Bhagia, Akshita and
                   Schwenk, Dustin and Atkinson, David and Authur, Russell and
                   Bogin, Ben and Chandu, Khyathi and Dumas, Jennifer and
                   Elazar, Yanai and Hofmann, Valentin and Jha, Ananya Harsh and
                   Kumar, Sachin and Lucy, Li and Lyu, Xinxi and Lambert, Nathan
                   and Magnusson, Ian and Morrison, Jacob and Muennighoff,
                   Niklas and Naik, Aakanksha and Nam, Crystal and Peters,
                   Matthew E and Ravichander, Abhilasha and Richardson, Kyle and
                   Shen, Zejiang and Strubell, Emma and Subramani, Nishant and
                   Tafjord, Oyvind and Walsh, Pete and Zettlemoyer, Luke and
                   Smith, Noah A and Hajishirzi, Hannaneh and Beltagy, Iz and
                   Groeneveld, Dirk and Dodge, Jesse and Lo, Kyle",
  journal       = "arXiv [cs.CL]",
  abstract      = "Information about pretraining corpora used to train the
                   current best-performing language models is seldom discussed:
                   commercial models rarely detail their data, and even open
                   models are often released without accompanying training data
                   or recipes to reproduce them. As a result, it is challenging
                   to conduct and advance scientific research on language
                   modeling, such as understanding how training data impacts
                   model capabilities and limitations. To facilitate scientific
                   research on language model pretraining, we curate and release
                   Dolma, a three-trillion-token English corpus, built from a
                   diverse mixture of web content, scientific papers, code,
                   public-domain books, social media, and encyclopedic
                   materials. We extensively document Dolma, including its
                   design principles, details about its construction, and a
                   summary of its contents. We present analyses and experimental
                   results on intermediate states of Dolma to share what we have
                   learned about important data curation practices. Finally, we
                   open-source our data curation toolkit to enable reproduction
                   of our work as well as support further research in
                   large-scale data curation.",
  month         =  "31~" # jan,
  year          =  2024,
  url           = "http://arxiv.org/abs/2402.00159",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2402.00159",
  keywords      = "Datasets;WP3",
  annote        = "[FBK - Sara Papi] One of the biggest open corpus for LLM
                   training, used for building OLMO LLM. The paper describes the
                   procedures and carefully provide documented information about
                   the collected textual data."
}

@ARTICLE{Groeneveld2024-id,
  title         = "{OLMo}: Accelerating the Science of Language Models",
  author        = "Groeneveld, Dirk and Beltagy, Iz and Walsh, Pete and Bhagia,
                   Akshita and Kinney, Rodney and Tafjord, Oyvind and Jha,
                   Ananya Harsh and Ivison, Hamish and Magnusson, Ian and Wang,
                   Yizhong and Arora, Shane and Atkinson, David and Authur,
                   Russell and Chandu, Khyathi Raghavi and Cohan, Arman and
                   Dumas, Jennifer and Elazar, Yanai and Gu, Yuling and Hessel,
                   Jack and Khot, Tushar and Merrill, William and Morrison,
                   Jacob and Muennighoff, Niklas and Naik, Aakanksha and Nam,
                   Crystal and Peters, Matthew E and Pyatkin, Valentina and
                   Ravichander, Abhilasha and Schwenk, Dustin and Shah, Saurabh
                   and Smith, Will and Strubell, Emma and Subramani, Nishant and
                   Wortsman, Mitchell and Dasigi, Pradeep and Lambert, Nathan
                   and Richardson, Kyle and Zettlemoyer, Luke and Dodge, Jesse
                   and Lo, Kyle and Soldaini, Luca and Smith, Noah A and
                   Hajishirzi, Hannaneh",
  journal       = "arXiv [cs.CL]",
  abstract      = "Language models (LMs) have become ubiquitous in both NLP
                   research and in commercial product offerings. As their
                   commercial importance has surged, the most powerful models
                   have become closed off, gated behind proprietary interfaces,
                   with important details of their training data, architectures,
                   and development undisclosed. Given the importance of these
                   details in scientifically studying these models, including
                   their biases and potential risks, we believe it is essential
                   for the research community to have access to powerful, truly
                   open LMs. To this end, this technical report details the
                   first release of OLMo, a state-of-the-art, truly Open
                   Language Model and its framework to build and study the
                   science of language modeling. Unlike most prior efforts that
                   have only released model weights and inference code, we
                   release OLMo and the whole framework, including training data
                   and training and evaluation code. We hope this release will
                   empower and strengthen the open research community and
                   inspire a new wave of innovation.",
  month         =  "1~" # feb,
  year          =  2024,
  url           = "http://arxiv.org/abs/2402.00838",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2402.00838",
  keywords      = "WP3;Models"
}

@ARTICLE{Kong2024-yl,
  title         = "Audio Flamingo: A Novel Audio Language Model with Few-Shot
                   Learning and Dialogue Abilities",
  author        = "Kong, Zhifeng and Goel, Arushi and Badlani, Rohan and Ping,
                   Wei and Valle, Rafael and Catanzaro, Bryan",
  journal       = "arXiv [cs.SD]",
  abstract      = "Augmenting large language models (LLMs) to understand audio
                   -- including non-speech sounds and non-verbal speech -- is
                   critically important for diverse real-world applications of
                   LLMs. In this paper, we propose Audio Flamingo, a novel audio
                   language model with 1) strong audio understanding abilities,
                   2) the ability to quickly adapt to unseen tasks via
                   in-context learning and retrieval, and 3) strong multi-turn
                   dialogue abilities. We introduce a series of training
                   techniques, architecture design, and data strategies to
                   enhance our model with these abilities. Extensive evaluations
                   across various audio understanding tasks confirm the efficacy
                   of our method, setting new state-of-the-art benchmarks.",
  month         =  "2~" # feb,
  year          =  2024,
  url           = "http://arxiv.org/abs/2402.01831",
  archivePrefix = "arXiv",
  primaryClass  = "cs.SD",
  eprint        = "2402.01831",
  keywords      = "WP3;Models",
  annote        = "[TLT - Stefano Perna] SOTA Audio understanding results.
                   Approach: data augmentation w. GPT-4 generating multi-turni
                   dialogs + In-context learning based RAG."
}

@ARTICLE{Guo2024-qb,
  title         = "Vision Superalignment: Weak-to-Strong Generalization for
                   Vision Foundation Models",
  author        = "Guo, Jianyuan and Chen, Hanting and Wang, Chengcheng and Han,
                   Kai and Xu, Chang and Wang, Yunhe",
  journal       = "arXiv [cs.CV]",
  abstract      = "Recent advancements in large language models have sparked
                   interest in their extraordinary and near-superhuman
                   capabilities, leading researchers to explore methods for
                   evaluating and optimizing these abilities, which is called
                   superalignment. In this context, our paper delves into the
                   realm of vision foundation models, focusing on the concept of
                   weak-to-strong generalization, which involves using a weaker
                   model to supervise a stronger one, aiming to enhance the
                   latter's capabilities beyond the former's limits. We
                   introduce a novel and adaptively adjustable loss function for
                   weak-to-strong supervision. Our comprehensive experiments
                   span various scenarios, including few-shot learning, transfer
                   learning, noisy label learning, and common knowledge
                   distillation settings. The results are striking: our approach
                   not only exceeds the performance benchmarks set by
                   strong-to-strong generalization but also surpasses the
                   outcomes of fine-tuning strong models with whole datasets.
                   This compelling evidence underscores the significant
                   potential of weak-to-strong generalization, showcasing its
                   capability to substantially elevate the performance of vision
                   foundation models. The code is available at
                   https://github.com/ggjy/vision\_weak\_to\_strong.",
  month         =  "6~" # feb,
  year          =  2024,
  url           = "http://arxiv.org/abs/2402.03749",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2402.03749"
}

@ARTICLE{Nguyen2024-iu,
  title         = "{SpiRit}-{LM}: Interleaved Spoken and Written Language Model",
  author        = "Nguyen, Tu Anh and Muller, Benjamin and Yu, Bokai and
                   Costa-jussa, Marta R and Elbayad, Maha and Popuri, Sravya and
                   Duquenne, Paul-Ambroise and Algayres, Robin and Mavlyutov,
                   Ruslan and Gat, Itai and Synnaeve, Gabriel and Pino, Juan and
                   Sagot, Benoit and Dupoux, Emmanuel",
  journal       = "arXiv [cs.CL]",
  abstract      = "We introduce SPIRIT-LM, a foundation multimodal language
                   model that freely mixes text and speech. Our model is based
                   on a pretrained text language model that we extend to the
                   speech modality by continuously training it on text and
                   speech units. Speech and text sequences are concatenated as a
                   single set of tokens, and trained with a word-level
                   interleaving method using a small automatically-curated
                   speech-text parallel corpus. SPIRIT-LM comes in two versions:
                   a BASE version that uses speech semantic units and an
                   EXPRESSIVE version that models expressivity using pitch and
                   style units in addition to the semantic units. For both
                   versions, the text is encoded with subword BPE tokens. The
                   resulting model displays both the semantic abilities of text
                   models and the expressive abilities of speech models.
                   Additionally, we demonstrate that SPIRIT-LM is able to learn
                   new tasks in a few-shot fashion across modalities (i.e. ASR,
                   TTS, Speech Classification).",
  month         =  "8~" # feb,
  year          =  2024,
  url           = "http://arxiv.org/abs/2402.05755",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2402.05755",
  keywords      = "Models;WP3;WP4"
}

@ARTICLE{Ustun2024-gv,
  title         = "Aya Model: An Instruction Finetuned Open-Access Multilingual
                   Language Model",
  author        = "Üstün, Ahmet and Aryabumi, Viraat and Yong, Zheng-Xin and Ko,
                   Wei-Yin and D'souza, Daniel and Onilude, Gbemileke and
                   Bhandari, Neel and Singh, Shivalika and Ooi, Hui-Lee and
                   Kayid, Amr and Vargus, Freddie and Blunsom, Phil and Longpre,
                   Shayne and Muennighoff, Niklas and Fadaee, Marzieh and
                   Kreutzer, Julia and Hooker, Sara",
  journal       = "arXiv [cs.CL]",
  abstract      = "Recent breakthroughs in large language models (LLMs) have
                   centered around a handful of data-rich languages. What does
                   it take to broaden access to breakthroughs beyond first-class
                   citizen languages? Our work introduces Aya, a massively
                   multilingual generative language model that follows
                   instructions in 101 languages of which over 50\% are
                   considered as lower-resourced. Aya outperforms mT0 and BLOOMZ
                   on the majority of tasks while covering double the number of
                   languages. We introduce extensive new evaluation suites that
                   broaden the state-of-art for multilingual eval across 99
                   languages -- including discriminative and generative tasks,
                   human evaluation, and simulated win rates that cover both
                   held-out tasks and in-distribution performance. Furthermore,
                   we conduct detailed investigations on the optimal finetuning
                   mixture composition, data pruning, as well as the toxicity,
                   bias, and safety of our models. We open-source our
                   instruction datasets and our model at
                   https://hf.co/CohereForAI/aya-101",
  month         =  "12~" # feb,
  year          =  2024,
  url           = "http://arxiv.org/abs/2402.07827",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2402.07827",
  keywords      = "Models;WP3"
}

@ARTICLE{Lajszczak2024-vz,
  title         = "{BASE} {TTS}: Lessons from building a billion-parameter
                   Text-to-Speech model on {100K} hours of data",
  author        = "Łajszczak, Mateusz and Cámbara, Guillermo and Li, Yang and
                   Beyhan, Fatih and van Korlaar, Arent and Yang, Fan and Joly,
                   Arnaud and Martín-Cortinas, Álvaro and Abbas, Ammar and
                   Michalski, Adam and Moinet, Alexis and Karlapati, Sri and
                   Muszyńska, Ewa and Guo, Haohan and Putrycz, Bartosz and
                   Gambino, Soledad López and Yoo, Kayeon and Sokolova, Elena
                   and Drugman, Thomas",
  journal       = "arXiv [cs.LG]",
  abstract      = "We introduce a text-to-speech (TTS) model called BASE TTS,
                   which stands for $\textbf{B}$ig $\textbf{A}$daptive
                   $\textbf{S}$treamable TTS with $\textbf{E}$mergent abilities.
                   BASE TTS is the largest TTS model to-date, trained on 100K
                   hours of public domain speech data, achieving a new
                   state-of-the-art in speech naturalness. It deploys a
                   1-billion-parameter autoregressive Transformer that converts
                   raw texts into discrete codes (``speechcodes'') followed by a
                   convolution-based decoder which converts these speechcodes
                   into waveforms in an incremental, streamable manner. Further,
                   our speechcodes are built using a novel speech tokenization
                   technique that features speaker ID disentanglement and
                   compression with byte-pair encoding. Echoing the
                   widely-reported ``emergent abilities'' of large language
                   models when trained on increasing volume of data, we show
                   that BASE TTS variants built with 10K+ hours and 500M+
                   parameters begin to demonstrate natural prosody on textually
                   complex sentences. We design and share a specialized dataset
                   to measure these emergent abilities for text-to-speech. We
                   showcase state-of-the-art naturalness of BASE TTS by
                   evaluating against baselines that include publicly available
                   large-scale text-to-speech systems: YourTTS, Bark and
                   TortoiseTTS. Audio samples generated by the model can be
                   heard at https://amazon-ltts-paper.com/.",
  month         =  "12~" # feb,
  year          =  2024,
  url           = "http://arxiv.org/abs/2402.08093",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2402.08093",
  keywords      = "Models;WP3"
}

@ARTICLE{Ma2024-fj,
  title         = "An Embarrassingly Simple Approach for {LLM} with Strong {ASR}
                   Capacity",
  author        = "Ma, Ziyang and Yang, Guanrou and Yang, Yifan and Gao, Zhifu
                   and Wang, Jiaming and Du, Zhihao and Yu, Fan and Chen, Qian
                   and Zheng, Siqi and Zhang, Shiliang and Chen, Xie",
  journal       = "arXiv [cs.CL]",
  abstract      = "In this paper, we focus on solving one of the most important
                   tasks in the field of speech processing, i.e., automatic
                   speech recognition (ASR), with speech foundation encoders and
                   large language models (LLM). Recent works have complex
                   designs such as compressing the output temporally for the
                   speech encoder, tackling modal alignment for the projector,
                   and utilizing parameter-efficient fine-tuning for the LLM. We
                   found that delicate designs are not necessary, while an
                   embarrassingly simple composition of off-the-shelf speech
                   encoder, LLM, and the only trainable linear projector is
                   competent for the ASR task. To be more specific, we benchmark
                   and explore various combinations of LLMs and speech encoders,
                   leading to the optimal LLM-based ASR system, which we call
                   SLAM-ASR. The proposed SLAM-ASR provides a clean setup and
                   little task-specific design, where only the linear projector
                   is trained. To the best of our knowledge, SLAM-ASR achieves
                   the best performance on the Librispeech benchmark among
                   LLM-based ASR models and even outperforms the latest
                   LLM-based audio-universal model trained on massive pair data.
                   Finally, we explore the capability emergence of LLM-based ASR
                   in the process of modal alignment. We hope that our study can
                   facilitate the research on extending LLM with cross-modality
                   capacity and shed light on the LLM-based ASR community.",
  month         =  "13~" # feb,
  year          =  2024,
  url           = "http://arxiv.org/abs/2402.08846",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2402.08846",
  keywords      = "Models;WP3",
  annote        = "[KIT - Supriti Sinhamahapatra] In this paper, the authors
                   explore LLM-based ASR systems with a framework, where the
                   only trainable linear projector is used to align the speech
                   encoder and the LLM."
}

@ARTICLE{Zhan2024-aq,
  title         = "{AnyGPT}: Unified Multimodal {LLM} with Discrete Sequence
                   Modeling",
  author        = "Zhan, Jun and Dai, Junqi and Ye, Jiasheng and Zhou, Yunhua
                   and Zhang, Dong and Liu, Zhigeng and Zhang, Xin and Yuan,
                   Ruibin and Zhang, Ge and Li, Linyang and Yan, Hang and Fu,
                   Jie and Gui, Tao and Sun, Tianxiang and Jiang, Yugang and
                   Qiu, Xipeng",
  journal       = "arXiv [cs.CL]",
  abstract      = "We introduce AnyGPT, an any-to-any multimodal language model
                   that utilizes discrete representations for the unified
                   processing of various modalities, including speech, text,
                   images, and music. AnyGPT can be trained stably without any
                   alterations to the current large language model (LLM)
                   architecture or training paradigms. Instead, it relies
                   exclusively on data-level preprocessing, facilitating the
                   seamless integration of new modalities into LLMs, akin to the
                   incorporation of new languages. We build a multimodal
                   text-centric dataset for multimodal alignment pre-training.
                   Utilizing generative models, we synthesize the first
                   large-scale any-to-any multimodal instruction dataset. It
                   consists of 108k samples of multi-turn conversations that
                   intricately interweave various modalities, thus equipping the
                   model to handle arbitrary combinations of multimodal inputs
                   and outputs. Experimental results demonstrate that AnyGPT is
                   capable of facilitating any-to-any multimodal conversation
                   while achieving performance comparable to specialized models
                   across all modalities, proving that discrete representations
                   can effectively and conveniently unify multiple modalities
                   within a language model. Demos are shown in
                   https://junzhan2000.github.io/AnyGPT.github.io/",
  month         =  "19~" # feb,
  year          =  2024,
  url           = "http://arxiv.org/abs/2402.12226",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2402.12226",
  keywords      = "Models;WP3"
}

@ARTICLE{Zhou2024-wp,
  title         = "{TinyLLaVA}: A Framework of Small-scale Large Multimodal
                   Models",
  author        = "Zhou, Baichuan and Hu, Ying and Weng, Xi and Jia, Junlong and
                   Luo, Jie and Liu, Xien and Wu, Ji and Huang, Lei",
  journal       = "arXiv [cs.LG]",
  abstract      = "We present the TinyLLaVA framework that provides a unified
                   perspective in designing and analyzing the small-scale Large
                   Multimodal Models (LMMs). We empirically study the effects of
                   different vision encoders, connection modules, language
                   models, training data and training recipes. Our extensive
                   experiments showed that better quality of data combined with
                   better training recipes, smaller LMMs can consistently
                   achieve on-par performances compared to bigger LMMs. Under
                   our framework, we train a family of small-scale LMMs. Our
                   best model, TinyLLaVA-3.1B, achieves better overall
                   performance against existing 7B models such as LLaVA-1.5 and
                   Qwen-VL. We hope our findings can serve as baselines for
                   future research in terms of data scaling, training setups and
                   model selections. Our model weights and codes will be made
                   public.",
  month         =  "22~" # feb,
  year          =  2024,
  url           = "http://arxiv.org/abs/2402.14289",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2402.14289",
  keywords      = "Models;WP3"
}

@ARTICLE{Maaz2024-vd,
  title         = "{PALO}: A Polyglot Large Multimodal Model for {5B} People",
  author        = "Maaz, Muhammad and Rasheed, Hanoona and Shaker, Abdelrahman
                   and Khan, Salman and Cholakal, Hisham and Anwer, Rao M and
                   Baldwin, Tim and Felsberg, Michael and Khan, Fahad S",
  journal       = "arXiv [cs.CL]",
  abstract      = "In pursuit of more inclusive Vision-Language Models (VLMs),
                   this study introduces a Large Multilingual Multimodal Model
                   called \textsc{Palo}. \textsc{Palo} offers visual reasoning
                   capabilities in 10 major languages, including English,
                   Chinese, Hindi, Spanish, French, Arabic, Bengali, Russian,
                   Urdu, and Japanese, that span a total of $\sim$5B people
                   (65\% of the world population). Our approach involves a
                   semi-automated translation approach to adapt the multimodal
                   instruction dataset from English to the target languages
                   using a fine-tuned Large Language Model, thereby ensuring
                   high linguistic fidelity while allowing scalability due to
                   minimal manual effort. The incorporation of diverse
                   instruction sets helps us boost overall performance across
                   multiple languages especially those that are underrepresented
                   like Hindi, Arabic, Bengali, and Urdu. The resulting models
                   are trained across three scales (1.7B, 7B and 13B parameters)
                   to show the generalization and scalability where we observe
                   substantial improvements compared to strong baselines. We
                   also propose the first multilingual multimodal benchmark for
                   the forthcoming approaches to evaluate their vision-language
                   reasoning capabilities across languages. Code:
                   https://github.com/mbzuai-oryx/PALO.",
  month         =  "22~" # feb,
  year          =  2024,
  url           = "http://arxiv.org/abs/2402.14818",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2402.14818",
  keywords      = "Models;WP3"
}

@ARTICLE{Yeo2024-tp,
  title         = "Where Visual Speech Meets Language: {VSP}-{LLM} Framework for
                   Efficient and Context-Aware Visual Speech Processing",
  author        = "Yeo, Jeong Hun and Han, Seunghee and Kim, Minsu and Ro, Yong
                   Man",
  journal       = "arXiv [cs.CV]",
  abstract      = "In visual speech processing, context modeling capability is
                   one of the most important requirements due to the ambiguous
                   nature of lip movements. For example, homophenes, words that
                   share identical lip movements but produce different sounds,
                   can be distinguished by considering the context. In this
                   paper, we propose a novel framework, namely Visual Speech
                   Processing incorporated with LLMs (VSP-LLM), to maximize the
                   context modeling ability by bringing the overwhelming power
                   of LLMs. Specifically, VSP-LLM is designed to perform
                   multi-tasks of visual speech recognition and translation,
                   where the given instructions control the type of task. The
                   input video is mapped to the input latent space of a LLM by
                   employing a self-supervised visual speech model. Focused on
                   the fact that there is redundant information in input frames,
                   we propose a novel deduplication method that reduces the
                   embedded visual features by employing visual speech units.
                   Through the proposed deduplication and Low Rank Adaptors
                   (LoRA), VSP-LLM can be trained in a computationally efficient
                   manner. In the translation dataset, the MuAViC benchmark, we
                   demonstrate that VSP-LLM can more effectively recognize and
                   translate lip movements with just 15 hours of labeled data,
                   compared to the recent translation model trained with 433
                   hours of labeld data.",
  month         =  "23~" # feb,
  year          =  2024,
  url           = "http://arxiv.org/abs/2402.15151",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2402.15151",
  keywords      = "WP3",
  annote        = "[FBK - Francesco Giuliari] They propose a VSR-LLM, they
                   combine the visual encoder from AV-Hubert and combine it with
                   LLaMa for Visual Speech recognition and Translation. Key
                   points: 1. No performance gained in VSR by using LLM for VSR,
                   compared to direct VSR without LLM.2. Improved performance on
                   multilingual translation compared to non-LLM (I guess due to
                   low data availability for non-english languages)3. Unlike
                   other models, here all the frame tokens are passed to the LLM
                   (instead of the video encoder extracting a single token for
                   all the video)"
}

@ARTICLE{Tian2024-mu,
  title         = "{EMO}: Emote Portrait Alive -- Generating Expressive Portrait
                   Videos with {Audio2Video} Diffusion Model under Weak
                   Conditions",
  author        = "Tian, Linrui and Wang, Qi and Zhang, Bang and Bo, Liefeng",
  journal       = "arXiv [cs.CV]",
  abstract      = "In this work, we tackle the challenge of enhancing the
                   realism and expressiveness in talking head video generation
                   by focusing on the dynamic and nuanced relationship between
                   audio cues and facial movements. We identify the limitations
                   of traditional techniques that often fail to capture the full
                   spectrum of human expressions and the uniqueness of
                   individual facial styles. To address these issues, we propose
                   EMO, a novel framework that utilizes a direct audio-to-video
                   synthesis approach, bypassing the need for intermediate 3D
                   models or facial landmarks. Our method ensures seamless frame
                   transitions and consistent identity preservation throughout
                   the video, resulting in highly expressive and lifelike
                   animations. Experimental results demonsrate that EMO is able
                   to produce not only convincing speaking videos but also
                   singing videos in various styles, significantly outperforming
                   existing state-of-the-art methodologies in terms of
                   expressiveness and realism.",
  month         =  "27~" # feb,
  year          =  2024,
  url           = "http://arxiv.org/abs/2402.17485",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2402.17485",
  keywords      = "Models;WP3",
  annote        = "[TLT - Stefano Perna] Generating Expressive Videos from
                   portrait image + input audio (speaking, singing, ...)"
}

@ARTICLE{Alves2024-fx,
  title         = "Tower: An Open Multilingual Large Language Model for
                   Translation-Related Tasks",
  author        = "Alves, Duarte M and Pombal, José and Guerreiro, Nuno M and
                   Martins, Pedro H and Alves, João and Farajian, Amin and
                   Peters, Ben and Rei, Ricardo and Fernandes, Patrick and
                   Agrawal, Sweta and Colombo, Pierre and de Souza, José G C and
                   Martins, André F T",
  journal       = "arXiv [cs.CL]",
  abstract      = "While general-purpose large language models (LLMs)
                   demonstrate proficiency on multiple tasks within the domain
                   of translation, approaches based on open LLMs are competitive
                   only when specializing on a single task. In this paper, we
                   propose a recipe for tailoring LLMs to multiple tasks present
                   in translation workflows. We perform continued pretraining on
                   a multilingual mixture of monolingual and parallel data,
                   creating TowerBase, followed by finetuning on instructions
                   relevant for translation processes, creating TowerInstruct.
                   Our final model surpasses open alternatives on several tasks
                   relevant to translation workflows and is competitive with
                   general-purpose closed LLMs. To facilitate future research,
                   we release the Tower models, our specialization dataset, an
                   evaluation framework for LLMs focusing on the translation
                   ecosystem, and a collection of model generations, including
                   ours, on our benchmark.",
  month         =  "27~" # feb,
  year          =  2024,
  url           = "http://arxiv.org/abs/2402.17733",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2402.17733",
  keywords      = "WP3;Models"
}

@ARTICLE{Jiawei_Zhao_Zhenyu_Zhang_Beidi_Chen_Zhangyang_Wang_Anima_Anandkumar_Yuandong_Tian2024-pe,
  title    = "{GaLore}: Memory-Efficient {LLM} Training by Gradient Low-Rank
              Projection",
  author   = "{Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima
              Anandkumar, Yuandong Tian}",
  journal  = "arXiv",
  abstract = "Training Large Language Models (LLMs) presents significant memory
              challenges, predominantly due to the growing size of weights and
              optimizer states. Common memory-reduction approaches, such as
              low-rank adaptation (LoRA), add a trainable low-rank matrix to the
              frozen pre-trained weight in each layer, reducing trainable
              parameters and optimizer states. However, such approaches
              typically underperform training with full-rank weights in both
              pre-training and fine-tuning stages since they limit the parameter
              search to a low-rank subspace and alter the training dynamics, and
              further, may require full-rank warm start. In this work, we
              propose Gradient Low-Rank Projection (GaLore), a training strategy
              that allows full-parameter learning but is more memory-efficient
              than common low-rank adaptation methods such as LoRA. Our approach
              reduces memory usage by up to 65.5\% in optimizer states while
              maintaining both efficiency and performance for pre-training on
              LLaMA 1B and 7B architectures with C4 dataset with up to 19.7B
              tokens, and on fine-tuning RoBERTa on GLUE tasks. Our 8-bit GaLore
              further reduces optimizer memory by up to 82.5\% and total
              training memory by 63.3\%, compared to a BF16 baseline. Notably,
              we demonstrate, for the first time, the feasibility of
              pre-training a 7B model on consumer GPUs with 24GB memory (e.g.,
              NVIDIA RTX 4090) without model parallel, checkpointing, or
              offloading strategies.",
  month    =  "5~" # jun,
  year     =  2024,
  url      = "https://arxiv.org/abs/2403.03507",
  eprint   = "2403.03507",
  keywords = "WP4",
  doi      = "10.48550",
  annote   = "[ KIT - Carlos Mullov ] Memory efficient training of LLMs through
              low-rank gradients. Presents an alternative to low-rank parameter
              training (LoRA, ...) but promises to be more memory efficient and
              closer to full fine-tuning. Also, unlike LoRA is suitable for
              pre-training the itself, not just fine-tuning."
}

@ARTICLE{Chen2024-fm,
  title         = "An image is worth 1/2 tokens after layer 2: Plug-and-play
                   inference acceleration for Large Vision-Language Models",
  author        = "Chen, Liang and Zhao, Haozhe and Liu, Tianyu and Bai, Shuai
                   and Lin, Junyang and Zhou, Chang and Chang, Baobao",
  journal       = "arXiv [cs.CV]",
  abstract      = "In this study, we identify the inefficient attention
                   phenomena in Large Vision-Language Models (LVLMs), notably
                   within prominent models like LLaVA-1.5, QwenVL-Chat and
                   Video-LLaVA. We find out that the attention computation over
                   visual tokens is of extreme inefficiency in the deep layers
                   of popular LVLMs, suggesting a need for a sparser approach
                   compared to textual data handling. To this end, we introduce
                   FastV, a versatile plug-and-play method designed to optimize
                   computational efficiency by learning adaptive attention
                   patterns in early layers and pruning visual tokens in
                   subsequent ones. Our evaluations demonstrate FastV's ability
                   to dramatically reduce computational costs (e.g., a 45
                   reduction in FLOPs for LLaVA-1.5-13B) without sacrificing
                   performance in a wide range of image and video understanding
                   tasks. The computational efficiency and performance trade-off
                   of FastV are highly customizable and pareto-efficient. It can
                   compress the FLOPs of a 13B-parameter model to achieve a
                   lower budget than that of a 7B-parameter model, while still
                   maintaining superior performance. We believe FastV has
                   practical values for deployment of LVLMs in edge devices and
                   commercial models. Code is released at
                   https://github.com/pkunlp-icler/FastV.",
  month         =  "11~" # mar,
  year          =  2024,
  url           = "http://arxiv.org/abs/2403.06764",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2403.06764",
  keywords      = "WP3",
  annote        = "[KIT - Irem Eyiokur] Recent work presented @ECCV2024. This
                   study identifies inefficiencies in attention computation over
                   visual tokens in Large Vision-Language Models (LVLMs) like
                   LLaVA-1.5, QwenVL-Chat, and Video-LLaVA, particularly in deep
                   layers, suggesting the need for a sparser approach.The
                   proposed FastV is reducing computational costs significantly
                   (e.g., 45\% FLOP reduction) without sacrificing performance
                   across image and video tasks."
}

@ARTICLE{Kelly2024-da,
  title         = "{VisionGPT}: Vision-Language Understanding Agent Using
                   Generalized Multimodal Framework",
  author        = "Kelly, Chris and Hu, Luhui and Yang, Bang and Tian, Yu and
                   Yang, Deshun and Yang, Cindy and Huang, Zaoshan and Li, Zihao
                   and Hu, Jiayin and Zou, Yuexian",
  journal       = "arXiv [cs.CV]",
  abstract      = "With the emergence of large language models (LLMs) and vision
                   foundation models, how to combine the intelligence and
                   capacity of these open-sourced or API-available models to
                   achieve open-world visual perception remains an open
                   question. In this paper, we introduce VisionGPT to
                   consolidate and automate the integration of state-of-the-art
                   foundation models, thereby facilitating vision-language
                   understanding and the development of vision-oriented AI.
                   VisionGPT builds upon a generalized multimodal framework that
                   distinguishes itself through three key features: (1)
                   utilizing LLMs (e.g., LLaMA-2) as the pivot to break down
                   users' requests into detailed action proposals to call
                   suitable foundation models; (2) integrating multi-source
                   outputs from foundation models automatically and generating
                   comprehensive responses for users; (3) adaptable to a wide
                   range of applications such as text-conditioned image
                   understanding/generation/editing and visual question
                   answering. This paper outlines the architecture and
                   capabilities of VisionGPT, demonstrating its potential to
                   revolutionize the field of computer vision through enhanced
                   efficiency, versatility, and generalization, and performance.
                   Our code and models will be made publicly available.
                   Keywords: VisionGPT, Open-world visual perception,
                   Vision-language understanding, Large language model, and
                   Foundation model",
  month         =  "14~" # mar,
  year          =  2024,
  url           = "http://arxiv.org/abs/2403.09027",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2403.09027",
  keywords      = "WP3",
  annote        = "[KIT - Irem Eyiokur] Introduces a framework to leverage the
                   capabilities of LLMs and vision foundation models for
                   open-world visual perception tasks. Three key features: uses
                   LLM to parse user requests, integrates outputs from multiple
                   foundation models, adaptable to various application"
}

@ARTICLE{Han2024-rh,
  title         = "{XLAVS}-{R}: Cross-Lingual Audio-Visual Speech Representation
                   Learning for Noise-Robust Speech Perception",
  author        = "Han, Hyojung and Anwar, Mohamed and Pino, Juan and Hsu,
                   Wei-Ning and Carpuat, Marine and Shi, Bowen and Wang,
                   Changhan",
  journal       = "arXiv [cs.SD]",
  abstract      = "Speech recognition and translation systems perform poorly on
                   noisy inputs, which are frequent in realistic environments.
                   Augmenting these systems with visual signals has the
                   potential to improve robustness to noise. However,
                   audio-visual (AV) data is only available in limited amounts
                   and for fewer languages than audio-only resources. To address
                   this gap, we present XLAVS-R, a cross-lingual audio-visual
                   speech representation model for noise-robust speech
                   recognition and translation in over 100 languages. It is
                   designed to maximize the benefits of limited multilingual AV
                   pre-training data, by building on top of audio-only
                   multilingual pre-training and simplifying existing
                   pre-training schemes. Extensive evaluation on the MuAViC
                   benchmark shows the strength of XLAVS-R on downstream
                   audio-visual speech recognition and translation tasks, where
                   it outperforms the previous state of the art by up to 18.5\%
                   WER and 4.7 BLEU given noisy AV inputs, and enables strong
                   zero-shot audio-visual ability with audio-only fine-tuning.",
  month         =  "21~" # mar,
  year          =  2024,
  url           = "http://arxiv.org/abs/2403.14402",
  archivePrefix = "arXiv",
  primaryClass  = "cs.SD",
  eprint        = "2403.14402",
  keywords      = "Models;WP3"
}

@ARTICLE{Samuel_Marks_Can_Rager_Eric_J_Michaud_Yonatan_Belinkov_David_Bau_Aaron_Mueller2024-yv,
  title    = "Sparse Feature Circuits: Discovering and Editing Interpretable
              Causal Graphs in Language Models",
  author   = "{Samuel Marks, Can Rager, Eric J. Michaud, Yonatan Belinkov, David
              Bau, Aaron Mueller}",
  journal  = "arXiv",
  abstract = "We introduce methods for discovering and applying sparse feature
              circuits. These are causally implicated subnetworks of
              human-interpretable features for explaining language model
              behaviors. Circuits identified in prior work consist of
              polysemantic and difficult-to-interpret units like attention heads
              or neurons, rendering them unsuitable for many downstream
              applications. In contrast, sparse feature circuits enable detailed
              understanding of unanticipated mechanisms. Because they are based
              on fine-grained units, sparse feature circuits are useful for
              downstream tasks: We introduce SHIFT, where we improve the
              generalization of a classifier by ablating features that a human
              judges to be task-irrelevant. Finally, we demonstrate an entirely
              unsupervised and scalable interpretability pipeline by discovering
              thousands of sparse feature circuits for automatically discovered
              model behaviors.",
  month    =  "28~" # mar,
  year     =  2024,
  url      = "https://arxiv.org/abs/2403.19647",
  eprint   = "2403.19647",
  keywords = "WP5",
  doi      = "10.48550",
  annote   = "[KIT - Carlos Mullov] In Krakau some talk came up about gender
              bias in translation. This paper explores the internal workings of
              language models, i.e. its ``internal circuits''. Section 3
              explores Pythia 70M's internal circuit for subject-verb agreement.
              Section 4 explores debiasing through trimming those feature
              circuits. Seems very interesting. For some more context: this
              paper is also referenced in the new survey paper on ``Inner
              Workings of Transformer-based Language Models'' at
              https://arxiv.org/abs/2405.00208."
}

@ARTICLE{Liu2024-jo,
  title         = "{ST}-{LLM}: Large Language Models are effective temporal
                   learners",
  author        = "Liu, Ruyang and Li, Chen and Tang, Haoran and Ge, Yixiao and
                   Shan, Ying and Li, Ge",
  journal       = "arXiv [cs.CV]",
  abstract      = "Large Language Models (LLMs) have showcased impressive
                   capabilities in text comprehension and generation, prompting
                   research efforts towards video LLMs to facilitate human-AI
                   interaction at the video level. However, how to effectively
                   encode and understand videos in video-based dialogue systems
                   remains to be solved. In this paper, we investigate a
                   straightforward yet unexplored question: Can we feed all
                   spatial-temporal tokens into the LLM, thus delegating the
                   task of video sequence modeling to the LLMs? Surprisingly,
                   this simple approach yields significant improvements in video
                   understanding. Based upon this, we propose ST-LLM, an
                   effective video-LLM baseline with Spatial-Temporal sequence
                   modeling inside LLM. Furthermore, to address the overhead and
                   stability issues introduced by uncompressed video tokens
                   within LLMs, we develop a dynamic masking strategy with
                   tailor-made training objectives. For particularly long
                   videos, we have also designed a global-local input module to
                   balance efficiency and effectiveness. Consequently, we
                   harness LLM for proficient spatial-temporal modeling, while
                   upholding efficiency and stability. Extensive experimental
                   results attest to the effectiveness of our method. Through a
                   more concise model and training pipeline, ST-LLM establishes
                   a new state-of-the-art result on VideoChatGPT-Bench and
                   MVBench. Codes have been available at
                   https://github.com/TencentARC/ST-LLM.",
  month         =  "30~" # mar,
  year          =  2024,
  url           = "http://arxiv.org/abs/2404.00308",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2404.00308",
  keywords      = "WP3",
  annote        = "[KIT - Irem Eyiokur] Recent work presented @ECCV2024. This
                   paper introduces ST-LLM, a novel video-Language Model
                   baseline that handles spatial-temporal sequence modeling
                   within LLMs. To address computational overhead and stability
                   challenges, the authors propose a dynamic masking strategy
                   and a global-local input module to balance efficiency and
                   performance, particularly for long videos."
}

@ARTICLE{Hu2024-ua,
  title         = "{WavLLM}: Towards Robust and Adaptive Speech Large Language
                   Model",
  author        = "Hu, Shujie and Zhou, Long and Liu, Shujie and Chen, Sanyuan
                   and Hao, Hongkun and Pan, Jing and Liu, Xunying and Li, Jinyu
                   and Sivasankaran, Sunit and Liu, Linquan and Wei, Furu",
  journal       = "arXiv [cs.CL]",
  abstract      = "The recent advancements in large language models (LLMs) have
                   revolutionized the field of natural language processing,
                   progressively broadening their scope to multimodal perception
                   and generation. However, effectively integrating listening
                   capabilities into LLMs poses significant challenges,
                   particularly with respect to generalizing across varied
                   contexts and executing complex auditory tasks. In this work,
                   we introduce WavLLM, a robust and adaptive speech large
                   language model with dual encoders, and a prompt-aware LoRA
                   weight adapter, optimized by a two-stage curriculum learning
                   approach. Leveraging dual encoders, we decouple different
                   types of speech information, utilizing a Whisper encoder to
                   process the semantic content of speech, and a WavLM encoder
                   to capture the unique characteristics of the speaker's
                   identity. Within the curriculum learning framework, WavLLM
                   first builds its foundational capabilities by optimizing on
                   mixed elementary single tasks, followed by advanced
                   multi-task training on more complex tasks such as
                   combinations of the elementary tasks. To enhance the
                   flexibility and adherence to different tasks and
                   instructions, a prompt-aware LoRA weight adapter is
                   introduced in the second advanced multi-task training stage.
                   We validate the proposed model on universal speech benchmarks
                   including tasks such as ASR, ST, SV, ER, and also apply it to
                   specialized datasets like Gaokao English listening
                   comprehension set for SQA, and speech Chain-of-Thought (CoT)
                   evaluation set. Experiments demonstrate that the proposed
                   model achieves state-of-the-art performance across a range of
                   speech tasks on the same model size, exhibiting robust
                   generalization capabilities in executing complex tasks using
                   CoT approach. Furthermore, our model successfully completes
                   Gaokao tasks without specialized training. The codes, models,
                   audio, and Gaokao evaluation set can be accessed at
                   \url{aka.ms/wavllm}.",
  month         =  "31~" # mar,
  year          =  2024,
  url           = "http://arxiv.org/abs/2404.00656",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2404.00656",
  keywords      = "WP3;Models",
  annote        = "[TLT - Stefano Perna] Speech Large Language Model (Input:
                   Audio | Text Output: Text). Dual audio encoders (Whisper,
                   WavLM). Multi-task fine-tuning and Online Prompt-aware LoRA
                   adapter."
}

@ARTICLE{Zhang2024-un,
  title         = "{SpeechAlign}: Aligning speech generation to human
                   preferences",
  author        = "Zhang, Dong and Li, Zhaowei and Li, Shimin and Zhang, Xin and
                   Wang, Pengyu and Zhou, Yaqian and Qiu, Xipeng",
  journal       = "arXiv [cs.CL]",
  abstract      = "Speech language models have significantly advanced in
                   generating realistic speech, with neural codec language
                   models standing out. However, the integration of human
                   feedback to align speech outputs to human preferences is
                   often neglected. This paper addresses this gap by first
                   analyzing the distribution gap in codec language models,
                   highlighting how it leads to discrepancies between the
                   training and inference phases, which negatively affects
                   performance. Then we explore leveraging learning from human
                   feedback to bridge the distribution gap. We introduce
                   SpeechAlign, an iterative self-improvement strategy that
                   aligns speech language models to human preferences.
                   SpeechAlign involves constructing a preference codec dataset
                   contrasting golden codec tokens against synthetic tokens,
                   followed by preference optimization to improve the codec
                   language model. This cycle of improvement is carried out
                   iteratively to steadily convert weak models to strong ones.
                   Through both subjective and objective evaluations, we show
                   that SpeechAlign can bridge the distribution gap and
                   facilitating continuous self-improvement of the speech
                   language model. Moreover, SpeechAlign exhibits robust
                   generalization capabilities and works for smaller models.
                   Code and models will be available at
                   https://github.com/0nutation/SpeechGPT.",
  month         =  "8~" # apr,
  year          =  2024,
  url           = "http://arxiv.org/abs/2404.05600",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2404.05600",
  annote        = "[TLT - Francesco Verdini] Paper that explores the
                   distribution gap of speech generated with autoregressive
                   models based on neural codecs and provides approaches to
                   reduce it, improving the quality of the generated speech."
}

@ARTICLE{Zichao_Li_Cihang_Xie_Ekin_Dogus_Cubuk2024-vp,
  title    = "Scaling (Down) {CLIP}: A Comprehensive Analysis of Data,
              Architecture, and Training Strategies",
  author   = "{Zichao Li, Cihang Xie, Ekin Dogus Cubuk}",
  journal  = "arXiv [cs.CV]",
  abstract = "This paper investigates the performance of the Contrastive
              Language-Image Pre-training (CLIP) when scaled down to limited
              computation budgets. We explore CLIP along three dimensions: data,
              architecture, and training strategies. With regards to data, we
              demonstrate the significance of high-quality training data and
              show that a smaller dataset of high-quality data can outperform a
              larger dataset with lower quality. We also examine how model
              performance varies with different dataset sizes, suggesting that
              smaller ViT models are better suited for smaller datasets, while
              larger models perform better on larger datasets with fixed
              compute. Additionally, we provide guidance on when to choose a
              CNN-based architecture or a ViT-based architecture for CLIP
              training. We compare four CLIP training strategies - SLIP, FLIP,
              CLIP, and CLIP+Data Augmentation - and show that the choice of
              training strategy depends on the available compute resource. Our
              analysis reveals that CLIP+Data Augmentation can achieve
              comparable performance to CLIP using only half of the training
              data. This work provides practical insights into how to
              effectively train and deploy CLIP models, making them more
              accessible and affordable for practical use in various
              applications.",
  month    =  "4~" # dec,
  year     =  2024,
  url      = "http://dx.doi.org/10.48550",
  eprint   = "2404.08197",
  keywords = "WP3",
  doi      = "10.48550",
  annote   = "[KIT - Carlos Mullov] This paper explores the scaling down of CLIP
              to fit limited computational budgets. They demonstrate that
              high-quality, smaller datasets often outperform larger ones, and
              that smaller ViT models are optimal for these datasets. Might be
              interesing to combine with the improved contrastive loss from
              ``Sigmoid Loss for Language Image Pre-Training''
              (https://arxiv.org/abs/2303.15343) if we ever need to pre-train
              our own vision encoder?"
}

@ARTICLE{Tedeschi2024-ya,
  title         = "{ALERT}: A Comprehensive Benchmark for Assessing Large
                   Language Models' Safety through Red Teaming",
  author        = "Tedeschi, Simone and Friedrich, Felix and Schramowski,
                   Patrick and Kersting, Kristian and Navigli, Roberto and
                   Nguyen, Huu and Li, Bo",
  journal       = "arXiv [cs.CL]",
  abstract      = "When building Large Language Models (LLMs), it is paramount
                   to bear safety in mind and protect them with guardrails.
                   Indeed, LLMs should never generate content promoting or
                   normalizing harmful, illegal, or unethical behavior that may
                   contribute to harm to individuals or society. This principle
                   applies to both normal and adversarial use. In response, we
                   introduce ALERT, a large-scale benchmark to assess safety
                   based on a novel fine-grained risk taxonomy. It is designed
                   to evaluate the safety of LLMs through red teaming
                   methodologies and consists of more than 45k instructions
                   categorized using our novel taxonomy. By subjecting LLMs to
                   adversarial testing scenarios, ALERT aims to identify
                   vulnerabilities, inform improvements, and enhance the overall
                   safety of the language models. Furthermore, the fine-grained
                   taxonomy enables researchers to perform an in-depth
                   evaluation that also helps one to assess the alignment with
                   various policies. In our experiments, we extensively evaluate
                   10 popular open- and closed-source LLMs and demonstrate that
                   many of them still struggle to attain reasonable levels of
                   safety.",
  month         =  "6~" # apr,
  year          =  2024,
  url           = "http://arxiv.org/abs/2404.08676",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2404.08676",
  keywords      = "Datasets;WP5",
  annote        = "[FBK - Sara Papi] The ALERT benchmark is proposed together
                   with a taxonomy to provide detailed insights about LLMs
                   weaknesses and vulnerabilities as well as inform targeted
                   safety enhancements"
}

@ARTICLE{Yang2024-la,
  title         = "A Large-Scale Evaluation of Speech Foundation Models",
  author        = "Yang, Shu-Wen and Chang, Heng-Jui and Huang, Zili and Liu,
                   Andy T and Lai, Cheng-I and Wu, Haibin and Shi, Jiatong and
                   Chang, Xuankai and Tsai, Hsiang-Sheng and Huang, Wen-Chin and
                   Feng, Tzu-Hsun and Chi, Po-Han and Lin, Yist Y and Chuang,
                   Yung-Sung and Huang, Tzu-Hsien and Tseng, Wei-Cheng and
                   Lakhotia, Kushal and Li, Shang-Wen and Mohamed, Abdelrahman
                   and Watanabe, Shinji and Lee, Hung-Yi",
  journal       = "arXiv [eess.AS]",
  abstract      = "The foundation model paradigm leverages a shared foundation
                   model to achieve state-of-the-art (SOTA) performance for
                   various tasks, requiring minimal downstream-specific modeling
                   and data annotation. This approach has proven crucial in the
                   field of Natural Language Processing (NLP). However, the
                   speech processing community lacks a similar setup to explore
                   the paradigm systematically. In this work, we establish the
                   Speech processing Universal PERformance Benchmark (SUPERB) to
                   study the effectiveness of the paradigm for speech. We
                   propose a unified multi-tasking framework to address speech
                   processing tasks in SUPERB using a frozen foundation model
                   followed by task-specialized, lightweight prediction heads.
                   Combining our results with community submissions, we verify
                   that the foundation model paradigm is promising for speech,
                   and our multi-tasking framework is simple yet effective, as
                   the best-performing foundation model shows competitive
                   generalizability across most SUPERB tasks. For
                   reproducibility and extensibility, we have developed a
                   long-term maintained platform that enables deterministic
                   benchmarking, allows for result sharing via an online
                   leaderboard, and promotes collaboration through a
                   community-driven benchmark database to support new
                   development cycles. Finally, we conduct a series of analyses
                   to offer an in-depth understanding of SUPERB and speech
                   foundation models, including information flows across tasks
                   inside the models, the correctness of the weighted-sum
                   benchmarking protocol and the statistical significance and
                   robustness of the benchmark.",
  month         =  "15~" # apr,
  year          =  2024,
  url           = "http://arxiv.org/abs/2404.09385",
  archivePrefix = "arXiv",
  primaryClass  = "eess.AS",
  eprint        = "2404.09385",
  keywords      = "WP3;Datasets;WP4",
  annote        = "[FBK - Sara Papi] The SUPERB benchmark is proposed to analyze
                   the performance of Speech Foundation Models including ASR and
                   ST tasks."
}

@ARTICLE{Xu2024-wc,
  title         = "{VASA}-1: Lifelike Audio-Driven Talking Faces Generated in
                   Real Time",
  author        = "Xu, Sicheng and Chen, Guojun and Guo, Yu-Xiao and Yang,
                   Jiaolong and Li, Chong and Zang, Zhenyu and Zhang, Yizhong
                   and Tong, Xin and Guo, Baining",
  journal       = "arXiv [cs.CV]",
  abstract      = "We introduce VASA, a framework for generating lifelike
                   talking faces with appealing visual affective skills (VAS)
                   given a single static image and a speech audio clip. Our
                   premiere model, VASA-1, is capable of not only producing lip
                   movements that are exquisitely synchronized with the audio,
                   but also capturing a large spectrum of facial nuances and
                   natural head motions that contribute to the perception of
                   authenticity and liveliness. The core innovations include a
                   holistic facial dynamics and head movement generation model
                   that works in a face latent space, and the development of
                   such an expressive and disentangled face latent space using
                   videos. Through extensive experiments including evaluation on
                   a set of new metrics, we show that our method significantly
                   outperforms previous methods along various dimensions
                   comprehensively. Our method not only delivers high video
                   quality with realistic facial and head dynamics but also
                   supports the online generation of 512x512 videos at up to 40
                   FPS with negligible starting latency. It paves the way for
                   real-time engagements with lifelike avatars that emulate
                   human conversational behaviors.",
  month         =  "16~" # apr,
  year          =  2024,
  url           = "http://arxiv.org/abs/2404.10667",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2404.10667",
  keywords      = "WP3;Models",
  annote        = "[TLT - Stefano Perna] TL;DR: single portrait photo + speech
                   audio = hyper-realistic talking face video with precise
                   lip-audio sync, lifelike facial behavior, and naturalistic
                   head movements, generated in real time."
}

@ARTICLE{Denisov2024-vc,
  title         = "Teaching a Multilingual Large Language Model to Understand
                   Multilingual Speech via Multi-Instructional Training",
  author        = "Denisov, Pavel and Vu, Ngoc Thang",
  journal       = "arXiv [cs.CL]",
  abstract      = "Recent advancements in language modeling have led to the
                   emergence of Large Language Models (LLMs) capable of various
                   natural language processing tasks. Despite their success in
                   text-based tasks, applying LLMs to the speech domain remains
                   limited and challenging. This paper presents BLOOMZMMS, a
                   novel model that integrates a multilingual LLM with a
                   multilingual speech encoder, aiming to harness the
                   capabilities of LLMs for speech recognition and beyond.
                   Utilizing a multi-instructional training approach, we
                   demonstrate the transferability of linguistic knowledge from
                   the text to the speech modality. Our experiments, conducted
                   on 1900 hours of transcribed data from 139 languages,
                   establish that a multilingual speech representation can be
                   effectively learned and aligned with a multilingual LLM.
                   While this learned representation initially shows limitations
                   in task generalization, we address this issue by generating
                   synthetic targets in a multi-instructional style. Our
                   zero-shot evaluation results confirm the robustness of our
                   approach across multiple tasks, including speech translation
                   and multilingual spoken language understanding, thereby
                   opening new avenues for applying LLMs in the speech domain.",
  month         =  "16~" # apr,
  year          =  2024,
  url           = "http://arxiv.org/abs/2404.10922",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2404.10922",
  keywords      = "WP3;Models",
  annote        = "[FBK - Sara Papi] BLOOMZMMS is presented, a novel model that
                   integrates a multilingual LLM with a multilingual speech
                   encoder, aiming to harness the capabilities of LLMs for
                   speech recognition and beyond, and demonstrates the
                   transferability of linguistic knowledge from the text to the
                   speech modality."
}

@ARTICLE{Ormazabal2024-xe,
  title         = "Reka Core, Flash, and Edge: A Series of Powerful Multimodal
                   Language Models",
  author        = "Ormazabal, Aitor and Zheng, Che and de Masson d'Autume,
                   Cyprien and Yogatama, Dani and Fu, Deyu and Ong, Donovan and
                   Chen, Eric and Lamprecht, Eugenie and Pham, Hai and Ong,
                   Isaac and Aleksiev, Kaloyan and Li, Lei and Henderson,
                   Matthew and Bain, Max and Artetxe, Mikel and Relan, Nishant
                   and Padlewski, Piotr and Liu, Qi and Chen, Ren and Phua,
                   Samuel and Yang, Yazheng and Tay, Yi and Wang, Yuqi and Zhu,
                   Zhongkai and Xie, Zhihui",
  journal       = "arXiv [cs.CL]",
  abstract      = "We introduce Reka Core, Flash, and Edge, a series of powerful
                   multimodal language models trained from scratch by Reka. Reka
                   models are able to process and reason with text, images,
                   video, and audio inputs. This technical report discusses
                   details of training some of these models and provides
                   comprehensive evaluation results. We show that Reka Edge and
                   Reka Flash are not only state-of-the-art but also outperform
                   many much larger models, delivering outsized values for their
                   respective compute class. Meanwhile, our most capable and
                   largest model, Reka Core, approaches the best frontier models
                   on both automatic evaluations and blind human evaluations. On
                   image question answering benchmarks (e.g. MMMU, VQAv2), Core
                   performs competitively to GPT4-V. Meanwhile, on multimodal
                   chat, Core ranks as the second most preferred model under a
                   blind third-party human evaluation setup, outperforming other
                   models such as Claude 3 Opus. On text benchmarks, Core not
                   only performs competitively to other frontier models on a set
                   of well-established benchmarks (e.g. MMLU, GSM8K) but also
                   outperforms GPT4-0613 on human evaluation. On video question
                   answering (Perception-Test), Core outperforms Gemini Ultra.
                   Models are shipped in production at http://chat.reka.ai . A
                   showcase of non cherry picked qualitative examples can also
                   be found at http://showcase.reka.ai .",
  month         =  "18~" # apr,
  year          =  2024,
  url           = "http://arxiv.org/abs/2404.12387",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2404.12387",
  keywords      = "WP3;Models",
  annote        = "[FBK - Sara Papi] Multimodal models capable of performing
                   several tasks, with interesting results compared to GPT. Only
                   API (under payment) and a chat version (Playground style) are
                   available."
}

@ARTICLE{Wallace2024-dv,
  title         = "The Instruction Hierarchy: Training {LLMs} to Prioritize
                   Privileged Instructions",
  author        = "Wallace, Eric and Xiao, Kai and Leike, Reimar and Weng,
                   Lilian and Heidecke, Johannes and Beutel, Alex",
  journal       = "arXiv [cs.CR]",
  abstract      = "Today's LLMs are susceptible to prompt injections,
                   jailbreaks, and other attacks that allow adversaries to
                   overwrite a model's original instructions with their own
                   malicious prompts. In this work, we argue that one of the
                   primary vulnerabilities underlying these attacks is that LLMs
                   often consider system prompts (e.g., text from an application
                   developer) to be the same priority as text from untrusted
                   users and third parties. To address this, we propose an
                   instruction hierarchy that explicitly defines how models
                   should behave when instructions of different priorities
                   conflict. We then propose a data generation method to
                   demonstrate this hierarchical instruction following behavior,
                   which teaches LLMs to selectively ignore lower-privileged
                   instructions. We apply this method to GPT-3.5, showing that
                   it drastically increases robustness -- even for attack types
                   not seen during training -- while imposing minimal
                   degradations on standard capabilities.",
  month         =  "19~" # apr,
  year          =  2024,
  url           = "http://arxiv.org/abs/2404.13208",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CR",
  eprint        = "2404.13208",
  keywords      = "WP5",
  annote        = "[FBK - Sara Papi] OpenAI published a paper introducing the
                   instruction hierarchy which defines the model behavior upon
                   confronting conflicting instructions. The method has several
                   implications in LLM security scenarios such as preventing
                   prompt injections, jailbreaks and other attacks."
}

@ARTICLE{Abdin2024-el,
  title         = "Phi-3 Technical Report: A Highly Capable Language Model
                   Locally on Your Phone",
  author        = "Abdin, Marah and Jacobs, Sam Ade and Awan, Ammar Ahmad and
                   Aneja, Jyoti and Awadallah, Ahmed and Awadalla, Hany and
                   Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Behl,
                   Harkirat and Benhaim, Alon and Bilenko, Misha and Bjorck,
                   Johan and Bubeck, Sébastien and Cai, Martin and Mendes, Caio
                   César Teodoro and Chen, Weizhu and Chaudhary, Vishrav and
                   Chopra, Parul and Del Giorno, Allie and de Rosa, Gustavo and
                   Dixon, Matthew and Eldan, Ronen and Iter, Dan and Garg, Amit
                   and Goswami, Abhishek and Gunasekar, Suriya and Haider, Emman
                   and Hao, Junheng and Hewett, Russell J and Huynh, Jamie and
                   Javaheripi, Mojan and Jin, Xin and Kauffmann, Piero and
                   Karampatziakis, Nikos and Kim, Dongwoo and Khademi, Mahoud
                   and Kurilenko, Lev and Lee, James R and Lee, Yin Tat and Li,
                   Yuanzhi and Liang, Chen and Liu, Weishung and Lin, Eric and
                   Lin, Zeqi and Madan, Piyush and Mitra, Arindam and Modi,
                   Hardik and Nguyen, Anh and Norick, Brandon and Patra, Barun
                   and Perez-Becker, Daniel and Portet, Thomas and Pryzant, Reid
                   and Qin, Heyang and Radmilac, Marko and Rosset, Corby and
                   Roy, Sambudha and Ruwase, Olatunji and Saarikivi, Olli and
                   Saied, Amin and Salim, Adil and Santacroce, Michael and Shah,
                   Shital and Shang, Ning and Sharma, Hiteshi and Song, Xia and
                   Tanaka, Masahiro and Wang, Xin and Ward, Rachel and Wang,
                   Guanhua and Witte, Philipp and Wyatt, Michael and Xu, Can and
                   Xu, Jiahang and Yadav, Sonali and Yang, Fan and Yang, Ziyi
                   and Yu, Donghan and Zhang, Chengruidong and Zhang, Cyril and
                   Zhang, Jianwen and Zhang, Li Lyna and Zhang, Yi and Zhang,
                   Yue and Zhang, Yunan and Zhou, Xiren",
  journal       = "arXiv [cs.CL]",
  abstract      = "We introduce phi-3-mini, a 3.8 billion parameter language
                   model trained on 3.3 trillion tokens, whose overall
                   performance, as measured by both academic benchmarks and
                   internal testing, rivals that of models such as Mixtral 8x7B
                   and GPT-3.5 (e.g., phi-3-mini achieves 69\% on MMLU and 8.38
                   on MT-bench), despite being small enough to be deployed on a
                   phone. The innovation lies entirely in our dataset for
                   training, a scaled-up version of the one used for phi-2,
                   composed of heavily filtered web data and synthetic data. The
                   model is also further aligned for robustness, safety, and
                   chat format. We also provide some initial parameter-scaling
                   results with a 7B and 14B models trained for 4.8T tokens,
                   called phi-3-small and phi-3-medium, both significantly more
                   capable than phi-3-mini (e.g., respectively 75\% and 78\% on
                   MMLU, and 8.7 and 8.9 on MT-bench).",
  month         =  "22~" # apr,
  year          =  2024,
  url           = "http://arxiv.org/abs/2404.14219",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2404.14219",
  keywords      = "Models;WP3",
  annote        = "[FBK - Sara Papi] a new 3.8B parameter language model called
                   phi-3-mini trained on 3.3 trillion tokens and is reported to
                   rival Mixtral 8x7B and GPT-3.5; has a default context length
                   of 4K but also includes a version that is extended to 128K
                   (phi-mini-128K); combines heavily filtered web data and
                   synthetic data to train the 3.8B models; it also reports
                   results on 7B and 14B models trained on 4.8T tokens
                   (phi-3-small and phi-3-medium)"
}

@ARTICLE{Elhoushi2024-ee,
  title         = "{LayerSkip}: Enabling Early Exit Inference and
                   Self-Speculative Decoding",
  author        = "Elhoushi, Mostafa and Shrivastava, Akshat and Liskovich,
                   Diana and Hosmer, Basil and Wasti, Bram and Lai, Liangzhen
                   and Mahmoud, Anas and Acun, Bilge and Agarwal, Saurabh and
                   Roman, Ahmed and Aly, Ahmed A and Chen, Beidi and Wu,
                   Carole-Jean",
  journal       = "arXiv [cs.CL]",
  abstract      = "We present LayerSkip, an end-to-end solution to speed-up
                   inference of large language models (LLMs). First, during
                   training we apply layer dropout, with low dropout rates for
                   earlier layers and higher dropout rates for later layers, and
                   an early exit loss where all transformer layers share the
                   same exit. Second, during inference, we show that this
                   training recipe increases the accuracy of early exit at
                   earlier layers, without adding any auxiliary layers or
                   modules to the model. Third, we present a novel
                   self-speculative decoding solution where we exit at early
                   layers and verify and correct with remaining layers of the
                   model. Our proposed self-speculative decoding approach has
                   less memory footprint than other speculative decoding
                   approaches and benefits from shared compute and activations
                   of the draft and verification stages. We run experiments on
                   different Llama model sizes on different types of training:
                   pretraining from scratch, continual pretraining, finetuning
                   on specific data domain, and finetuning on specific task. We
                   implement our inference solution and show speedups of up to
                   2.16x on summarization for CNN/DM documents, 1.82x on coding,
                   and 2.0x on TOPv2 semantic parsing task.",
  month         =  "25~" # apr,
  year          =  2024,
  url           = "http://arxiv.org/abs/2404.16710",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2404.16710",
  keywords      = "WP3;WP4",
  annote        = "[FBK - Sara Papi] LayerSkip is an end-to-end solution
                   designed to accelerate inference of large language models
                   (LLMs). During training, they apply layer dropout and an
                   early exit loss. In inference, they improve early exit
                   accuracy without additional layers. They propose a
                   self-speculative decoding method, which reduces memory
                   footprint and benefits from shared compute and activations.
                   Experiments show speedups of up to 2.16x for summarization,
                   1.82x for coding, and 2.0x for semantic parsing."
}

@ARTICLE{Xu2024-ue,
  title         = "{PLLaVA} : Parameter-free {LLaVA} Extension from Images to
                   Videos for Video Dense Captioning",
  author        = "Xu, Lin and Zhao, Yilin and Zhou, Daquan and Lin, Zhijie and
                   Ng, See Kiong and Feng, Jiashi",
  journal       = "arXiv [cs.CV]",
  abstract      = "Vision-language pre-training has significantly elevated
                   performance across a wide range of image-language
                   applications. Yet, the pre-training process for video-related
                   tasks demands exceptionally large computational and data
                   resources, which hinders the progress of video-language
                   models. This paper investigates a straight-forward, highly
                   efficient, and resource-light approach to adapting an
                   existing image-language pre-trained model for dense video
                   understanding. Our preliminary experiments reveal that
                   directly fine-tuning pre-trained image-language models with
                   multiple frames as inputs on video datasets leads to
                   performance saturation or even a drop. Our further
                   investigation reveals that it is largely attributed to the
                   bias of learned high-norm visual features. Motivated by this
                   finding, we propose a simple but effective pooling strategy
                   to smooth the feature distribution along the temporal
                   dimension and thus reduce the dominant impacts from the
                   extreme features. The new model is termed Pooling LLaVA, or
                   PLLaVA in short. PLLaVA achieves new state-of-the-art
                   performance on modern benchmark datasets for both video
                   question-answer and captioning tasks. Notably, on the recent
                   popular VideoChatGPT benchmark, PLLaVA achieves a score of
                   3.48 out of 5 on average of five evaluated dimensions,
                   exceeding the previous SOTA results from GPT4V (IG-VLM) by
                   9\%. On the latest multi-choice benchmark MVBench, PLLaVA
                   achieves 58.1\% accuracy on average across 20 sub-tasks,
                   14.5\% higher than GPT4V (IG-VLM). Code is available at
                   https://pllava.github.io/",
  month         =  "25~" # apr,
  year          =  2024,
  url           = "http://arxiv.org/abs/2404.16994",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2404.16994",
  keywords      = "WP3;Models",
  annote        = "[KIT - Irem Eyiokur] In this paper PLLaVA (Pooling LLaVA), a
                   novel pooling strategy is proposed to adapt image-language
                   pretrained models to video understanding tasks such as video
                   captioning and VQA. They claim that finetuning of these
                   pretrained models with videos directly cause performance
                   saturation and even drop."
}

@ARTICLE{Andrew_Melnik_Michal_Ljubljanac_Cong_Lu_Qi_Yan_Weiming_Ren_Helge_Ritter2024-xt,
  title    = "Video Diffusion Models: A Survey",
  author   = "{Andrew Melnik, Michal Ljubljanac, Cong Lu, Qi Yan, Weiming Ren,
              Helge Ritter}",
  journal  = "arXiv:cs.CV",
  abstract = "Diffusion generative models have recently become a robust
              technique for producing and modifying coherent, high-quality
              video. This survey offers a systematic overview of critical
              elements of diffusion models for video generation, covering
              applications, architectural choices, and the modeling of temporal
              dynamics. Recent advancements in the field are summarized and
              grouped into development trends. The survey concludes with an
              overview of remaining challenges and an outlook on the future of
              the field. Website: this https URL",
  month    =  "6~" # may,
  year     =  2024,
  url      = "http://dx.doi.org/10.48550",
  eprint   = "2405.03150",
  keywords = "WP3",
  doi      = "10.48550",
  annote   = "[KIT - Carlos Mullov] A recent survey paper video diffusion
              models. It goes into video generation (unconditioned,
              text-conditionred, image-conditioned), video completion,
              audio-conditioning, and lastly video editing (i.e. video-to-video
              generation). Also goes into available datasets for training and
              evaluation and into evaluation metrics."
}

@ARTICLE{Yaman2024-vi,
  title         = "Audio-Visual Speech Representation Expert for Enhanced
                   Talking Face Video Generation and Evaluation",
  author        = "Yaman, Dogucan and Eyiokur, Fevziye Irem and Bärmann, Leonard
                   and Aktı, Seymanur and Ekenel, Hazım Kemal and Waibel,
                   Alexander",
  journal       = "arXiv [cs.CV]",
  abstract      = "In the task of talking face generation, the objective is to
                   generate a face video with lips synchronized to the
                   corresponding audio while preserving visual details and
                   identity information. Current methods face the challenge of
                   learning accurate lip synchronization while avoiding
                   detrimental effects on visual quality, as well as robustly
                   evaluating such synchronization. To tackle these problems, we
                   propose utilizing an audio-visual speech representation
                   expert (AV-HuBERT) for calculating lip synchronization loss
                   during training. Moreover, leveraging AV-HuBERT's features,
                   we introduce three novel lip synchronization evaluation
                   metrics, aiming to provide a comprehensive assessment of lip
                   synchronization performance. Experimental results, along with
                   a detailed ablation study, demonstrate the effectiveness of
                   our approach and the utility of the proposed evaluation
                   metrics.",
  month         =  "7~" # may,
  year          =  2024,
  url           = "http://arxiv.org/abs/2405.04327",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2405.04327",
  keywords      = "WP4",
  annote        = "[KIT - Irem Eyiokur] AV-HuBERT is used for multimodal feature
                   extraction to learn lip synchronization. Three evaluation
                   metrics for measuring lip synchronization are proposed."
}

@ARTICLE{Pietro_Astolfi_Reyhane_Askari_Hemmat_Jun_Chen_Kushal_Tirumala_Rim_Assouel_Mazda_Moayeri_Arjang_Talattof_Kamalika_Chaudhuri_Zechun_Liu_Xilun_Chen_Quentin_Garrido_Karen_Ullrich_Aishwarya_Agrawal_Kate_Saenko_Asli_Celikyilmaz_Vikas_Chandra2024-jj,
  title    = "An Introduction to Vision-Language Modeling",
  author   = "Pietro Astolfi, Reyhane Askari Hemmat, Jun Chen, Kushal Tirumala,
              Rim Assouel, Mazda Moayeri, Arjang Talattof, Kamalika Chaudhuri,
              Zechun Liu, Xilun Chen, Quentin Garrido, Karen Ullrich, Aishwarya
              Agrawal, Kate Saenko, Asli Celikyilmaz, Vikas Chandra, Florian
              Bordes, Richard Yuanzhe Pang, Anurag Ajay, Alexander C Li, Adrien
              Bardes, Suzanne Petryk, Oscar Mañas, Zhiqiu Lin, Anas Mahmoud,
              Bargav Jayaraman, Mark Ibrahim, Melissa Hall, Yunyang Xiong,
              Jonathan Lebensold, Candace Ross, Srihari Jayakumar, Chuan Guo,
              Diane Bouchacourt, Haider Al-Tahan, Karthik Padthe, Vasu Sharma,
              Hu Xu, Xiaoqing Ellen Tan, Megan Richards, Samuel Lavoie,",
  journal  = "arXiv [cs.LG]",
  abstract = "Following the recent popularity of Large Language Models (LLMs),
              several attempts have been made to extend them to the visual
              domain. From having a visual assistant that could guide us through
              unfamiliar environments to generative models that produce images
              using only a high-level text description, the vision-language
              model (VLM) applications will significantly impact our
              relationship with technology. However, there are many challenges
              that need to be addressed to improve the reliability of those
              models. While language is discrete, vision evolves in a much
              higher dimensional space in which concepts cannot always be easily
              discretized. To better understand the mechanics behind mapping
              vision to language, we present this introduction to VLMs which we
              hope will help anyone who would like to enter the field. First, we
              introduce what VLMs are, how they work, and how to train them.
              Then, we present and discuss approaches to evaluate VLMs. Although
              this work primarily focuses on mapping images to language, we also
              discuss extending VLMs to videos.",
  month    =  "27~" # may,
  year     =  2024,
  url      = "http://dx.doi.org/10.48550",
  eprint   = "2405.17247",
  keywords = "WP3",
  doi      = "10.48550",
  annote   = "[KIT - Carlos Mullov] This is a (quite long) tutorial on
              ``vision-language modeling''. It explains different
              methods/models, from CLIP to LLaVA. Also goes into extending these
              approaches to video modality in the last chapter."
}

@ARTICLE{Wang2024-pb,
  title         = "{BLSP}-{KD}: Bootstrapping Language-Speech Pre-training via
                   Knowledge Distillation",
  author        = "Wang, Chen and Liao, Minpeng and Huang, Zhongqiang and Zhang,
                   Jiajun",
  journal       = "arXiv [cs.CL]",
  abstract      = "Recent end-to-end approaches have shown promise in extending
                   large language models (LLMs) to speech inputs, but face
                   limitations in directly assessing and optimizing alignment
                   quality and fail to achieve fine-grained alignment due to
                   speech-text length mismatch. We introduce BLSP-KD, a novel
                   approach for Bootstrapping Language-Speech Pretraining via
                   Knowledge Distillation, which addresses these limitations
                   through two key techniques. First, it optimizes speech-text
                   alignment by minimizing the divergence between the LLM's
                   next-token prediction distributions for speech and text
                   inputs using knowledge distillation. Second, it employs a
                   continuous-integrate-andfire strategy to segment speech into
                   tokens that correspond one-to-one with text tokens, enabling
                   fine-grained alignment. We also introduce Partial LoRA
                   (PLoRA), a new adaptation method supporting LLM finetuning
                   for speech inputs under knowledge distillation. Quantitative
                   evaluation shows that BLSP-KD outperforms previous end-to-end
                   baselines and cascaded systems with comparable scale of
                   parameters, facilitating general instruction-following
                   capabilities for LLMs with speech inputs. This approach
                   provides new possibilities for extending LLMs to spoken
                   language interactions.",
  month         =  "29~" # may,
  year          =  2024,
  url           = "http://arxiv.org/abs/2405.19041",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2405.19041",
  keywords      = "WP3",
  annote        = "[FBK - Sara Papi] BLSP-KD is introduced, which optimizes
                   speech-text alignment by minimizing the divergence between
                   the LLM's next-token prediction distributions for speech and
                   text inputs using knowledge distillation, and employs a
                   continuous-integrate-and-fire strategy to segment speech into
                   tokens that correspond one-to-one with text tokens, enabling
                   fine-grained alignment."
}

@ARTICLE{Deng2024-wv,
  title    = "{Wav2Prompt}: End-to-end speech prompt generation and tuning for
              {LLM} in zero and few-shot learning",
  author   = "Deng, Keqi and Sun, Guangzhi and Woodland, Phil",
  journal  = "ArXiv",
  volume   = "abs/2406.00522",
  abstract = "Wav2Prompt is proposed which allows straightforward integration
              between spoken input and a text-based large language model (LLM).
              Wav2Prompt uses a simple training process with only the same data
              used to train an automatic speech recognition (ASR) model. After
              training, Wav2Prompt learns continuous representations from speech
              and uses them as LLM prompts. To avoid task over-fitting issues
              found in prior work and preserve the emergent abilities of LLMs,
              Wav2Prompt takes LLM token embeddings as the training targets and
              utilises a continuous integrate-and-fire mechanism for explicit
              speech-text alignment. Therefore, a Wav2Prompt-LLM combination can
              be applied to zero-shot spoken language tasks such as speech
              translation (ST), speech understanding (SLU), speech question
              answering (SQA) and spoken-query-based QA (SQQA). It is shown that
              for these zero-shot tasks, Wav2Prompt performs similarly to an
              ASR-LLM cascade and better than recent prior work. If relatively
              small amounts of task-specific paired data are available in
              few-shot scenarios, the Wav2Prompt-LLM combination can be
              end-to-end (E2E) fine-tuned. The Wav2Prompt-LLM combination then
              yields greatly improved results relative to an ASR-LLM cascade for
              the above tasks. For instance, for English-French ST with the
              BLOOMZ-7B1 LLM, a Wav2Prompt-LLM combination gave a 8.5 BLEU point
              increase over an ASR-LLM cascade.",
  month    =  "1~" # jun,
  year     =  2024,
  url      = "http://dx.doi.org/10.48550/arXiv.2406.00522",
  eprint   = "2406.00522",
  keywords = "WP3",
  doi      = "10.48550/arXiv.2406.00522",
  issn     = "2331-8422",
  annote   = "[KIT - Maike Züfle] Wav2Prompt is proposed which allows
              integrating spoken input with a text-based large language model
              (LLM). Wav2Prompt uses a straightforward training process with
              only the same data used to train an automatic speech recognition
              (ASR) model. After training, Wav2Prompt learns continuous
              representations from speech and uses them as LLM prompts. Maybe
              relevant for the next version of SpeechLMM?"
}

@ARTICLE{Boeddeker2024-uz,
  title         = "Once more Diarization: Improving meeting transcription
                   systems through segment-level speaker reassignment",
  author        = "Boeddeker, Christoph and Cord-Landwehr, Tobias and
                   Haeb-Umbach, Reinhold",
  journal       = "arXiv [eess.AS]",
  abstract      = "Diarization is a crucial component in meeting transcription
                   systems to ease the challenges of speech enhancement and
                   attribute the transcriptions to the correct speaker.
                   Particularly in the presence of overlapping or noisy speech,
                   these systems have problems reliably assigning the correct
                   speaker labels, leading to a significant amount of speaker
                   confusion errors. We propose to add segment-level speaker
                   reassignment to address this issue. By revisiting, after
                   speech enhancement, the speaker attribution for each segment,
                   speaker confusion errors from the initial diarization stage
                   are significantly reduced. Through experiments across
                   different system configurations and datasets, we further
                   demonstrate the effectiveness and applicability in various
                   domains. Our results show that segment-level speaker
                   reassignment successfully rectifies at least 40\% of speaker
                   confusion word errors, highlighting its potential for
                   enhancing diarization accuracy in meeting transcription
                   systems.",
  month         =  "5~" # jun,
  year          =  2024,
  url           = "https://github.com/fgnt/speaker_reassignment",
  archivePrefix = "arXiv",
  primaryClass  = "eess.AS",
  eprint        = "2406.03155",
  keywords      = "WP3;WP5;WP4"
}

@ARTICLE{Zhou2024-bd,
  title         = "{MLVU}: A comprehensive benchmark for Multi-task Long Video
                   Understanding",
  author        = "Zhou, Junjie and Shu, Yan and Zhao, Bo and Wu, Boya and Xiao,
                   Shitao and Yang, Xi and Xiong, Yongping and Zhang, Bo and
                   Huang, Tiejun and Liu, Zheng",
  journal       = "arXiv [cs.CV]",
  abstract      = "The evaluation of Long Video Understanding (LVU) performance
                   poses an important but challenging research problem. Despite
                   previous efforts, the existing video understanding benchmarks
                   are severely constrained by several issues, especially the
                   insufficient lengths of videos, a lack of diversity in video
                   types and evaluation tasks, and the inappropriateness for
                   evaluating LVU performances. To address the above problems,
                   we propose a new benchmark, called MLVU (Multi-task Long
                   Video Understanding Benchmark), for the comprehensive and
                   in-depth evaluation of LVU. MLVU presents the following
                   critical values: 1) The substantial and flexible extension of
                   video lengths, which enables the benchmark to evaluate LVU
                   performance across a wide range of durations. 2) The
                   inclusion of various video genres, e.g., movies, surveillance
                   footage, egocentric videos, cartoons, game videos, etc.,
                   which reflects the models' LVU performances in different
                   scenarios. 3) The development of diversified evaluation
                   tasks, which enables a comprehensive examination of MLLMs'
                   key abilities in long-video understanding. The empirical
                   study with 20 latest MLLMs reveals significant room for
                   improvement in today's technique, as all existing methods
                   struggle with most of the evaluation tasks and exhibit severe
                   performance degradation when handling longer videos.
                   Additionally, it suggests that factors such as context
                   length, image-understanding quality, and the choice of LLM
                   backbone can play critical roles in future advancements. We
                   anticipate that MLVU will advance the research of long video
                   understanding by providing a comprehensive and in-depth
                   analysis of MLLMs.",
  month         =  "6~" # jun,
  year          =  2024,
  url           = "http://arxiv.org/abs/2406.04264",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2406.04264",
  keywords      = "WP3;Models",
  annote        = "[FBK - Francesco Giuliari] Benchmark for Long Video
                   understanding. Useful part is the list of methods for the
                   long video task, as they show ways to introduce a memory
                   mechanism to deal with many frames."
}

@ARTICLE{Shi2024-vu,
  title         = "{ML}-{SUPERB} 2.0: Benchmarking Multilingual Speech Models
                   Across Modeling Constraints, Languages, and Datasets",
  author        = "Shi, Jiatong and Wang, Shih-Heng and Chen, William and
                   Bartelds, Martijn and Kumar, Vanya Bannihatti and Tian,
                   Jinchuan and Chang, Xuankai and Jurafsky, Dan and Livescu,
                   Karen and Lee, Hung-Yi and Watanabe, Shinji",
  journal       = "arXiv [cs.SD]",
  abstract      = "ML-SUPERB evaluates self-supervised learning (SSL) models on
                   the tasks of language identification and automatic speech
                   recognition (ASR). This benchmark treats the models as
                   feature extractors and uses a single shallow downstream
                   model, which can be fine-tuned for a downstream task.
                   However, real-world use cases may require different
                   configurations. This paper presents ML-SUPERB~2.0, which is a
                   new benchmark for evaluating pre-trained SSL and supervised
                   speech models across downstream models, fine-tuning setups,
                   and efficient model adaptation approaches. We find
                   performance improvements over the setup of ML-SUPERB.
                   However, performance depends on the downstream model design.
                   Also, we find large performance differences between languages
                   and datasets, suggesting the need for more targeted
                   approaches to improve multilingual ASR performance.",
  month         =  "12~" # jun,
  year          =  2024,
  url           = "http://arxiv.org/abs/2406.08641",
  archivePrefix = "arXiv",
  primaryClass  = "cs.SD",
  eprint        = "2406.08641",
  keywords      = "WP4;Datasets",
  annote        = "[FBK - Sara Papi] The ML-SUPERB 2.0 benchmark is presented,
                   which is a new benchmark for evaluating pre-trained SSL and
                   supervised speech models across downstream models,
                   fine-tuning setups, and efficient model adaptation
                   approaches."
}

@ARTICLE{Sun2024-gk,
  title         = "Video-{SALMONN}: Speech-enhanced audio-visual large language
                   models",
  author        = "Sun, Guangzhi and Yu, Wenyi and Tang, Changli and Chen,
                   Xianzhao and Tan, Tian and Li, Wei and Lu, Lu and Ma, Zejun
                   and Wang, Yuxuan and Zhang, Chao",
  journal       = "arXiv [cs.CV]",
  abstract      = "Speech understanding as an element of the more generic video
                   understanding using audio-visual large language models
                   (av-LLMs) is a crucial yet understudied aspect. This paper
                   proposes video-SALMONN, a single end-to-end av-LLM for video
                   processing, which can understand not only visual frame
                   sequences, audio events and music, but speech as well. To
                   obtain fine-grained temporal information required by speech
                   understanding, while keeping efficient for other video
                   elements, this paper proposes a novel multi-resolution causal
                   Q-Former (MRC Q-Former) structure to connect pre-trained
                   audio-visual encoders and the backbone large language model.
                   Moreover, dedicated training approaches including the
                   diversity loss and the unpaired audio-visual mixed training
                   scheme are proposed to avoid frames or modality dominance. On
                   the introduced speech-audio-visual evaluation benchmark,
                   video-SALMONN achieves more than 25\% absolute accuracy
                   improvements on the video-QA task and over 30\% absolute
                   accuracy improvements on audio-visual QA tasks with human
                   speech. In addition, video-SALMONN demonstrates remarkable
                   video comprehension and reasoning abilities on tasks that are
                   unprecedented by other av-LLMs. Our training code and model
                   checkpoints are available at
                   \texttt{\url{https://github.com/bytedance/SALMONN/}}.",
  month         =  "21~" # jun,
  year          =  2024,
  url           = "http://arxiv.org/abs/2406.15704",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2406.15704",
  keywords      = "WP3;Models",
  annote        = "[KIT - Irem Eyiokur] Video-SALMONN is an end-to-end
                   audio-visual large language model. The main difference from
                   SALMONN is having video streams as input. Audio-visual
                   sequences are embedded with separate frozen audio, speech,
                   and video encoders, then, features are temporally
                   synchronized and processed by the proposed multi-resolution
                   causal (MRC) Q-Former."
}

@ARTICLE{Moslem2024-hs,
  title         = "Leveraging Synthetic Audio Data for End-to-End Low-Resource
                   Speech Translation",
  author        = "Moslem, Yasmin",
  journal       = "arXiv [cs.CL]",
  abstract      = "This paper describes our system submission to the
                   International Conference on Spoken Language Translation
                   (IWSLT 2024) for Irish-to-English speech translation. We
                   built end-to-end systems based on Whisper, and employed a
                   number of data augmentation techniques, such as speech
                   back-translation and noise augmentation. We investigate the
                   effect of using synthetic audio data and discuss several
                   methods for enriching signal diversity.",
  month         =  "25~" # jun,
  year          =  2024,
  url           = "http://arxiv.org/abs/2406.17363",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2406.17363",
  keywords      = "Datasets;WP3;WP4",
  annote        = "[TAUS - Lisa Vasileva] Using back-translated audio data in a
                   similar fashion to how back-translated text data was used to
                   improve MT in low-resource scenarios. The paper describes
                   experiments in English-to-Irish speech translation:
                   fine-tuning Whisper with authentic+synthetic data is reported
                   to show improvements across multiple metrics (compared to
                   fine-tuning on synthetic data alone)."
}

@ARTICLE{Lu2024-al,
  title         = "{DeSTA}: Enhancing Speech Language Models through Descriptive
                   Speech-Text Alignment",
  author        = "Lu, Ke-Han and Chen, Zhehuai and Fu, Szu-Wei and Huang, He
                   and Ginsburg, Boris and Wang, Yu-Chiang Frank and Lee,
                   Hung-Yi",
  journal       = "arXiv [eess.AS]",
  abstract      = "Recent speech language models (SLMs) typically incorporate
                   pre-trained speech models to extend the capabilities from
                   large language models (LLMs). In this paper, we propose a
                   Descriptive Speech-Text Alignment approach that leverages
                   speech captioning to bridge the gap between speech and text
                   modalities, enabling SLMs to interpret and generate
                   comprehensive natural language descriptions, thereby
                   facilitating the capability to understand both linguistic and
                   non-linguistic features in speech. Enhanced with the proposed
                   approach, our model demonstrates superior performance on the
                   Dynamic-SUPERB benchmark, particularly in generalizing to
                   unseen tasks. Moreover, we discover that the aligned model
                   exhibits a zero-shot instruction-following capability without
                   explicit speech instruction tuning. These findings highlight
                   the potential to reshape instruction-following SLMs by
                   incorporating rich, descriptive speech captions.",
  month         =  "27~" # jun,
  year          =  2024,
  url           = "http://arxiv.org/abs/2406.18871",
  archivePrefix = "arXiv",
  primaryClass  = "eess.AS",
  eprint        = "2406.18871",
  keywords      = "WP3;Models",
  annote        = "[FBK - Sara Papi] This paper introduces a Descriptive
                   Speech-Text Alignment approach that uses speech captioning to
                   bridge speech and text in speech language models (SLMs). The
                   method improves performance on the DynamicSUPERB benchmark
                   and enables zero-shot instruction-following without explicit
                   tuning."
}

@ARTICLE{Puvvada2024-bd,
  title         = "Less is More: Accurate Speech Recognition \& Translation
                   without Web-Scale Data",
  author        = "Puvvada, Krishna C and Żelasko, Piotr and Huang, He and
                   Hrinchuk, Oleksii and Koluguri, Nithin Rao and Dhawan, Kunal
                   and Majumdar, Somshubra and Rastorgueva, Elena and Chen,
                   Zhehuai and Lavrukhin, Vitaly and Balam, Jagadeesh and
                   Ginsburg, Boris",
  journal       = "arXiv [cs.CL]",
  abstract      = "Recent advances in speech recognition and translation rely on
                   hundreds of thousands of hours of Internet speech data. We
                   argue that state-of-the art accuracy can be reached without
                   relying on web-scale data. Canary - multilingual ASR and
                   speech translation model, outperforms current
                   state-of-the-art models - Whisper, OWSM, and Seamless-M4T on
                   English, French, Spanish, and German languages, while being
                   trained on an order of magnitude less data than these models.
                   Three key factors enables such data-efficient model: (1) a
                   FastConformer-based attention encoder-decoder architecture
                   (2) training on synthetic data generated with machine
                   translation and (3) advanced training techniques:
                   data-balancing, dynamic data blending, dynamic bucketing and
                   noise-robust fine-tuning. The model, weights, and training
                   code will be open-sourced.",
  month         =  "28~" # jun,
  year          =  2024,
  url           = "http://arxiv.org/abs/2406.19674",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2406.19674",
  keywords      = "WP3;Models",
  annote        = "[FBK - Sara Papi] The paper that presents Canary, the SFM of
                   NVIDIA covering English, French, Spanish, and German
                   languages. The backbone is FastConformer and training is
                   performed with less than 30k hours for 2 days on 128 GPUs
                   with 80GB of RAM."
}

@ARTICLE{Xu2024-gj,
  title         = "{SlowFast}-{LLaVA}: A strong training-free baseline for video
                   large language models",
  author        = "Xu, Mingze and Gao, Mingfei and Gan, Zhe and Chen, Hong-You
                   and Lai, Zhengfeng and Gang, Haiming and Kang, Kai and
                   Dehghan, Afshin",
  journal       = "arXiv [cs.CV]",
  abstract      = "We propose SlowFast-LLaVA (or SF-LLaVA for short), a
                   training-free video large language model (LLM) that can
                   jointly capture detailed spatial semantics and long-range
                   temporal context without exceeding the token budget of
                   commonly used LLMs. This is realized by using a two-stream
                   SlowFast design of inputs for Video LLMs to aggregate
                   features from sampled frames in an effective way.
                   Specifically, the Slow pathway extracts features at a low
                   frame rate while keeping as much spatial detail as possible
                   (e.g., with 12x24 tokens), and the Fast pathway operates on a
                   high frame rate but uses a larger spatial pooling stride
                   (e.g., downsampling 6x) to focus on the motion cues. As a
                   result, this design allows us to adequately capture both
                   spatial and temporal features that are beneficial for
                   detailed video understanding. Experimental results show that
                   SF-LLaVA outperforms existing training-free methods on a wide
                   range of video tasks. On some benchmarks, it achieves
                   comparable or even better performance compared to
                   state-of-the-art Video LLMs that are fine-tuned on video
                   datasets. Code has been made available at:
                   https://github.com/apple/ml-slowfast-llava.",
  month         =  "22~" # jul,
  year          =  2024,
  url           = "http://arxiv.org/abs/2407.15841",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2407.15841",
  keywords      = "WP3;Models",
  annote        = "[KIT - Irem Eyiokur] A new training-free LLaVA model that
                   streams the input video in two pathways (slow and fast). The
                   claim is that this training-free design allows for capturing
                   long-range temporal context and spatial semantics in the
                   videos enabling better understanding without exceeding
                   commonly used token sizes."
}

@ARTICLE{Maimon2024-xr,
  title         = "A suite for acoustic language model evaluation",
  author        = "Maimon, Gallil and Roth, Amit and Adi, Yossi",
  journal       = "arXiv [cs.SD]",
  abstract      = "Speech language models have recently demonstrated great
                   potential as universal speech processing systems. Such models
                   have the ability to model the rich acoustic information
                   existing in audio signals, beyond spoken content, such as
                   emotion, background noise, etc. Despite this, evaluation
                   benchmarks which evaluate awareness to a wide range of
                   acoustic aspects, are lacking. To help bridge this gap, we
                   introduce SALMon, a novel evaluation suite encompassing
                   background noise, emotion, speaker identity and room impulse
                   response. The proposed benchmarks both evaluate the
                   consistency of the inspected element and how much it matches
                   the spoken text. We follow a modelling based approach,
                   measuring whether a model gives correct samples higher scores
                   than incorrect ones. This approach makes the benchmark fast
                   to compute even for large models. We evaluated several speech
                   language models on SALMon, thus highlighting the strengths
                   and weaknesses of each evaluated method. Code and data are
                   publicly available at
                   https://pages.cs.huji.ac.il/adiyoss-lab/salmon/ .",
  month         =  "11~" # sep,
  year          =  2024,
  url           = "http://arxiv.org/abs/2409.07437",
  archivePrefix = "arXiv",
  primaryClass  = "cs.SD",
  eprint        = "2409.07437",
  keywords      = "WP4;Datasets",
  annote        = "[FBK - Sara Papi] The paper proposes SALMon, an evaluation
                   benchmark (test suite) for evaluating how much speech
                   foundation models behave with acoustic elements like
                   sentiment and background noise."
}

@ARTICLE{Bie2024-kb,
  title         = "Learning Source Disentanglement in Neural Audio Codec",
  author        = "Bie, Xiaoyu and Liu, Xubo and Richard, Gaël",
  journal       = "arXiv [cs.SD]",
  abstract      = "Neural audio codecs have significantly advanced audio
                   compression by efficiently converting continuous audio
                   signals into discrete tokens. These codecs preserve
                   high-quality sound and enable sophisticated sound generation
                   through generative models trained on these tokens. However,
                   existing neural codec models are typically trained on large,
                   undifferentiated audio datasets, neglecting the essential
                   discrepancies between sound domains like speech, music, and
                   environmental sound effects. This oversight complicates data
                   modeling and poses additional challenges to the
                   controllability of sound generation. To tackle these issues,
                   we introduce the Source-Disentangled Neural Audio Codec
                   (SD-Codec), a novel approach that combines audio coding and
                   source separation. By jointly learning audio resynthesis and
                   separation, SD-Codec explicitly assigns audio signals from
                   different domains to distinct codebooks, sets of discrete
                   representations. Experimental results indicate that SD-Codec
                   not only maintains competitive resynthesis quality but also,
                   supported by the separation results, demonstrates successful
                   disentanglement of different sources in the latent space,
                   thereby enhancing interpretability in audio codec and
                   providing potential finer control over the audio generation
                   process.",
  month         =  "17~" # sep,
  year          =  2024,
  url           = "http://arxiv.org/abs/2409.11228",
  archivePrefix = "arXiv",
  primaryClass  = "cs.SD",
  eprint        = "2409.11228",
  annote        = "[TLT - Francesco Verdini] Neural Codec with latent space
                   disentanglement for different modalities."
}

@ARTICLE{Shen2024-jg,
  title         = "{LongVU}: Spatiotemporal adaptive compression for long
                   video-language understanding",
  author        = "Shen, Xiaoqian and Xiong, Yunyang and Zhao, Changsheng and
                   Wu, Lemeng and Chen, Jun and Zhu, Chenchen and Liu, Zechun
                   and Xiao, Fanyi and Varadarajan, Balakrishnan and Bordes,
                   Florian and Liu, Zhuang and Xu, Hu and Kim, Hyunwoo J and
                   Soran, Bilge and Krishnamoorthi, Raghuraman and Elhoseiny,
                   Mohamed and Chandra, Vikas",
  journal       = "arXiv [cs.CV]",
  abstract      = "Multimodal Large Language Models (MLLMs) have shown promising
                   progress in understanding and analyzing video content.
                   However, processing long videos remains a significant
                   challenge constrained by LLM's context size. To address this
                   limitation, we propose LongVU, a spatiotemporal adaptive
                   compression mechanism thats reduces the number of video
                   tokens while preserving visual details of long videos. Our
                   idea is based on leveraging cross-modal query and inter-frame
                   dependencies to adaptively reduce temporal and spatial
                   redundancy in videos. Specifically, we leverage DINOv2
                   features to remove redundant frames that exhibit high
                   similarity. Then we utilize text-guided cross-modal query for
                   selective frame feature reduction. Further, we perform
                   spatial token reduction across frames based on their temporal
                   dependencies. Our adaptive compression strategy effectively
                   processes a large number of frames with little visual
                   information loss within given context length. Our LongVU
                   consistently surpass existing methods across a variety of
                   video understanding benchmarks, especially on hour-long video
                   understanding tasks such as VideoMME and MLVU. Given a
                   light-weight LLM, our LongVU also scales effectively into a
                   smaller size with state-of-the-art video understanding
                   performance.",
  month         =  "22~" # oct,
  year          =  2024,
  url           = "http://arxiv.org/abs/2410.17434",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2410.17434",
  keywords      = "WP3",
  annote        = "[ITU - Izzet Mustu] They propose a spatiotemporal adaptive
                   compression mechanism that reduces the number of video tokens
                   while preserving visual details of long videos. The
                   compression mechanism includes DINOv2 to remove redundant
                   frames,text-guided cross-modal query for selective frame
                   feature reduction and spatial token reduction across frames
                   based on their temporal dependencies."
}

@ARTICLE{Peng2024-xk,
  title    = "{VoiceTextBlender}: Augmenting large language models with speech
              capabilities via single-stage joint speech-text supervised
              fine-tuning",
  author   = "Peng, Yifan and Puvvada, Krishna C and Chen, Zhehuai and Zelasko,
              Piotr and Huang, He and Dhawan, Kunal and Hu, Ke and Watanabe,
              Shinji and Balam, Jagadeesh and Ginsburg, Boris",
  journal  = "ArXiv",
  volume   = "abs/2410.17485",
  abstract = "Recent studies have augmented large language models (LLMs) with
              speech capabilities, leading to the development of speech language
              models (SpeechLMs). Earlier SpeechLMs focused on single-turn
              speech-based question answering (QA), where user input comprised a
              speech context and a text question. More recent studies have
              extended this to multi-turn conversations, though they often
              require complex, multi-stage supervised fine-tuning (SFT) with
              diverse data. Another critical challenge with SpeechLMs is
              catastrophic forgetting, where models optimized for speech tasks
              suffer significant degradation in text-only performance. To
              mitigate these issues, we propose a novel single-stage joint
              speech-text SFT approach on the low-rank adaptation (LoRA) of the
              LLM backbone. Our joint SFT combines text-only SFT data with three
              types of speech-related data: speech recognition and translation,
              speech-based QA, and mixed-modal SFT. Compared to previous
              SpeechLMs with 7B or 13B parameters, our 3B model demonstrates
              superior performance across various speech benchmarks while
              preserving the original capabilities on text-only tasks.
              Furthermore, our model shows emergent abilities of effectively
              handling previously unseen prompts and tasks, including
              multi-turn, mixed-modal inputs.",
  month    =  "23~" # oct,
  year     =  2024,
  url      = "https://github.com/pyf98/NeMo_VoiceTextBlender",
  eprint   = "2410.17485",
  keywords = "WP3",
  doi      = "10.48550/arXiv.2410.17485",
  issn     = "2331-8422",
  annote   = "[KIT - Maike Züfle] This paper introduces a SpeechLM capable of
              multimodal multiturn-QA. Specifically, the model is trained in a
              single-stage speech-text SFT approach on the low-rank adaptation
              (LoRA) of the LLM backbone. Speech and text segments are mixed
              throughout sentences in the multi-turn SpeechQA training,
              addressing data scarcitiy for multimodal SpeechQA, and efficiently
              enabling the model to handle multi-turn SpeechQA. Maybe relevant
              for the next version of SpeechLMM?"
}

@ARTICLE{Liu2024-nz,
  title         = "Protecting privacy in Multimodal Large Language Models with
                   {MLLMU}-Bench",
  author        = "Liu, Zheyuan and Dou, Guangyao and Jia, Mengzhao and Tan,
                   Zhaoxuan and Zeng, Qingkai and Yuan, Yongle and Jiang, Meng",
  journal       = "arXiv [cs.CL]",
  abstract      = "Generative models such as Large Language Models (LLM) and
                   Multimodal Large Language models (MLLMs) trained on massive
                   web corpora can memorize and disclose individuals'
                   confidential and private data, raising legal and ethical
                   concerns. While many previous works have addressed this issue
                   in LLM via machine unlearning, it remains largely unexplored
                   for MLLMs. To tackle this challenge, we introduce Multimodal
                   Large Language Model Unlearning Benchmark (MLLMU-Bench), a
                   novel benchmark aimed at advancing the understanding of
                   multimodal machine unlearning. MLLMU-Bench consists of 500
                   fictitious profiles and 153 profiles for public celebrities,
                   each profile feature over 14 customized question-answer
                   pairs, evaluated from both multimodal (image+text) and
                   unimodal (text) perspectives. The benchmark is divided into
                   four sets to assess unlearning algorithms in terms of
                   efficacy, generalizability, and model utility. Finally, we
                   provide baseline results using existing generative model
                   unlearning algorithms. Surprisingly, our experiments show
                   that unimodal unlearning algorithms excel in generation and
                   cloze tasks, while multimodal unlearning approaches perform
                   better in classification tasks with multimodal inputs.",
  month         =  "29~" # oct,
  year          =  2024,
  url           = "http://arxiv.org/abs/2410.22108",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2410.22108",
  keywords      = "WP5",
  annote        = "[KIT - Maike Züfle] Multimodal Large Language models trained
                   on massive web corpora can memorize and disclose individuals’
                   confidential and private data, raising legal and ethical
                   concerns. Machine Unlearning is a method to address this
                   issue. This paper introduces a benchmark for multiple tasks
                   (generation and classification) to assess a model's ability
                   to ``unlearn'' private information. This dataset could be
                   useful for WP5, when assessing criticality scenarios with
                   newer versions of SpeechLMM."
}

@ARTICLE{Andreas2024-qp,
  title         = "{PaliGemma} 2: A family of versatile {VLMs} for transfer",
  author        = "Andreas, Steiner and Pinto, André Susano and Michael,
                   Tschannen and Daniel, Keysers and Xiao, Wang and Yonatan,
                   Bitton and Alexey, Gritsenko and Matthias, Minderer and
                   Anthony, Sherbondy and Shangbang, Long and Siyang, Qin and
                   Reeve, Ingle and Emanuele, Bugliarello and Sahar, Kazemzadeh
                   and Thomas, Mesnard and Ibrahim, Alabdulmohsin and Lucas,
                   Beyer and Xiaohua, Zhai",
  journal       = "arXiv [cs.CV]",
  abstract      = "PaliGemma 2 is an upgrade of the PaliGemma open
                   Vision-Language Model (VLM) based on the Gemma 2 family of
                   language models. We combine the SigLIP-So400m vision encoder
                   that was also used by PaliGemma with the whole range of Gemma
                   2 models, from the 2B one all the way up to the 27B model. We
                   train these models at three resolutions (224px, 448px, and
                   896px) in multiple stages to equip them with broad knowledge
                   for transfer via fine-tuning. The resulting family of base
                   models covering different model sizes and resolutions allows
                   us to investigate factors impacting transfer performance
                   (such as learning rate) and to analyze the interplay between
                   the type of task, model size, and resolution. We further
                   increase the number and breadth of transfer tasks beyond the
                   scope of PaliGemma including different OCR-related tasks such
                   as table structure recognition, molecular structure
                   recognition, music score recognition, as well as long
                   fine-grained captioning and radiography report generation, on
                   which PaliGemma 2 obtains state-of-the-art results.",
  month         =  "4~" # dec,
  year          =  2024,
  url           = "http://arxiv.org/abs/2412.03555",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2412.03555",
  keywords      = "WP3;Models",
  annote        = "[KIT] Google's new Vision-Language model, based on SigLip
                   (Google's improved CLIP) and Gemma. Uses the Gemma license, a
                   permissive license that allows redistribution, fine-tuning,
                   commercial use, and derivative works."
}

@ARTICLE{Pagnoni2024-gc,
  title         = "Byte Latent Transformer: Patches scale better than tokens",
  author        = "Pagnoni, Artidoro and Pasunuru, Ram and Rodriguez, Pedro and
                   Nguyen, John and Muller, Benjamin and Li, Margaret and Zhou,
                   Chunting and Yu, Lili and Weston, Jason and Zettlemoyer, Luke
                   and Ghosh, Gargi and Lewis, Mike and Holtzman, Ari and Iyer,
                   Srinivasan",
  journal       = "arXiv [cs.CL]",
  abstract      = "We introduce the Byte Latent Transformer (BLT), a new
                   byte-level LLM architecture that, for the first time, matches
                   tokenization-based LLM performance at scale with significant
                   improvements in inference efficiency and robustness. BLT
                   encodes bytes into dynamically sized patches, which serve as
                   the primary units of computation. Patches are segmented based
                   on the entropy of the next byte, allocating more compute and
                   model capacity where increased data complexity demands it. We
                   present the first FLOP controlled scaling study of byte-level
                   models up to 8B parameters and 4T training bytes. Our results
                   demonstrate the feasibility of scaling models trained on raw
                   bytes without a fixed vocabulary. Both training and inference
                   efficiency improve due to dynamically selecting long patches
                   when data is predictable, along with qualitative improvements
                   on reasoning and long tail generalization. Overall, for fixed
                   inference costs, BLT shows significantly better scaling than
                   tokenization-based models, by simultaneously growing both
                   patch and model size.",
  month         =  "13~" # dec,
  year          =  2024,
  url           = "http://arxiv.org/abs/2412.09871",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2412.09871",
  keywords      = "WP3",
  annote        = "[KIT] The BLT is a byte-level language model that uses an
                   internal neural tokenization. Based on the entropy spikes
                   within a small byte-level LM, the input bytes are downsampled
                   into ``patches''. The downsampling scheme seems interesting
                   for WP3 but too specific to the text modality. See the other
                   entry (Dynamic Chunking for End-to-End Hierarchical Sequence
                   Modeling) for a similar approach that might transfer to
                   non-text modalities."
}

@ARTICLE{Springer2025-vb,
  title         = "Overtrained language models are harder to fine-tune",
  author        = "Springer, Jacob Mitchell and Goyal, Sachin and Wen, Kaiyue
                   and Kumar, Tanishq and Yue, Xiang and Malladi, Sadhika and
                   Neubig, Graham and Raghunathan, Aditi",
  journal       = "arXiv [cs.CL]",
  abstract      = "Large language models are pre-trained on ever-growing token
                   budgets under the assumption that better pre-training
                   performance translates to improved downstream models. In this
                   work, we challenge this assumption and show that extended
                   pre-training can make models harder to fine-tune, leading to
                   degraded final performance. We term this phenomenon
                   catastrophic overtraining. For example, the instruction-tuned
                   OLMo-1B model pre-trained on 3T tokens leads to over 2\%
                   worse performance on multiple standard LLM benchmarks than
                   its 2.3T token counterpart. Through controlled experiments
                   and theoretical analysis, we show that catastrophic
                   overtraining arises from a systematic increase in the broad
                   sensitivity of pre-trained parameters to modifications,
                   including but not limited to fine-tuning. Our findings call
                   for a critical reassessment of pre-training design that
                   considers the downstream adaptability of the model.",
  month         =  "24~" # mar,
  year          =  2025,
  url           = "http://arxiv.org/abs/2503.19206",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2503.19206",
  annote        = "[KIT] This recent paper seems to find that ``over-trained''
                   language models (like Llama 3.x) perform worse in downstream
                   fine-tuning when you fine-tune from some sort of
                   ``post-trained'' (instruction fine-tuned, ...)
                   checkpoint.This seems potentially highly relevant to out WP4
                   issues with bad fine-tuning performance, and might give an
                   explanation, since we continue pre-training/fine-tuning from
                   the instruction fine-tuned variants."
}

@ARTICLE{Wang2025-hj,
  title         = "Vision as {LoRA}",
  author        = "Wang, Han and Ye, Yongjie and Li, Bingru and Nie, Yuxiang and
                   Lu, Jinghui and Tang, Jingqun and Wang, Yanjie and Huang, Can",
  journal       = "arXiv [cs.CV]",
  abstract      = "We introduce Vision as LoRA (VoRA), a novel paradigm for
                   transforming an LLM into an MLLM. Unlike prevalent MLLM
                   architectures that rely on external vision modules for vision
                   encoding, VoRA internalizes visual capabilities by
                   integrating vision-specific LoRA layers directly into the
                   LLM. This design allows the added parameters to be seamlessly
                   merged into the LLM during inference, eliminating structural
                   complexity and minimizing computational overhead. Moreover,
                   inheriting the LLM's ability of handling flexible context,
                   VoRA can process inputs at arbitrary resolutions. To further
                   strengthen VoRA's visual capabilities, we introduce a
                   block-wise distillation method that transfers visual priors
                   from a pre-trained ViT into the LoRA layers, effectively
                   accelerating training by injecting visual knowledge.
                   Additionally, we apply bi-directional attention masks to
                   better capture the context information of an image. We
                   successfully demonstrate that with additional pre-training
                   data, VoRA can perform comparably with conventional
                   encode-based MLLMs. All training data, codes, and model
                   weights will be released at https://github.com/Hon-Wong/VoRA.",
  month         =  "26~" # mar,
  year          =  2025,
  url           = "http://arxiv.org/abs/2503.20680",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2503.20680",
  keywords      = "WP3;WP4",
  annote        = "[FBK - Francesco Giuliari] Here they showcase how you can
                   bypass the use of an encoder (vision encoder here) and
                   instead train directly as set of lora weights for the LLM
                   that act as encoder"
}

@ARTICLE{Arora2025-rw,
  title         = "On the landscape of spoken language models: A comprehensive
                   survey",
  author        = "Arora, Siddhant and Chang, Kai-Wei and Chien, Chung-Ming and
                   Peng, Yifan and Wu, Haibin and Adi, Yossi and Dupoux,
                   Emmanuel and Lee, Hung-Yi and Livescu, Karen and Watanabe,
                   Shinji",
  journal       = "arXiv [cs.CL]",
  abstract      = "The field of spoken language processing is undergoing a shift
                   from training custom-built, task-specific models toward using
                   and optimizing spoken language models (SLMs) which act as
                   universal speech processing systems. This trend is similar to
                   the progression toward universal language models that has
                   taken place in the field of (text) natural language
                   processing. SLMs include both ``pure'' language models of
                   speech -- models of the distribution of tokenized speech
                   sequences -- and models that combine speech encoders with
                   text language models, often including both spoken and written
                   input or output. Work in this area is very diverse, with a
                   range of terminology and evaluation settings. This paper aims
                   to contribute an improved understanding of SLMs via a
                   unifying literature survey of recent work in the context of
                   the evolution of the field. Our survey categorizes the work
                   in this area by model architecture, training, and evaluation
                   choices, and describes some key challenges and directions for
                   future work.",
  month         =  "11~" # apr,
  year          =  2025,
  url           = "http://arxiv.org/abs/2504.08528",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2504.08528",
  keywords      = "WP3",
  annote        = "[PI - Francesco Cariaggi] Extremely insightful survey on the
                   different types of Spoken Language Models (SLMs). The authors
                   make a distinction between pure speech LMs (e.g. Audio LM),
                   interleaved speech/text LMs (e.g. SpiritLM) and speech-aware
                   text LMs (e.g. SALMONN)."
}

@ARTICLE{Yang2025-fz,
  title         = "{Qwen3} Technical Report",
  author        = "Yang, An and Li, Anfeng and Yang, Baosong and Zhang, Beichen
                   and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Gao, Chang
                   and Huang, Chengen and Lv, Chenxu and Zheng, Chujie and Liu,
                   Dayiheng and Zhou, Fan and Huang, Fei and Hu, Feng and Ge,
                   Hao and Wei, Haoran and Lin, Huan and Tang, Jialong and Yang,
                   Jian and Tu, Jianhong and Zhang, Jianwei and Yang, Jianxin
                   and Yang, Jiaxi and Zhou, Jing and Zhou, Jingren and Lin,
                   Junyang and Dang, Kai and Bao, Keqin and Yang, Kexin and Yu,
                   Le and Deng, Lianghao and Li, Mei and Xue, Mingfeng and Li,
                   Mingze and Zhang, Pei and Wang, Peng and Zhu, Qin and Men,
                   Rui and Gao, Ruize and Liu, Shixuan and Luo, Shuang and Li,
                   Tianhao and Tang, Tianyi and Yin, Wenbiao and Ren, Xingzhang
                   and Wang, Xinyu and Zhang, Xinyu and Ren, Xuancheng and Fan,
                   Yang and Su, Yang and Zhang, Yichang and Zhang, Yinger and
                   Wan, Yu and Liu, Yuqiong and Wang, Zekun and Cui, Zeyu and
                   Zhang, Zhenru and Zhou, Zhipeng and Qiu, Zihan",
  journal       = "arXiv [cs.CL]",
  abstract      = "In this work, we present Qwen3, the latest version of the
                   Qwen model family. Qwen3 comprises a series of large language
                   models (LLMs) designed to advance performance, efficiency,
                   and multilingual capabilities. The Qwen3 series includes
                   models of both dense and Mixture-of-Expert (MoE)
                   architectures, with parameter scales ranging from 0.6 to 235
                   billion. A key innovation in Qwen3 is the integration of
                   thinking mode (for complex, multi-step reasoning) and
                   non-thinking mode (for rapid, context-driven responses) into
                   a unified framework. This eliminates the need to switch
                   between different models--such as chat-optimized models
                   (e.g., GPT-4o) and dedicated reasoning models (e.g.,
                   QwQ-32B)--and enables dynamic mode switching based on user
                   queries or chat templates. Meanwhile, Qwen3 introduces a
                   thinking budget mechanism, allowing users to allocate
                   computational resources adaptively during inference, thereby
                   balancing latency and performance based on task complexity.
                   Moreover, by leveraging the knowledge from the flagship
                   models, we significantly reduce the computational resources
                   required to build smaller-scale models, while ensuring their
                   highly competitive performance. Empirical evaluations
                   demonstrate that Qwen3 achieves state-of-the-art results
                   across diverse benchmarks, including tasks in code
                   generation, mathematical reasoning, agent tasks, etc.,
                   competitive against larger MoE models and proprietary models.
                   Compared to its predecessor Qwen2.5, Qwen3 expands
                   multilingual support from 29 to 119 languages and dialects,
                   enhancing global accessibility through improved cross-lingual
                   understanding and generation capabilities. To facilitate
                   reproducibility and community-driven research and
                   development, all Qwen3 models are publicly accessible under
                   Apache 2.0.",
  month         =  "14~" # may,
  year          =  2025,
  url           = "http://arxiv.org/abs/2505.09388",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2505.09388",
  keywords      = "Models",
  annote        = "[TAUS - Amir Soleimani] The new Qwen LLM update. It supports
                   +100 languages but more importantly many small size models
                   are available and can be used for data synthesizing /
                   cleaning / processing.Models are available at
                   https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f"
}

@ARTICLE{Hu2025-uo,
  title         = "Efficient and direct duplex modeling for speech-to-speech
                   language model",
  author        = "Hu, Ke and Hosseini-Asl, Ehsan and Chen, Chen and Casanova,
                   Edresson and Ghosh, Subhankar and Żelasko, Piotr and Chen,
                   Zhehuai and Li, Jason and Balam, Jagadeesh and Ginsburg,
                   Boris",
  journal       = "arXiv [cs.CL]",
  abstract      = "Spoken dialogue is an intuitive form of human-computer
                   interaction, yet current speech language models often remain
                   constrained to turn-based exchanges, lacking real-time
                   adaptability such as user barge-in. We propose a novel duplex
                   speech to speech (S2S) architecture featuring continuous user
                   inputs and codec agent outputs with channel fusion that
                   directly models simultaneous user and agent streams. Using a
                   pretrained streaming encoder for user input enables the first
                   duplex S2S model without requiring speech pretrain. Separate
                   architectures for agent and user modeling facilitate codec
                   fine-tuning for better agent voices and halve the bitrate
                   (0.6 kbps) compared to previous works. Experimental results
                   show that the proposed model outperforms previous duplex
                   models in reasoning, turn-taking, and barge-in abilities. The
                   model requires significantly less speech data, as speech
                   pretrain is skipped, which markedly simplifies the process of
                   building a duplex S2S model from any LLMs. Finally, it is the
                   first openly available duplex S2S model with training and
                   inference code to foster reproducibility.",
  month         =  "21~" # may,
  year          =  2025,
  url           = "http://arxiv.org/abs/2505.15670",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2505.15670",
  keywords      = "WP3",
  annote        = "[KIT - Maike Züfle] This paper proposes a novel duplex speech
                   to speech (S2S) architecture featuring continuous user inputs
                   and codec agent outputs with channel fusion that directly
                   models simultaneous user and agent streams. Using a
                   pretrained streaming encoder for user input enables the first
                   duplex S2S model without requiring speech pretrain. This is a
                   promising approach, since it is quite data efficient. Also
                   the code is openly available."
}

@ARTICLE{Neplenbroek2025-kg,
  title         = "Reading between the prompts: How stereotypes shape {LLM}'s
                   implicit personalization",
  author        = "Neplenbroek, Vera and Bisazza, Arianna and Fernández, Raquel",
  journal       = "arXiv [cs.CL]",
  abstract      = "Generative Large Language Models (LLMs) infer user's
                   demographic information from subtle cues in the conversation
                   -- a phenomenon called implicit personalization. Prior work
                   has shown that such inferences can lead to lower quality
                   responses for users assumed to be from minority groups, even
                   when no demographic information is explicitly provided. In
                   this work, we systematically explore how LLMs respond to
                   stereotypical cues using controlled synthetic conversations,
                   by analyzing the models' latent user representations through
                   both model internals and generated answers to targeted user
                   questions. Our findings reveal that LLMs do infer demographic
                   attributes based on these stereotypical signals, which for a
                   number of groups even persists when the user explicitly
                   identifies with a different demographic group. Finally, we
                   show that this form of stereotype-driven implicit
                   personalization can be effectively mitigated by intervening
                   on the model's internal representations using a trained
                   linear probe to steer them toward the explicitly stated
                   identity. Our results highlight the need for greater
                   transparency and control in how LLMs represent user identity.",
  month         =  "22~" # may,
  year          =  2025,
  url           = "http://arxiv.org/abs/2505.16467",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2505.16467",
  keywords      = "WP5",
  annote        = "[KIT - Maike Züfle] Generative Large Language Models (LLMs)
                   infer user’s demographic information from sub-tle cues in the
                   conversation — a phenomenon called implicit personalization.
                   This work systematically explores how LLMs respond to
                   stereotypical cues using controlled synthetic
                   conversations.The dataset is publicly availble
                   (https://github.com/Veranep/implicit-personalization-stereotypes)
                   and could be used to test SpeechLMM for WP5."
}

@ARTICLE{Xie2025-xm,
  title         = "Enhancing generalization of speech large language models with
                   multi-task behavior imitation and speech-text interleaving",
  author        = "Xie, Jingran and Li, Xiang and Wang, Hui and Yu, Yue and
                   Xiang, Yang and Wu, Xixin and Wu, Zhiyong",
  journal       = "arXiv [eess.AS]",
  abstract      = "Large language models (LLMs) have shown remarkable
                   generalization across tasks, leading to increased interest in
                   integrating speech with LLMs. These speech LLMs (SLLMs)
                   typically use supervised fine-tuning to align speech with
                   text-based LLMs. However, the lack of annotated speech data
                   across a wide range of tasks hinders alignment efficiency,
                   resulting in poor generalization. To address these issues, we
                   propose a novel multi-task 'behavior imitation' method with
                   speech-text interleaving, called MTBI, which relies solely on
                   paired speech and transcripts. By ensuring the LLM decoder
                   generates equivalent responses to paired speech and text, we
                   achieve a more generalized SLLM. Interleaving is used to
                   further enhance alignment efficiency. We introduce a simple
                   benchmark to evaluate prompt and task generalization across
                   different models. Experimental results demonstrate that our
                   MTBI outperforms SOTA SLLMs on both prompt and task
                   generalization, while requiring less supervised speech data.",
  month         =  "24~" # may,
  year          =  2025,
  url           = "http://arxiv.org/abs/2505.18644",
  archivePrefix = "arXiv",
  primaryClass  = "eess.AS",
  eprint        = "2505.18644",
  keywords      = "WP3",
  annote        = "[FBK - Sara Papi] A a novel multi-task ’behavior imitation’
                   method with speech-text interleaving (MTBI), which relies
                   solely on paired speech and transcripts, is proposed to merge
                   speech foundation models (WavLM) with large language models
                   (Llama2)."
}

@ARTICLE{Mousavi2025-tb,
  title         = "Discrete audio tokens: More than a survey!",
  author        = "Mousavi, Pooneh and Maimon, Gallil and Moumen, Adel and
                   Petermann, Darius and Shi, Jiatong and Wu, Haibin and Yang,
                   Haici and Kuznetsova, Anastasia and Ploujnikov, Artem and
                   Marxer, Ricard and Ramabhadran, Bhuvana and Elizalde,
                   Benjamin and Lugosch, Loren and Li, Jinyu and Subakan, Cem
                   and Woodland, Phil and Kim, Minje and Lee, Hung-Yi and
                   Watanabe, Shinji and Adi, Yossi and Ravanelli, Mirco",
  journal       = "arXiv [cs.SD]",
  abstract      = "Discrete audio tokens are compact representations that aim to
                   preserve perceptual quality, phonetic content, and speaker
                   characteristics while enabling efficient storage and
                   inference, as well as competitive performance across diverse
                   downstream tasks.They provide a practical alternative to
                   continuous features, enabling the integration of speech and
                   audio into modern large language models (LLMs). As interest
                   in token-based audio processing grows, various tokenization
                   methods have emerged, and several surveys have reviewed the
                   latest progress in the field. However, existing studies often
                   focus on specific domains or tasks and lack a unified
                   comparison across various benchmarks. This paper presents a
                   systematic review and benchmark of discrete audio tokenizers,
                   covering three domains: speech, music, and general audio. We
                   propose a taxonomy of tokenization approaches based on
                   encoder-decoder, quantization techniques, training paradigm,
                   streamability, and application domains. We evaluate
                   tokenizers on multiple benchmarks for reconstruction,
                   downstream performance, and acoustic language modeling, and
                   analyze trade-offs through controlled ablation studies. Our
                   findings highlight key limitations, practical considerations,
                   and open challenges, providing insight and guidance for
                   future research in this rapidly evolving area. For more
                   information, including our main results and tokenizer
                   database, please refer to our website:
                   https://poonehmousavi.github.io/dates-website/.",
  month         =  "11~" # jun,
  year          =  2025,
  url           = "http://arxiv.org/abs/2506.10274",
  archivePrefix = "arXiv",
  primaryClass  = "cs.SD",
  eprint        = "2506.10274",
  keywords      = "WP3",
  annote        = "[PI - Francesco Cariaggi] Survey on discrete audio tokens
                   offering a taxonomy of audio ``tokenization'' approaches.
                   They also evaluate them on several benchmarks and identify
                   tradeoffs between different approaches."
}

@ARTICLE{Rei2025-iz,
  title         = "Tower+: Bridging generality and translation specialization in
                   multilingual {LLMs}",
  author        = "Rei, Ricardo and Guerreiro, Nuno M and Pombal, José and
                   Alves, João and Teixeirinha, Pedro and Farajian, Amin and
                   Martins, André F T",
  journal       = "arXiv [cs.CL]",
  abstract      = "Fine-tuning pretrained LLMs has been shown to be an effective
                   strategy for reaching state-of-the-art performance on
                   specific tasks like machine translation. However, this
                   process of adaptation often implies sacrificing
                   general-purpose capabilities, such as conversational
                   reasoning and instruction-following, hampering the utility of
                   the system in real-world applications that require a mixture
                   of skills. In this paper, we introduce Tower+, a suite of
                   models designed to deliver strong performance across both
                   translation and multilingual general-purpose text
                   capabilities. We achieve a Pareto frontier between
                   translation specialization and multilingual general-purpose
                   capabilities by introducing a novel training recipe that
                   builds on Tower (Alves et al., 2024), comprising continued
                   pretraining, supervised fine-tuning, preference optimization,
                   and reinforcement learning with verifiable rewards. At each
                   stage of training, we carefully generate and curate data to
                   strengthen performance on translation as well as
                   general-purpose tasks involving code generation, mathematics
                   problem solving, and general instruction-following. We
                   develop models at multiple scales: 2B, 9B, and 72B. Our
                   smaller models often outperform larger general-purpose
                   open-weight and proprietary LLMs (e.g., Llama 3.3 70B,
                   GPT-4o). Our largest model delivers best-in-class translation
                   performance for high-resource languages and top results in
                   multilingual Arena Hard evaluations and in IF-MT, a benchmark
                   we introduce for evaluating both translation and
                   instruction-following. Our findings highlight that it is
                   possible to rival frontier models in general capabilities,
                   while optimizing for specific business domains, such as
                   translation and localization.",
  month         =  "20~" # jun,
  year          =  2025,
  url           = "http://arxiv.org/abs/2506.17080",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2506.17080",
  keywords      = "Models",
  annote        = "[TAUS, Amir Soleimani] A fine-tuned multilingual and
                   translation-based LLM which can preserve its performance on
                   general applications as well."
}

@ARTICLE{Ramirez2025-wd,
  title         = "Controlling what you share: Assessing language model
                   adherence to privacy preferences",
  author        = "Ramírez, Guillem and Birch, Alexandra and Titov, Ivan",
  journal       = "arXiv [cs.CL]",
  abstract      = "Large language models (LLMs) are primarily accessed via
                   commercial APIs, but this often requires users to expose
                   their data to service providers. In this paper, we explore
                   how users can stay in control of their data by using privacy
                   profiles: simple natural language instructions that say what
                   should and should not be revealed. We build a framework where
                   a local model uses these instructions to rewrite queries,
                   only hiding details deemed sensitive by the user, before
                   sending them to an external model, thus balancing privacy
                   with performance. To support this research, we introduce
                   PEEP, a multilingual dataset of real user queries annotated
                   to mark private content and paired with synthetic privacy
                   profiles. Our experiments with lightweight LLMs show they can
                   follow these instructions to some extent, but also face
                   consistent challenges, highlighting the need for models that
                   better understand and comply with user-defined privacy
                   preferences.",
  month         =  "7~" # jul,
  year          =  2025,
  url           = "http://arxiv.org/abs/2507.05391",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2507.05391",
  keywords      = "WP5",
  annote        = "[KIT] This work uses ``privacy profiles'' to control what an
                   LLM will share. These privacy profiles are natural language
                   descriptions of what should not be revealed. The paper
                   introduces a dataset ``PEEP'' to measure LLM adherence to the
                   privacy profiles and finds consistent challenges."
}

@ARTICLE{Guo2025-ai,
  title         = "{StreamUni}: Achieving streaming speech translation with a
                   unified Large Speech-Language Model",
  author        = "Guo, Shoutao and Li, Xiang and Zhang, Shaolei and Liu, Mengge
                   and Chen, Wei and Feng, Yang",
  journal       = "arXiv [cs.CL]",
  abstract      = "Streaming speech translation (StreamST) requires determining
                   appropriate timing, known as policy, to generate translations
                   while continuously receiving source speech inputs, balancing
                   low latency with high translation quality. However, existing
                   StreamST methods typically operate on sentence-level speech
                   segments, referred to as simultaneous speech translation
                   (SimulST). In practice, they require collaboration with
                   segmentation models to accomplish StreamST, where the
                   truncated speech segments constrain SimulST models to make
                   policy decisions and generate translations based on limited
                   contextual information. Moreover, SimulST models struggle to
                   learn effective policies due to the complexity of speech
                   inputs and cross-lingual generation. To address these
                   challenges, we propose StreamUni, which achieves StreamST
                   through a unified Large Speech-Language Model (LSLM).
                   Specifically, StreamUni incorporates speech Chain-of-Thought
                   (CoT) in guiding the LSLM to generate multi-stage outputs.
                   Leveraging these multi-stage outputs, StreamUni
                   simultaneously accomplishes speech segmentation, policy
                   decision, and translation generation, completing StreamST
                   without requiring massive policy-specific training.
                   Additionally, we propose a streaming CoT training method that
                   enhances low-latency policy decisions and generation
                   capabilities using limited CoT data. Experiments demonstrate
                   that our approach achieves state-of-the-art performance on
                   StreamST tasks.",
  month         =  "10~" # jul,
  year          =  2025,
  url           = "https://github.com/ictnlp/StreamUni",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2507.07803",
  keywords      = "WP3;WP4",
  annote        = "[KIT - Maike Züfle] Existing StreamST methods typically
                   operate on sentence-level speech segments, referred to as
                   simultaneous speech translation (SimulST). In practice, they
                   require collaboration with segmentation models to accomplish
                   StreamST, where the truncated speech segments constrain
                   SimulST modelsto make policy decisions and generate
                   translations based on limited contextual information. This
                   paper proposes StreamUni, which achieves StreamST through a
                   unified Large Speech-Language Model. Specifically, StreamUni
                   incorporates speech Chain-of-Thought (CoT) in guiding the
                   LSLM to generate multi-stage outputs."
}

@ARTICLE{Hwang2025-kz,
  title         = "Dynamic chunking for end-to-end hierarchical sequence
                   modeling",
  author        = "Hwang, Sukjun and Wang, Brandon and Gu, Albert",
  journal       = "arXiv [cs.LG]",
  abstract      = "Despite incredible progress in language models (LMs) in
                   recent years, largely resulting from moving away from
                   specialized models designed for specific tasks to general
                   models based on powerful architectures (e.g. the Transformer)
                   that learn everything from raw data, pre-processing steps
                   such as tokenization remain a barrier to true end-to-end
                   foundation models. We introduce a collection of new
                   techniques that enable a dynamic chunking mechanism which
                   automatically learns content -- and context -- dependent
                   segmentation strategies learned jointly with the rest of the
                   model. Incorporating this into an explicit hierarchical
                   network (H-Net) allows replacing the (implicitly
                   hierarchical) tokenization-LM-detokenization pipeline with a
                   single model learned fully end-to-end. When compute- and
                   data- matched, an H-Net with one stage of hierarchy operating
                   at the byte level outperforms a strong Transformer language
                   model operating over BPE tokens. Iterating the hierarchy to
                   multiple stages further increases its performance by modeling
                   multiple levels of abstraction, demonstrating significantly
                   better scaling with data and matching a token-based
                   Transformer of twice its size. H-Nets pretrained on English
                   show significantly increased character-level robustness, and
                   qualitatively learn meaningful data-dependent chunking
                   strategies without any heuristics or explicit supervision.
                   Finally, the H-Net's improvement over tokenized pipelines is
                   further increased in languages and modalities with weaker
                   tokenization heuristics, such as Chinese and code, or DNA
                   sequences (nearly 4x improvement in data efficiency over
                   baselines), showing the potential of true end-to-end models
                   that learn and scale better from unprocessed data.",
  month         =  "10~" # jul,
  year          =  2025,
  url           = "http://arxiv.org/abs/2507.07955",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2507.07955",
  keywords      = "WP3",
  annote        = "[KIT] This paper introduces a downsampling of (e.g.
                   byte-based) inputs, inspired by the Byte Latent Transformer
                   (BLT) approach of tokenization, but designed to be more
                   modality agnostic than the BLT tokenization. Compared to the
                   BLT entropy spikes, this work uses a downsampling modules
                   with a router based on neighbouring position similarity."
}

@ARTICLE{Cloud2025-gm,
  title         = "Subliminal Learning: Language models transmit behavioral
                   traits via hidden signals in data",
  author        = "Cloud, Alex and Le, Minh and Chua, James and Betley, Jan and
                   Sztyber-Betley, Anna and Hilton, Jacob and Marks, Samuel and
                   Evans, Owain",
  journal       = "arXiv [cs.LG]",
  abstract      = "We study subliminal learning, a surprising phenomenon where
                   language models transmit behavioral traits via semantically
                   unrelated data. In our main experiments, a ``teacher'' model
                   with some trait T (such as liking owls or being misaligned)
                   generates a dataset consisting solely of number sequences.
                   Remarkably, a ``student'' model trained on this dataset
                   learns T. This occurs even when the data is filtered to
                   remove references to T. We observe the same effect when
                   training on code or reasoning traces generated by the same
                   teacher model. However, we do not observe the effect when the
                   teacher and student have different base models. To help
                   explain our findings, we prove a theoretical result showing
                   that subliminal learning occurs in all neural networks under
                   certain conditions, and demonstrate subliminal learning in a
                   simple MLP classifier. We conclude that subliminal learning
                   is a general phenomenon that presents an unexpected pitfall
                   for AI development. Distillation could propagate unintended
                   traits, even when developers try to prevent this via data
                   filtering.",
  month         =  "20~" # jul,
  year          =  2025,
  url           = "http://arxiv.org/abs/2507.14805",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2507.14805",
  annote        = "[KIT - Irem Eyiokur] - In this paper, authors find that
                   language models can transmit hidden behavioral traits to
                   other models through seemingly unrelated data, a phenomenon
                   they call subliminal learning. Even when a teacher model only
                   produces number sequences or filtered outputs without
                   explicit references to its traits, a student model fine-tuned
                   on that data inherits those traits (e.g., preferences,
                   misalignment). The results suggest that distillation can
                   unintentionally propagate undesirable behaviors, highlighting
                   a significant risk for AI safety and alignment."
}

@ARTICLE{Ioannis_Tsiamas_and_Gerard_I_Gallego_and_Jose_A_R_Fonollosa_and_Marta_R_Costa-jussa2024-fy,
  title    = "Pushing the Limits of Zero-shot End-to-End Speech Translation",
  author   = "{Ioannis Tsiamas and Gerard I. Gállego and José A. R. Fonollosa
              and Marta R. Costa-jussà}",
  journal  = "arXiv",
  abstract = "Data scarcity and the modality gap between the speech and text
              modalities are two major obstacles of end-to-end Speech
              Translation (ST) systems, thus hindering their performance. Prior
              work has attempted to mitigate these challenges by leveraging
              external MT data and optimizing distance metrics that bring closer
              the speech-text representations. However, achieving competitive
              results typically requires some ST data. For this reason, we
              introduce ZeroSwot, a method for zero-shot ST that bridges the
              modality gap without any paired ST data. Leveraging a novel CTC
              compression and Optimal Transport, we train a speech encoder using
              only ASR data, to align with the representation space of a
              massively multilingual MT model. The speech encoder seamlessly
              integrates with the MT model at inference, enabling direct
              translation from speech to text, across all languages supported by
              the MT model. Our experiments show that we can effectively close
              the modality gap without ST data, while our results on MuST-C and
              CoVoST demonstrate our method's superiority over not only previous
              zero-shot models, but also supervised ones, achieving
              state-of-the-art results.",
  year     =  2024,
  url      = "https://arxiv.org/abs/2402.10422",
  keywords = "WP4;Models",
  doi      = " https://doi.org/10.48550/arXiv.2402.10422"
}

@INPROCEEDINGS{Carletta2006-la,
  title     = "The {AMI} Meeting Corpus: A Pre-announcement",
  author    = "Carletta, Jean and Ashby, Simone and Bourban, Sebastien and
               Flynn, Mike and Guillemot, Mael and Hain, Thomas and Kadlec,
               Jaroslav and Karaiskos, Vasilis and Kraaij, Wessel and
               Kronenthal, Melissa and Lathoud, Guillaume and Lincoln, Mike and
               Lisowska, Agnes and McCowan, Iain and Post, Wilfried and Reidsma,
               Dennis and Wellner, Pierre",
  booktitle = "Machine Learning for Multimodal Interaction",
  publisher = "Springer Berlin Heidelberg",
  pages     = "28--39",
  abstract  = "The AMI Meeting Corpus is a multi-modal data set consisting of
               100 hours of meeting recordings. It is being created in the
               context of a project that is developing meeting browsing
               technology and will eventually be released publicly. Some of the
               meetings it contains are naturally occurring, and some are
               elicited, particularly using a scenario in which the participants
               play different roles in a design team, taking a design project
               from kick-off to completion over the course of a day. The corpus
               is being recorded using a wide range of devices including
               close-talking and far-field microphones, individual and room-view
               video cameras, projection, a whiteboard, and individual pens, all
               of which produce output signals that are synchronized with each
               other. It is also being hand-annotated for many different
               phenomena, including orthographic transcription, discourse
               properties such as named entities and dialogue acts, summaries,
               emotions, and some head and hand gestures. We describe the data
               set, including the rationale behind using elicited material, and
               explain how the material is being recorded, transcribed and
               annotated.",
  year      =  2006,
  url       = "http://dx.doi.org/10.1007/11677482_3",
  keywords  = "WP3;Datasets",
  doi       = "10.1007/11677482\_3",
  annote    = "[TLT - Stefano Perna] The AMI Meeting Corpus is a multi-modal
               data set consisting of 100 hours of meeting recordings.
               Annotation: orthographic transcription, annotations for many
               different phenomena (dialog acts, head movement etc. )."
}

@INPROCEEDINGS{Hukkelas2019-wl,
  title     = "{DeepPrivacy}: A Generative Adversarial Network for Face
               Anonymization",
  author    = "Hukkelås, Håkon and Mester, Rudolf and Lindseth, Frank",
  booktitle = "Advances in Visual Computing",
  publisher = "Springer International Publishing",
  pages     = "565--578",
  abstract  = "We propose a novel architecture which is able to automatically
               anonymize faces in images while retaining the original data
               distribution. We ensure total anonymization of all faces in an
               image by generating images exclusively on privacy-safe
               information. Our model is based on a conditional generative
               adversarial network, generating images considering the original
               pose and image background. The conditional information enables us
               to generate highly realistic faces with a seamless transition
               between the generated face and the existing background.
               Furthermore, we introduce a diverse dataset of human faces,
               including unconventional poses, occluded faces, and a vast
               variability in backgrounds. Finally, we present experimental
               results reflecting the capability of our model to anonymize
               images while preserving the data distribution, making the data
               suitable for further training of deep learning models. As far as
               we know, no other solution has been proposed that guarantees the
               anonymization of faces while generating realistic images.",
  year      =  2019,
  url       = "http://dx.doi.org/10.1007/978-3-030-33720-9_44",
  keywords  = "WP4",
  doi       = "10.1007/978-3-030-33720-9\_44",
  annote    = "[ITU - Mustafa İzzet Muştu] They detect faces using DSFD and
               estimate face landmarks using a modified Mask R-CNN. They use a
               customized U-Net architecture in the GAN to generate images."
}

@INCOLLECTION{Nicmanis2024-mt,
  title     = "A multi-layered approach to evaluating speech translation
               performance of meetings",
  author    = "Nicmanis, Dāvis and Bergmanis, Toms and Salimbajevs, Askars and
               Pinnis, Mārcis",
  booktitle = "Lecture Notes in Networks and Systems",
  publisher = "Springer Nature Switzerland",
  address   = "Cham",
  pages     = "128--137",
  series    = "Lecture notes in networks and systems",
  year      =  2024,
  url       = "http://dx.doi.org/10.1007/978-3-031-73110-5_10",
  doi       = "10.1007/978-3-031-73110-5\_10",
  isbn      = "9783031731099,9783031731105",
  issn      = "2367-3370,2367-3389",
  language  = "en",
  annote    = "[KIT - Maike Züfle] This paper present a multi-layer evaluation
               suite for automatic speechtranslation of meetings. They further
               present how to use the data sets and annotations to evaluate
               components involved in cascaded speech translation systems:
               speaker diarisation, speech segmentation, automatic speech
               recognition, punctuation restoration and sentence splitting,
               speech normalisation, and machine translation.This is especially
               interesting for us, since it targets exactly our usecase
               (meetings)."
}

@INPROCEEDINGS{Zhong2023-fi,
  title     = "Identity-Preserving Talking Face Generation with Landmark and
               Appearance Priors",
  author    = "Zhong, Weizhi and Fang, Chaowei and Cai, Yinqi and Wei, Pengxu
               and Zhao, Gangming and Lin, Liang and Li, Guanbin",
  booktitle = "2023 IEEE/CVF Conference on Computer Vision and Pattern
               Recognition (CVPR)",
  publisher = "IEEE",
  pages     = "9729--9738",
  abstract  = "Generating talking face videos from audio attracts lots of
               research interest. A few person-specific methods can generate
               vivid videos but require the target speaker's videos for training
               or fine-tuning. Existing person-generic methods have difficulty
               in generating realistic and lip-synced videos while preserving
               identity information. To tackle this problem, we propose a
               two-stage framework consisting of audio-to-landmark generation
               and landmark-to-video rendering procedures. First, we devise a
               novel Transformer-based landmark generator to infer lip and jaw
               landmarks from the audio. Prior landmark characteristics of the
               speaker's face are employed to make the generated landmarks
               coincide with the facial outline of the speaker. Then, a video
               rendering model is built to translate the generated landmarks
               into face images. During this stage, prior appearance information
               is extracted from the lower-half occluded target face and static
               reference images, which helps generate realistic and
               identity-preserving visual content. For effectively exploring the
               prior information of static reference images, we align static
               reference images with the target face's pose and expression based
               on motion fields. Moreover, auditory features are reused to
               guarantee that the generated face images are well synchronized
               with the audio. Extensive experiments demonstrate that our method
               can produce more realistic, lip-synced, and identity-preserving
               videos than existing person-generic talking face generation
               methods.",
  month     =  jun,
  year      =  2023,
  url       = "http://dx.doi.org/10.1109/CVPR52729.2023.00938",
  keywords  = "WP4",
  doi       = "10.1109/CVPR52729.2023.00938",
  issn      = "2575-7075,1063-6919",
  annote    = "[ITU - Ziya Ata Yazıcı] Identity preserving landmark-based lip
               resynchronization for downstream tasks."
}

@INPROCEEDINGS{Klemp2023-sx,
  title     = "{LDFA}: Latent Diffusion Face Anonymization for Self-driving
               Applications",
  author    = "Klemp, Marvin and Rösch, Kevin and Wagner, Royden and Quehl,
               Jannik and Lauer, Martin",
  booktitle = "2023 IEEE/CVF Conference on Computer Vision and Pattern
               Recognition Workshops (CVPRW)",
  publisher = "IEEE",
  pages     = "3199--3205",
  abstract  = "In order to protect vulnerable road users (VRUs), such as
               pedestrians or cyclists, it is essential that intelligent
               transportation systems (ITS) accurately identify them. Therefore,
               datasets used to train perception models of ITS must contain a
               significant number of vulnerable road users. However, data
               protection regulations require that individuals are anonymized in
               such datasets. In this work, we introduce a novel deep
               learning-based pipeline for face anonymization in the context of
               ITS. In contrast to related methods, we do not use generative
               adversarial networks (GANs) but build upon recent advances in
               diffusion models. We propose a two-stage method, which contains a
               face detection model followed by a latent diffusion model to
               generate realistic face in-paintings. To demonstrate the
               versatility of anonymized images, we train segmentation methods
               on anonymized data and evaluate them on non-anonymized data. Our
               experiments reveal that our pipeline is better suited to
               anonymize data for segmentation than naive methods and performes
               comparably with recent GAN-based methods. Moreover, face
               detectors achieve higher mAP scores for faces anonymized by our
               method compared to naive or recent GAN-based methods. Code is
               available at
               https://github.com/KITMRT/latent\_diffusion\_face\_anonymization.",
  month     =  jun,
  year      =  2023,
  url       = "http://dx.doi.org/10.1109/CVPRW59228.2023.00322",
  keywords  = "WP4",
  doi       = "10.1109/CVPRW59228.2023.00322",
  issn      = "2160-7516,2160-7508",
  annote    = "[ITU - Mustafa İzzet Muştu] The system works in two steps: first,
               it finds faces with RetinaFace and then replaces them with
               generated face images from stable diffusion."
}

@INPROCEEDINGS{Ma2024-gs,
  title     = "Extending Whisper with Prompt Tuning to Target-Speaker {ASR}",
  author    = "Ma, Hao and Peng, Zhiyuan and Shao, Mingjie and Li, Jing and Liu,
               Ju",
  booktitle = "ICASSP 2024 - 2024 IEEE International Conference on Acoustics,
               Speech and Signal Processing (ICASSP)",
  publisher = "IEEE",
  pages     = "12516--12520",
  abstract  = "Target-speaker automatic speech recognition (ASR) aims to
               transcribe the desired speech of a target speaker from
               multi-talker overlapped utterances. Most of the existing
               target-speaker ASR (TS-ASR) methods involve either training from
               scratch or fully finetuning a pre-trained model, leading to
               significant training costs and becoming inapplicable to large
               foundation models. This work leverages prompt tuning, a
               parameter-efficient fine-tuning approach, to extend Whisper, a
               large-scale single-talker ASR model, to TS-ASR. Variants of
               prompt tuning approaches along with their configurations are
               explored and optimized for TS-ASR. Experimental results show that
               prompt tuning can achieve performance comparable to
               state-of-the-art full training approaches while only requiring
               about 1\% of task-specific model parameters. Notably, the
               original Whisper’s features, such as inverse text normalization
               and timestamp tagging, are retained in target-speaker ASR,
               keeping the generated transcriptions natural and informative.",
  month     =  "14~" # apr,
  year      =  2024,
  url       = "http://dx.doi.org/10.1109/ICASSP48485.2024.10447492",
  keywords  = "Models",
  doi       = "10.1109/ICASSP48485.2024.10447492",
  issn      = "2379-190X,1520-6149",
  annote    = "[KIT - Supriti Sinhamahapatra] this paper, aims to transcribe the
               desired speech of a target speaker from multi-talker overlapped
               utterances."
}

@INPROCEEDINGS{Hukkelas2023-wj,
  title     = "{DeepPrivacy2}: Towards Realistic Full-Body Anonymization",
  author    = "Hukkelås, Håkon and Lindseth, Frank",
  booktitle = "2023 IEEE/CVF Winter Conference on Applications of Computer
               Vision (WACV)",
  publisher = "IEEE",
  pages     = "1329--1338",
  abstract  = "Generative Adversarial Networks (GANs) are widely adopted for
               anonymization of human figures. However, current state-of-the-art
               limits anonymization to the task of face anonymization. In this
               paper, we propose a novel anonymization framework (DeepPrivacy2)
               for realistic anonymization of human figures and faces. We
               introduce a new large and diverse dataset for full-body
               synthesis, which significantly improves image quality and
               diversity of generated images. Furthermore, we propose a
               style-based GAN that produces high-quality, diverse, and editable
               anonymizations. We demonstrate that our full-body anonymization
               framework provides stronger privacy guarantees than previously
               proposed methods. Source code and appendix is available at:
               github.com/hukkelas/deep\_privacy2.",
  month     =  jan,
  year      =  2023,
  url       = "http://dx.doi.org/10.1109/WACV56688.2023.00138",
  keywords  = "WP4",
  doi       = "10.1109/WACV56688.2023.00138",
  isbn      = "9781665493468,9781665493475",
  issn      = "2642-9381,2472-6737",
  annote    = "[ITU - Mustafa İzzet Muştu] They propose an architecture that can
               replace faces as well as bodies with synthetic images. They
               detect and segment faces or whole bodies using three types of
               detectors. DSFD is used for face detection, Continuous Surface
               Embeddings (CSE), and Mask-RCNN are used for segmentation.
               Synthetic data is generated by a style-based GAN."
}

@INPROCEEDINGS{Blattmann2023-lu,
  title     = "Align your latents: High-resolution video synthesis with latent
               diffusion models",
  author    = "Blattmann, Andreas and Rombach, Robin and Ling, Huan and
               Dockhorn, Tim and Kim, Seung Wook and Fidler, Sanja and Kreis,
               Karsten",
  booktitle = "2023 IEEE/CVF Conference on Computer Vision and Pattern
               Recognition (CVPR)",
  publisher = "IEEE",
  month     =  jun,
  year      =  2023,
  url       = "http://dx.doi.org/10.1109/cvpr52729.2023.02161",
  keywords  = "WP3",
  doi       = "10.1109/cvpr52729.2023.02161",
  annote    = "[ITU - Izzet Mustu] They insert temporal layers in pretrained LDM
               to turn it to a video generator. They freeze spatial layers and
               train only temporal layers."
}

@ARTICLE{Lee2025-to,
  title    = "Behavior-{SD}: Behaviorally aware spoken dialogue generation with
              large language models",
  author   = "Lee, Sehun and Kim, Kang-Wook and Kim, Gunhee",
  journal  = "North Am Chapter Assoc Comput Linguistics",
  pages    = "9574--9593",
  year     =  2025,
  url      = "https://aclanthology.org/2025.naacl-long.484",
  keywords = "WP5",
  doi      = "10.18653/v1/2025.naacl-long.484",
  annote   = "[KIT - Maike Züfle] Spoken dialogue involves behaviors like
              turn-taking, interruptions, filler words, and backchannels, which
              make interactions more natural and engaging but are often
              overlooked in language models. These models struggle to explicitly
              model these behavioral traits, resulting in a less natural and
              personalized communication style that aligns with user needs. This
              paper introduces a large spoken dialogue dataset (2164 hours),
              that we could possibly use in WP5 to show that our model aligns
              with user's needs."
}

@ARTICLE{Zhe_Chen_Weiyun_Wang_Hao_Tian_Shenglong_Ye_Zhangwei_Gao_Erfei_Cui_Wenwen_Tong_Kongzhi_Hu_Jiapeng_Luo_Zheng_Ma_Ji_Ma_Jiaqi_Wang_Xiaoyi_Dong_Hang_Yan_Hewei_Guo_Conghui_He_Botian_Shi_Zhenjiang_Jin_Chao_Xu_Bin_Wang_Xingjian_Wei_Wei_Li_Wenjian_Zhang_Bo_Zhang_Pinlong_Cai_Licheng_Wen_Xiangchao_Yan_Min_Dou_Lewei_Lu_Xizhou_Zhu_Tong_Lu_Dahua_Lin_Yu_Qiao_Jifeng_Dai_Wenhai_Wang2024-ns,
  title    = "How Far Are We to {GPT}-{4V}? Closing the Gap to Commercial
              Multimodal Models with Open-Source Suites",
  author   = "{Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao,
              Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, Ji Ma,
              Jiaqi Wang, Xiaoyi Dong, Hang Yan, Hewei Guo, Conghui He, Botian
              Shi, Zhenjiang Jin, Chao Xu, Bin Wang, Xingjian Wei, Wei Li,
              Wenjian Zhang, Bo Zhang, Pinlong Cai, Licheng Wen, Xiangchao Yan,
              Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng
              Dai, Wenhai Wang}",
  journal  = "arxiv",
  abstract = "In this report, we introduce InternVL 1.5, an open-source
              multimodal large language model (MLLM) to bridge the capability
              gap between open-source and proprietary commercial models in
              multimodal understanding. We introduce three simple improvements:
              (1) Strong Vision Encoder: we explored a continuous learning
              strategy for the large-scale vision foundation model --
              InternViT-6B, boosting its visual understanding capabilities, and
              making it can be transferred and reused in different LLMs. (2)
              Dynamic High-Resolution: we divide images into tiles ranging from
              1 to 40 of 448×448 pixels according to the aspect ratio and
              resolution of the input images, which supports up to 4K resolution
              input. (3) High-Quality Bilingual Dataset: we carefully collected
              a high-quality bilingual dataset that covers common scenes,
              document images, and annotated them with English and Chinese
              question-answer pairs, significantly enhancing performance in OCR-
              and Chinese-related tasks. We evaluate InternVL 1.5 through a
              series of benchmarks and comparative studies. Compared to both
              open-source and proprietary models, InternVL 1.5 shows competitive
              performance, achieving state-of-the-art results in 8 of 18
              benchmarks. Code has been released at this https URL.",
  month    =  "25~" # apr,
  year     =  2024,
  url      = "https://arxiv.org/abs/2404.16821",
  doi      = "10.48550",
  annote   = "[ KIT - Carlos Mullov ] A multimodal (vision-text) language model
              based on dynamic high resolution, stronger vision encoder, and
              higher quality data."
}

@ARTICLE{Chameleon_Team2024-sm,
  title    = "Chameleon: Mixed-Modal Early-Fusion Foundation Models",
  author   = "{Chameleon Team}",
  journal  = "arXiv",
  abstract = "We present Chameleon, a family of early-fusion token-based
              mixed-modal models capable of understanding and generating images
              and text in any arbitrary sequence. We outline a stable training
              approach from inception, an alignment recipe, and an architectural
              parameterization tailored for the early-fusion, token-based,
              mixed-modal setting. The models are evaluated on a comprehensive
              range of tasks, including visual question answering, image
              captioning, text generation, image generation, and long-form mixed
              modal generation. Chameleon demonstrates broad and general
              capabilities, including state-of-the-art performance in image
              captioning tasks, outperforms Llama-2 in text-only tasks while
              being competitive with models such as Mixtral 8x7B and Gemini-Pro,
              and performs non-trivial image generation, all in a single model.
              It also matches or exceeds the performance of much larger models,
              including Gemini Pro and GPT-4V, according to human judgments on a
              new long-form mixed-modal generation evaluation, where either the
              prompt or outputs contain mixed sequences of both images and text.
              Chameleon marks a significant step forward in a unified modeling
              of full multimodal documents.",
  month    =  "16~" # may,
  year     =  2024,
  url      = "http://dx.doi.org/10.48550",
  keywords = "WP3;Models",
  doi      = "10.48550",
  annote   = "[KIT - Carlos Mullov] A new series of mixed modality (vision+text)
              foundation models trained by Meta. Somewhat similar to AnyGPU, it
              works by tokenizing images and using a unified token space for
              image and text tokens. Unlike models such as LLaVA it can thus
              also generate images in the output. The authors aren't clear about
              this, but they don't seem don't seem to distribute the weights."
}

@MISC{Sameer_undated-qg,
  title    = "{ZeroST}: Zero-Shot Speech Translation",
  author   = "Sameer, Khurana and Chiori, Hori and Antoine, Laurent and Gordon,
              Wichern and Jonathan, Le Roux",
  url      = "https://www.merl.com/publications/docs/TR2024-122.pdf",
  keywords = "Models;WP3;WP4",
  annote   = "[FBK - Sara Papi] Paper on creating an SFM+LLM architecture for
              speech translation in zero-shot by avoiding specific finetuning
              for enabling translation between languages. Whisper and MMS are
              used as SFM, NLLB as LLM, and Q-Former as adapter."
}

@INPROCEEDINGS{Han2023-fr,
  title     = "{CHAMPAGNE}: Learning Real-world Conversation from Large-Scale
               Web Videos",
  author    = "Han, Seungju and Hessel, Jack and Dziri, Nouha and Choi, Yejin
               and Yu, Youngjae",
  booktitle = "Proceedings of the IEEE/CVF International Conference on Computer
               Vision",
  pages     = "15498--15509",
  year      =  2023,
  url       = "http://openaccess.thecvf.com/content/ICCV2023/html/Han_CHAMPAGNE_Learning_Real-world_Conversation_from_Large-Scale_Web_Videos_ICCV_2023_paper.html",
  keywords  = "Datasets;WP3",
  annote    = "[TAUS - Lisa Vasileva] Describes training a multimodal generative
               model (CHAMPAGNE) that includes visual context (visual
               groundedness) for conversation tasks, and reports improvement
               over a number of evaluation metrics for conversation-related
               tasks.Additionally, describes the pipeline for collecting the
               dataset (YTD-18M) that was used to train CHAMPAGNE."
}

@MISC{DefossezUnknown-hc,
  title        = "Moshi: a speech-text foundation model for real-time dialogue",
  author       = "Defossez, Alexandre and Mazare, Laurent",
  howpublished = "\url{https://kyutai.org/Moshi.pdf}",
  note         = "Accessed: 2024-9-27",
  annote       = "[TLT - Francesco Verdini] Technical Report of a speech-text
                  foundational model. Novel approaches for both audio in input
                  and audio in output. Demo available online."
}

@MISC{Puvvada_undated-ry,
  title        = "{NVIDIA} {NeMo} Canary",
  author       = "Puvvada*', ['krishna C and Żelasko*', 'piotr and Huang*', 'he
                  and Hrinchuk*', 'oleksii and Koluguri*', 'nithin Rao and
                  Majumdar', 'somshubra and Rastorgueva', 'elena and Dhawan',
                  'kunal and Chen', 'zhehuai and Larukhin', 'vitaly and Balam',
                  'jagadeesh and Ginsburg'], 'boris",
  abstract     = "State of the Art Speech Recognition and Translation",
  howpublished = "\url{https://nvidia.github.io/NeMo/blogs/2024/2024-02-canary/}",
  note         = "Accessed: 2024-2-27",
  keywords     = "Models;WP3;WP4"
}

@INPROCEEDINGS{Jain2024-tm,
  title     = "Vcoder: Versatile vision encoders for multimodal large language
               models",
  author    = "Jain, Jitesh and Yang, Jianwei and Shi, Humphrey",
  booktitle = "Proceedings of the IEEE/CVF Conference on Computer Vision and
               Pattern Recognition",
  pages     = "27992--28002",
  year      =  2024,
  keywords  = "WP3;Models",
  annote    = "[KIT - Irem Eyiokur] The paper proposes using Versatile vision
               encoders (VCoder) as adapters to LLaVA-1.5 to improve the object
               perception capabilities of Multimodal Large Language Models
               (MLLMs), which often struggle with tasks like identifying or
               counting entities in images."
}

@MISC{Banks2024-ca,
  title        = "Gemma: Introducing new state-of-the-art open models",
  author       = "Banks, Jeanine",
  booktitle    = "Google",
  abstract     = "Gemma is a family of lightweight, state-of-the art open models
                  built from the same research and technology used to create the
                  Gemini models.",
  month        =  "21~" # feb,
  year         =  2024,
  howpublished = "\url{https://blog.google/technology/developers/gemma-open-models/}",
  note         = "Accessed: 2024-2-27",
  keywords     = "Models;WP3",
  language     = "en"
}

@ARTICLE{Yaman2023-iz,
  title     = "Plug the Leaks: Advancing Audio-driven Talking Face Generation by
               Preventing Unintended Information Flow",
  author    = "Yaman, D and Eyiokur, F I and Bärmann, L and Ekenel, H K and
               {others}",
  journal   = "arXiv preprint arXiv",
  publisher = "arxiv.org",
  abstract  = "… In this paper, we improve audio - driven talking face gen
               eration by identifying problems in … -lip generator and adaptive
               triplet loss to prevent unintended flow of lip and pose
               information …",
  year      =  2023,
  url       = "https://arxiv.org/abs/2307.09368",
  keywords  = "WP4",
  annote    = "[KIT - Irem Eyiokur] Proposes a silent lip face generation module
               and lip synchronized talking face generation model by preventing
               leaking of unintended information. Works with generalized data,
               unseen faces, and various languages"
}

@MISC{Ma_undated-pn,
  title       = "{SLAM}-{LLM}: Speech, Language, Audio, Music Processing with
                 Large Language Model",
  author      = "Ma, Ziyang",
  institution = "Github",
  abstract    = "Speech, Language, Audio, Music Processing with Large Language
                 Model - ddlBoJack/SLAM-LLM",
  url         = "https://github.com/ddlBoJack/SLAM-LLM",
  keywords    = "WP3",
  language    = "en",
  annote      = "[FBK - Sara Papi] A deep learning toolkit that allows
                 researchers and developers to train custom multimodal large
                 language model (MLLM), focusing on Speech, Language, Audio,
                 Music processing."
}

@ARTICLE{Samuele-Cornell-Jordan-Darefsky-Zhiyao-Duan-Shinji-WatanabeUnknown-fh,
  title    = "Generating Data with Text-to-Speech and Large-Language Models for
              Conversational Speech Recognition",
  author   = "{Samuele Cornell, Jordan Darefsky, Zhiyao Duan, Shinji Watanabe}",
  abstract = "Currently, a common approach in many speech processing tasks is to
              leverage large scale pre-trained models by fine-tuning them on
              in-domain data for a particular application. Yet obtaining even a
              small amount of such data can be problematic, especially for
              sensitive domains and conversational speech scenarios, due to both
              privacy issues and annotation costs. To address this, synthetic
              data generation using single speaker datasets has been employed.
              Yet, for multi-speaker cases, such an approach often requires
              extensive manual effort and is prone to domain mismatches. In this
              work, we propose a synthetic data generation pipeline for
              multi-speaker conversational ASR, leveraging a large language
              model (LLM) for content creation and a conversational
              multi-speaker text-to-speech (TTS) model for speech synthesis. We
              conduct evaluation by fine-tuning the Whisper ASR model for
              telephone and distant conversational speech settings, using both
              in-domain data and generated synthetic data. Our results show that
              the proposed method is able to significantly outperform classical
              multi-speaker generation approaches that use external,
              non-conversational speech datasets.",
  url      = "https://arxiv.org/abs/2408.09215",
  keywords = "WP3;Datasets",
  annote   = "A method to apply LLM to generate TTS data."
}

@INPROCEEDINGS{Cappellazzo2024-gs,
  title    = "Continual Contrastive Spoken Language Understanding",
  author   = "Cappellazzo, Umberto and Fini, Enrico and Yang, Muqiao and
              Falavigna, Daniele and Brutti, Alessio and Raj, Bhiksha",
  year     =  2024,
  url      = "https://arxiv.org/abs/2310.02699",
  keywords = "WP3;WP4",
  annote   = "ACL 2024"
}

@ARTICLE{Norelli2024-ui,
  title   = "{ASIF}: Coupled data turns unimodal models to multimodal without
             training",
  author  = "Norelli, Antonio and Fumero, Marco and Maiorca, Valentino and
             Moschella, Luca and Rodola, Emanuele and Locatello, Francesco",
  journal = "Adv. Neural Inf. Process. Syst.",
  volume  =  36,
  year    =  2024,
  url     = "https://proceedings.neurips.cc/paper_files/paper/2023/hash/3186591903d9db31770ad131adb5ceb4-Abstract-Conference.html",
  issn    = "1049-5258"
}

@MISC{Snowflake_AI_Research2024-yo,
  title        = "Snowflake Arctic - {LLM} for Enterprise {AI}",
  author       = "{Snowflake AI Research}",
  booktitle    = "Snowflake",
  abstract     = "Snowflake Arctic from the Snowflake AI research team is a
                  top-tier enterprise LLM that pushes the frontiers of
                  cost-effective training and openness.",
  month        =  "24~" # apr,
  year         =  2024,
  howpublished = "\url{https://www.snowflake.com/blog/arctic-open-efficient-foundation-language-models-snowflake}",
  note         = "Accessed: 2024-5-3",
  keywords     = "Models;WP3",
  language     = "en",
  annote       = "[FBK - Sara Papi] an open-source LLM (Apache 2.0 license.)
                  that uses a unique Dense-MoE Hybrid transformer architecture;
                  performs on par with Llama3 70B in enterprise metrics like
                  coding (HumanEval+ \& MBPP+), SQL (Spider) and instruction
                  following (IFEval); claims to use 17x less compute budget than
                  Llama 3 70B; the training compute is roughly under \$2 million
                  (less than 3K GPU weeks)."
}

@MISC{undated-jk,
  title       = "speech-trident: Awesome speech/audio {LLMs}, representation
                 learning, and codec models",
  author      = "(張凱爲), Kai-Wei Chang",
  institution = "Github",
  abstract    = "Awesome speech/audio LLMs, representation learning, and codec
                 models - ga642381/speech-trident",
  url         = "https://github.com/ga642381/speech-trident",
  keywords    = "WP3;Models",
  language    = "en",
  annote      = "[FBK - Sara Papi] Collection of speech/audio large language
                 models divided in three areas: (1) representation learning, (2)
                 neural codec, and (3) language models."
}

@ARTICLE{StapUnknown-vq,
  title    = "The Effect of Language Diversity When Fine-Tuning Large Language
              Models for Translation",
  author   = "Stap, David and Monz, Christof",
  abstract = "Prior research diverges on language diversity in LLM fine-tuning:
              Some studies report benefits while others find no advantages.
              Through controlled fine-tuning experiments across 132 translation
              directions, we systematically resolve these disparities. We find
              that expanding language diversity during fine-tuning improves
              translation quality for both unsupervised and -- surprisingly --
              supervised pairs, despite less diverse models being fine-tuned
              exclusively on these supervised pairs. However, benefits plateau
              or decrease beyond a certain diversity threshold. We show that
              increased language diversity creates more language-agnostic
              representations. These representational adaptations help explain
              the improved performance in models fine-tuned with greater
              diversity.",
  keywords = "WP4",
  annote   = "[TAUS, Amir Soleimani] This paper shows how language diversity
              improve performance. This is for translation but similar pattern
              should be seen on different applications."
}

@INPROCEEDINGS{Li2023-yp,
  title     = "{LoftQ}: {LoRA}-Fine-Tuning-Aware Quantization for Large Language
               Models",
  author    = "Li, Yixiao and Yu, Yifan and Liang, Chen and He, Pengcheng and
               Karampatziakis, Nikos and Chen, Weizhu and Zhao, Tuo",
  booktitle = "ICLR 2024",
  abstract  = "Quantization is an indispensable technique for serving Large
               Language Models (LLMs) and has recently found its way into LoRA
               fine-tuning. In this work we focus on the scenario where
               quantization and LoRA fine-tuning are applied together on a
               pre-trained model. In such cases it is common to observe a
               consistent gap in the performance on […]",
  month     =  "11~" # oct,
  year      =  2023,
  url       = "https://www.microsoft.com/en-us/research/publication/loftq-lora-fine-tuning-aware-quantization-for-large-language-models/",
  keywords  = "WP4",
  language  = "en",
  annote    = "[FBK - Sara Papi] LoftQ, a technique that streamlines the
               fine-tuning process in LLMs. LoftQ combines ideas from methods
               such as LoRA or QLora with techniques like quantization and
               adaptive initialization to build a highly optimal fine-tuning
               process."
}

@MISC{Nanotron-ResearchUnknown-zd,
  title        = "The Ultra-Scale Playbook: Training {LLMs} on {GPU} Clusters",
  author       = "{Nanotron Research}",
  howpublished = "\url{https://huggingface.co/spaces/nanotron/ultrascale-playbook}",
  keywords     = "WP3;WP4",
  annote       = "[CYF - Szymon Mazurek] A technical report on distributed
                  computing specifically from the perspective of training LLMs -
                  a ton of knowledge."
}

@INPROCEEDINGS{Meyer2020-om,
  title     = "Artie Bias Corpus: An Open Dataset for Detecting Demographic Bias
               in Speech Applications",
  author    = "Meyer, Josh and Rauchenstein, Lindy and Eisenberg, Joshua D and
               Howell, Nicholas",
  editor    = "Calzolari, Nicoletta and Béchet, Frédéric and Blache, Philippe
               and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry
               and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and
               Mariani, Joseph and Mazo, Hélène and Moreno, Asuncion and Odijk,
               Jan and Piperidis, Stelios",
  booktitle = "Proceedings of the Twelfth Language Resources and Evaluation
               Conference",
  publisher = "European Language Resources Association",
  address   = "Marseille, France",
  pages     = "6462--6468",
  abstract  = "We describe the creation of the Artie Bias Corpus, an English
               dataset of expert-validated \textlessaudio,
               transcript\textgreater pairs with demographic tags for age,
               gender, accent. We also release open software which may be used
               with the Artie Bias Corpus to detect demographic bias in
               Automatic Speech Recognition systems, and can be extended to
               other speech technologies. The Artie Bias Corpus is a curated
               subset of the Mozilla Common Voice corpus, which we release under
               a Creative Commons CC0 license -- the most open and permissive
               license for data. This article contains information on the
               criteria used to select and annotate the Artie Bias Corpus in
               addition to experiments in which we detect and attempt to
               mitigate bias in end-to-end speech recognition models. We we
               observe a significant accent bias in our baseline DeepSpeech
               model, with more accurate transcriptions of US English compared
               to Indian English. We do not, however, find evidence for a
               significant gender bias. We then show significant improvements on
               individual demographic groups from fine-tuning.",
  month     =  may,
  year      =  2020,
  url       = "https://aclanthology.org/2020.lrec-1.796",
  keywords  = "Datasets",
  language  = "English",
  annote    = "[TAUS - Lisa Vasileva] Creation of an English speech dataset for
               bias detection.Filtered subset of CommonVoice, annotated for
               gender, age and accent (manual expert annotation). Demonstrated
               the corpus' ability to detect bias in ASR models."
}

@MISC{Pandey2024-fy,
  title        = "{BharatGPT} Aims to Become Meta of Indic {LLMs}",
  author       = "Pandey, Mohit",
  booktitle    = "Analytics India Magazine",
  abstract     = "BharatGPT researchers know the power of open source and have
                  plans to release a bunch of models initially for the developer
                  community.",
  month        =  "23~" # jan,
  year         =  2024,
  howpublished = "\url{https://analyticsindiamag.com/bharatgpt-aims-to-become-meta-of-indic-llms/}",
  note         = "Accessed: 2024-2-27",
  language     = "en"
}

@MISC{Xiong_undated-oy,
  title        = "Introducing The Foundation Model Transparency Index",
  author       = "Xiong, Betty and Zhang, Daniel",
  booktitle    = "Stanford HAI",
  abstract     = "A new index rates the transparency of 10 foundation model
                  companies and finds them lacking.",
  howpublished = "\url{https://hai.stanford.edu/news/introducing-foundation-model-transparency-index}",
  note         = "Accessed: 2024-2-27",
  language     = "en"
}

@MISC{noauthor_undated-zf,
  title        = "Revisiting Feature Prediction for Learning Visual
                  Representations from Video",
  abstract     = "This paper explores feature prediction as a stand-alone
                  objective for unsupervised learning from video and introduces
                  V-JEPA, a collection of vision...",
  howpublished = "\url{https://ai.meta.com/research/publications/revisiting-feature-prediction-for-learning-visual-representations-from-video/}",
  note         = "Accessed: 2024-2-27",
  keywords     = "Models",
  language     = "en"
}

@MISC{noauthor_undated-qk,
  title        = "Empathic Voice Interface ({EVI})",
  abstract     = "Hume's Empathic Voice Interface (EVI) is the world’s first
                  emotionally intelligent voice AI.",
  howpublished = "\url{https://dev.hume.ai/docs/empathic-voice-interface-evi/overview}",
  note         = "Accessed: 2024-4-2",
  keywords     = "WP3",
  language     = "en",
  annote       = "[TLT - Stefano Perna]"
}

@MISC{UnknownUnknown-os,
  title        = "deepseek-ai/{DeepSeek}-{R1} · Hugging Face",
  abstract     = "We’re on a journey to advance and democratize artificial
                  intelligence through open source and open science.",
  howpublished = "\url{https://huggingface.co/deepseek-ai/DeepSeek-R1}",
  note         = "Accessed: 2025-1-27",
  keywords     = "Models",
  annote       = "[KIT - Irem Eyiokur] New open-source model (685B) that
                  performs comparable to OpenAI-o1 in reasoning, math and coding
                  tasks."
}

@MISC{noauthor_undated-xw,
  title        = "Accelerating Generative {AI} with {PyTorch} {IV}: Seamless
                  {M4T}, fast",
  booktitle    = "PyTorch",
  abstract     = "This post is the fourth part of a multi-series blog focused on
                  how to accelerate generative AI models with pure, native
                  PyTorch. To skip to the code, check out our github
                  (seamless\_communication, fairseq2). We are excited to share a
                  breadth of newly released PyTorch performance features
                  alongside practical examples to see how far we can push
                  PyTorch native performance. In part one, we showed how to
                  accelerate Segment Anything over 8x using only pure, native
                  PyTorch. In part two, we showed how to accelerate Llama-7B by
                  almost 10x using only native PyTorch optimizations. In part
                  three, we showed how to accelerate text-to-image diffusion
                  models up to 3x using only native Pytorch optimizations.",
  howpublished = "\url{https://pytorch.org/blog/accelerating-generative-ai-4/}",
  note         = "Accessed: 2024-2-27",
  language     = "en"
}
