@ARTICLE{Huang2023-di,
  title         = "{AudioGPT}: Understanding and Generating Speech, Music,
                   Sound, and Talking Head",
  author        = "Huang, Rongjie and Li, Mingze and Yang, Dongchao and Shi,
                   Jiatong and Chang, Xuankai and Ye, Zhenhui and Wu, Yuning
                   and Hong, Zhiqing and Huang, Jiawei and Liu, Jinglin and
                   Ren, Yi and Zhao, Zhou and Watanabe, Shinji",
  abstract      = "Large language models (LLMs) have exhibited remarkable
                   capabilities across a variety of domains and tasks,
                   challenging our understanding of learning and cognition.
                   Despite the recent success, current LLMs are not capable of
                   processing complex audio information or conducting spoken
                   conversations (like Siri or Alexa). In this work, we propose
                   a multi-modal AI system named AudioGPT, which complements
                   LLMs (i.e., ChatGPT) with 1) foundation models to process
                   complex audio information and solve numerous understanding
                   and generation tasks; and 2) the input/output interface
                   (ASR, TTS) to support spoken dialogue. With an increasing
                   demand to evaluate multi-modal LLMs of human intention
                   understanding and cooperation with foundation models, we
                   outline the principles and processes and test AudioGPT in
                   terms of consistency, capability, and robustness.
                   Experimental results demonstrate the capabilities of
                   AudioGPT in solving AI tasks with speech, music, sound, and
                   talking head understanding and generation in multi-round
                   dialogues, which empower humans to create rich and diverse
                   audio content with unprecedented ease. Our system is
                   publicly available at
                   \textbackslashurl\{https://github.com/AIGC-Audio/AudioGPT\}.",
  month         =  apr,
  year          =  2023,
  url           = "http://arxiv.org/abs/2304.12995",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2304.12995"
}

@ARTICLE{Hassid2023-uv,
  title         = "Textually Pretrained Speech Language Models",
  author        = "Hassid, Michael and Remez, Tal and Nguyen, Tu Anh and Gat,
                   Itai and Conneau, Alexis and Kreuk, Felix and Copet, Jade
                   and Defossez, Alexandre and Synnaeve, Gabriel and Dupoux,
                   Emmanuel and Schwartz, Roy and Adi, Yossi",
  abstract      = "Speech language models (SpeechLMs) process and generate
                   acoustic data only, without textual supervision. In this
                   work, we propose TWIST, a method for training SpeechLMs
                   using a warm-start from a pretrained textual language
                   models. We show using both automatic and human evaluations
                   that TWIST outperforms a cold-start SpeechLM across the
                   board. We empirically analyze the effect of different model
                   design choices such as the speech tokenizer, the pretrained
                   textual model, and the dataset size. We find that model and
                   dataset scale both play an important role in constructing
                   better-performing SpeechLMs. Based on our observations, we
                   present the largest (to the best of our knowledge) SpeechLM
                   both in terms of number of parameters and training data. We
                   additionally introduce two spoken versions of the StoryCloze
                   textual benchmark to further improve model evaluation and
                   advance future research in the field. We make speech
                   samples, code and models publicly available:
                   https://pages.cs.huji.ac.il/adiyoss-lab/twist/ .",
  month         =  may,
  year          =  2023,
  url           = "http://arxiv.org/abs/2305.13009",
  keywords      = "WP3",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2305.13009"
}
