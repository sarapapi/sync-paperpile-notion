@INPROCEEDINGS{Tang2024-vi,
  title     = "{SALMONN}: Towards Generic Hearing Abilities for Large Language
               Models",
  booktitle = "The Twelfth International Conference on Learning Representations",
  author    = "Tang, Changli and Yu, Wenyi and Sun, Guangzhi and Chen, Xianzhao
               and Tan, Tian and Li, Wei and Lu, Lu and Ma, Zejun and Zhang,
               Chao",
  abstract  = "Hearing is arguably an essential ability of artificial
               intelligence (AI) agents in the physical world, which refers to
               the perception and understanding of general auditory information
               consisting of at least three types of sounds: speech, audio
               events, and music. In this paper, we propose SALMONN, a speech
               audio language music open neural network, built by integrating a
               pre-trained text-based large language model (LLM) with speech
               and audio encoders into a single multimodal model. SALMONN
               enables the LLM to directly process and understand general audio
               inputs and achieve competitive performances on a number of
               speech and audio tasks used in training, such as automatic
               speech recognition and translation, auditory-information-based
               question answering, emotion recognition, speaker verification,
               and music and audio captioning \textbackslashtextit\{etc.\}
               SALMONN also has a diverse set of emergent abilities unseen in
               the training, which includes but is not limited to speech
               translation to untrained languages, speech-based slot filling,
               spoken-query-based question answering, audio-based storytelling,
               and speech audio co-reasoning \textbackslashtextit\{etc\}. The
               presence of the cross-modal emergent abilities is studied, and a
               novel few-shot activation tuning approach is proposed to
               activate such abilities of SALMONN. To our knowledge, SALMONN is
               the first model of its type and can be regarded as a step
               towards AI with generic hearing abilities. An interactive demo
               of SALMONN is available at
               \textbackslashtexttt\{\textbackslashurl\{https://github.com/bytedance/SALMONN\}\},
               and the training code and model checkpoints will be released
               upon acceptance.",
  year      =  2024,
  url       = "https://openreview.net/forum?id=14rn7HpKVk",
  annote    = "[FBK - Sara Papi] A model combining Speech Foundation Models
               (BEATs and Whisper) and LLM (Vicuna) is proposed. The modality
               matching is realized with a Window-level Q-former. Many
               audio-speech related tasks are performed, including ASR and ST,
               for which examples of templates can be found.",
  keywords  = "WP4;Models"
}

@MISC{Pandey2024-fy,
  title        = "{BharatGPT} Aims to Become Meta of Indic {LLMs}",
  booktitle    = "Analytics India Magazine",
  author       = "Pandey, Mohit",
  abstract     = "BharatGPT researchers know the power of open source and have
                  plans to release a bunch of models initially for the
                  developer community.",
  month        =  jan,
  year         =  2024,
  url          = "https://analyticsindiamag.com/bharatgpt-aims-to-become-meta-of-indic-llms/",
  howpublished = "\url{https://analyticsindiamag.com/bharatgpt-aims-to-become-meta-of-indic-llms/}",
  note         = "Accessed: 2024-2-27",
  language     = "en"
}

@MISC{noauthor_undated-ru,
  title        = "{GPTZero}",
  booktitle    = "{GPTZero}",
  abstract     = "Covered by >100 media outlets, GPTZero is the most advanced
                  AI detector for ChatGPT, GPT-4, Bard. Check up to 50000
                  characters for AI plagiarism in seconds.",
  url          = "https://gptzero.me/",
  howpublished = "\url{https://gptzero.me/}",
  note         = "Accessed: 2024-2-28",
  language     = "en"
}

@MISC{noauthor_undated-xw,
  title        = "Accelerating Generative {AI} with {PyTorch} {IV}: Seamless
                  {M4T}, fast",
  booktitle    = "{PyTorch}",
  abstract     = "This post is the fourth part of a multi-series blog focused
                  on how to accelerate generative AI models with pure, native
                  PyTorch. To skip to the code, check out our github
                  (seamless\_communication, fairseq2). We are excited to share
                  a breadth of newly released PyTorch performance features
                  alongside practical examples to see how far we can push
                  PyTorch native performance. In part one, we showed how to
                  accelerate Segment Anything over 8x using only pure, native
                  PyTorch. In part two, we showed how to accelerate Llama-7B by
                  almost 10x using only native PyTorch optimizations. In part
                  three, we showed how to accelerate text-to-image diffusion
                  models up to 3x using only native Pytorch optimizations.",
  url          = "https://pytorch.org/blog/accelerating-generative-ai-4/",
  howpublished = "\url{https://pytorch.org/blog/accelerating-generative-ai-4/}",
  note         = "Accessed: 2024-2-27",
  language     = "en"
}

@MISC{Puvvada_undated-ry,
  title        = "{NVIDIA} {NeMo} Canary",
  author       = "Puvvada*', ['krishna C and {\.Z}elasko*', 'piotr and Huang*',
                  'he and Hrinchuk*', 'oleksii and Koluguri*', 'nithin Rao and
                  Majumdar', 'somshubra and Rastorgueva', 'elena and Dhawan',
                  'kunal and Chen', 'zhehuai and Larukhin', 'vitaly and Balam',
                  'jagadeesh and Ginsburg'], 'boris",
  abstract     = "State of the Art Speech Recognition and Translation",
  url          = "https://nvidia.github.io/NeMo/blogs/2024/2024-02-canary/",
  howpublished = "\url{https://nvidia.github.io/NeMo/blogs/2024/2024-02-canary/}",
  note         = "Accessed: 2024-2-27",
  keywords     = "Models;WP3;WP4"
}

@MISC{Xiong_undated-oy,
  title        = "Introducing The Foundation Model Transparency Index",
  booktitle    = "Stanford {HAI}",
  author       = "Xiong, Betty and Zhang, Daniel",
  abstract     = "A new index rates the transparency of 10 foundation model
                  companies and finds them lacking.",
  url          = "https://hai.stanford.edu/news/introducing-foundation-model-transparency-index",
  howpublished = "\url{https://hai.stanford.edu/news/introducing-foundation-model-transparency-index}",
  note         = "Accessed: 2024-2-27",
  language     = "en"
}

@MISC{noauthor_undated-zf,
  title        = "Revisiting Feature Prediction for Learning Visual
                  Representations from Video",
  abstract     = "This paper explores feature prediction as a stand-alone
                  objective for unsupervised learning from video and introduces
                  V-JEPA, a collection of vision...",
  url          = "https://ai.meta.com/research/publications/revisiting-feature-prediction-for-learning-visual-representations-from-video/",
  howpublished = "\url{https://ai.meta.com/research/publications/revisiting-feature-prediction-for-learning-visual-representations-from-video/}",
  note         = "Accessed: 2024-2-27",
  keywords     = "Models",
  language     = "en"
}

@MISC{Banks2024-ca,
  title        = "Gemma: Introducing new state-of-the-art open models",
  booktitle    = "Google",
  author       = "Banks, Jeanine",
  abstract     = "Gemma is a family of lightweight, state-of-the art open
                  models built from the same research and technology used to
                  create the Gemini models.",
  month        =  feb,
  year         =  2024,
  url          = "https://blog.google/technology/developers/gemma-open-models/",
  howpublished = "\url{https://blog.google/technology/developers/gemma-open-models/}",
  note         = "Accessed: 2024-2-27",
  keywords     = "Models;WP3",
  language     = "en"
}

@ARTICLE{Ioannis_Tsiamas_and_Gerard_I_Gallego_and_Jose_A_R_Fonollosa_and_Marta_R_Costa-jussa2024-fy,
  title    = "Pushing the Limits of Zero-shot {End-to-End} Speech Translation",
  author   = "{Ioannis Tsiamas and Gerard I. G{\'a}llego and Jos{\'e} A. R.
              Fonollosa and Marta R. Costa-juss{\`a}}",
  abstract = "Data scarcity and the modality gap between the speech and text
              modalities are two major obstacles of end-to-end Speech
              Translation (ST) systems, thus hindering their performance. Prior
              work has attempted to mitigate these challenges by leveraging
              external MT data and optimizing distance metrics that bring
              closer the speech-text representations. However, achieving
              competitive results typically requires some ST data. For this
              reason, we introduce ZeroSwot, a method for zero-shot ST that
              bridges the modality gap without any paired ST data. Leveraging a
              novel CTC compression and Optimal Transport, we train a speech
              encoder using only ASR data, to align with the representation
              space of a massively multilingual MT model. The speech encoder
              seamlessly integrates with the MT model at inference, enabling
              direct translation from speech to text, across all languages
              supported by the MT model. Our experiments show that we can
              effectively close the modality gap without ST data, while our
              results on MuST-C and CoVoST demonstrate our method's superiority
              over not only previous zero-shot models, but also supervised
              ones, achieving state-of-the-art results.",
  journal  = "arXiv",
  year     =  2024,
  url      = "https://arxiv.org/abs/2402.10422",
  doi      = " https://doi.org/10.48550/arXiv.2402.10422"
}

@ARTICLE{Wang2023-gz,
  title         = "Large-scale {Multi-Modal} Pre-trained Models: A
                   Comprehensive Survey",
  author        = "Wang, Xiao and Chen, Guangyao and Qian, Guangwu and Gao,
                   Pengcheng and Wei, Xiao-Yong and Wang, Yaowei and Tian,
                   Yonghong and Gao, Wen",
  abstract      = "With the urgent demand for generalized deep models, many
                   pre-trained big models are proposed, such as BERT, ViT, GPT,
                   etc. Inspired by the success of these models in single
                   domains (like computer vision and natural language
                   processing), the multi-modal pre-trained big models have
                   also drawn more and more attention in recent years. In this
                   work, we give a comprehensive survey of these models and
                   hope this paper could provide new insights and helps fresh
                   researchers to track the most cutting-edge works.
                   Specifically, we firstly introduce the background of
                   multi-modal pre-training by reviewing the conventional deep
                   learning, pre-training works in natural language process,
                   computer vision, and speech. Then, we introduce the task
                   definition, key challenges, and advantages of multi-modal
                   pre-training models (MM-PTMs), and discuss the MM-PTMs with
                   a focus on data, objectives, network architectures, and
                   knowledge enhanced pre-training. After that, we introduce
                   the downstream tasks used for the validation of large-scale
                   MM-PTMs, including generative, classification, and
                   regression tasks. We also give visualization and analysis of
                   the model parameters and results on representative
                   downstream tasks. Finally, we point out possible research
                   directions for this topic that may benefit future works. In
                   addition, we maintain a continuously updated paper list for
                   large-scale pre-trained multi-modal big models:
                   https://github.com/wangxiao5791509/MultiModal\_BigModels\_Survey",
  month         =  feb,
  year          =  2023,
  url           = "http://arxiv.org/abs/2302.10035",
  keywords      = "WP3",
  archivePrefix = "arXiv",
  eprint        = "2302.10035",
  primaryClass  = "cs.CV",
  arxivid       = "2302.10035"
}

@ARTICLE{Thompson2024-gz,
  title         = "A Shocking Amount of the Web is Machine Translated: Insights
                   from {Multi-Way} Parallelism",
  author        = "Thompson, Brian and Dhaliwal, Mehak Preet and Frisch, Peter
                   and Domhan, Tobias and Federico, Marcello",
  abstract      = "We show that content on the web is often translated into
                   many languages, and the low quality of these multi-way
                   translations indicates they were likely created using
                   Machine Translation (MT). Multi-way parallel, machine
                   generated content not only dominates the translations in
                   lower resource languages; it also constitutes a large
                   fraction of the total web content in those languages. We
                   also find evidence of a selection bias in the type of
                   content which is translated into many languages, consistent
                   with low quality English content being translated en masse
                   into many lower resource languages, via MT. Our work raises
                   serious concerns about training models such as multilingual
                   large language models on both monolingual and bilingual data
                   scraped from the web.",
  month         =  jan,
  year          =  2024,
  url           = "http://arxiv.org/abs/2401.05749",
  archivePrefix = "arXiv",
  eprint        = "2401.05749",
  primaryClass  = "cs.CL",
  arxivid       = "2401.05749"
}

@ARTICLE{Hassid2023-uv,
  title         = "Textually Pretrained Speech Language Models",
  author        = "Hassid, Michael and Remez, Tal and Nguyen, Tu Anh and Gat,
                   Itai and Conneau, Alexis and Kreuk, Felix and Copet, Jade
                   and Defossez, Alexandre and Synnaeve, Gabriel and Dupoux,
                   Emmanuel and Schwartz, Roy and Adi, Yossi",
  abstract      = "Speech language models (SpeechLMs) process and generate
                   acoustic data only, without textual supervision. In this
                   work, we propose TWIST, a method for training SpeechLMs
                   using a warm-start from a pretrained textual language
                   models. We show using both automatic and human evaluations
                   that TWIST outperforms a cold-start SpeechLM across the
                   board. We empirically analyze the effect of different model
                   design choices such as the speech tokenizer, the pretrained
                   textual model, and the dataset size. We find that model and
                   dataset scale both play an important role in constructing
                   better-performing SpeechLMs. Based on our observations, we
                   present the largest (to the best of our knowledge) SpeechLM
                   both in terms of number of parameters and training data. We
                   additionally introduce two spoken versions of the StoryCloze
                   textual benchmark to further improve model evaluation and
                   advance future research in the field. We make speech
                   samples, code and models publicly available:
                   https://pages.cs.huji.ac.il/adiyoss-lab/twist/ .",
  month         =  may,
  year          =  2023,
  url           = "http://arxiv.org/abs/2305.13009",
  keywords      = "WP3",
  archivePrefix = "arXiv",
  eprint        = "2305.13009",
  primaryClass  = "cs.CL",
  arxivid       = "2305.13009"
}

@ARTICLE{Huang2023-di,
  title         = "{AudioGPT}: Understanding and Generating Speech, Music,
                   Sound, and Talking Head",
  author        = "Huang, Rongjie and Li, Mingze and Yang, Dongchao and Shi,
                   Jiatong and Chang, Xuankai and Ye, Zhenhui and Wu, Yuning
                   and Hong, Zhiqing and Huang, Jiawei and Liu, Jinglin and
                   Ren, Yi and Zhao, Zhou and Watanabe, Shinji",
  abstract      = "Large language models (LLMs) have exhibited remarkable
                   capabilities across a variety of domains and tasks,
                   challenging our understanding of learning and cognition.
                   Despite the recent success, current LLMs are not capable of
                   processing complex audio information or conducting spoken
                   conversations (like Siri or Alexa). In this work, we propose
                   a multi-modal AI system named AudioGPT, which complements
                   LLMs (i.e., ChatGPT) with 1) foundation models to process
                   complex audio information and solve numerous understanding
                   and generation tasks; and 2) the input/output interface
                   (ASR, TTS) to support spoken dialogue. With an increasing
                   demand to evaluate multi-modal LLMs of human intention
                   understanding and cooperation with foundation models, we
                   outline the principles and processes and test AudioGPT in
                   terms of consistency, capability, and robustness.
                   Experimental results demonstrate the capabilities of
                   AudioGPT in solving AI tasks with speech, music, sound, and
                   talking head understanding and generation in multi-round
                   dialogues, which empower humans to create rich and diverse
                   audio content with unprecedented ease. Our system is
                   publicly available at
                   \textbackslashurl\{https://github.com/AIGC-Audio/AudioGPT\}.",
  month         =  apr,
  year          =  2023,
  url           = "http://arxiv.org/abs/2304.12995",
  keywords      = "Models;WP3",
  archivePrefix = "arXiv",
  eprint        = "2304.12995",
  primaryClass  = "cs.CL",
  arxivid       = "2304.12995"
}

@ARTICLE{Moschella2022-uh,
  title         = "Relative representations enable zero-shot latent space
                   communication",
  author        = "Moschella, Luca and Maiorca, Valentino and Fumero, Marco and
                   Norelli, Antonio and Locatello, Francesco and Rodol{\`a},
                   Emanuele",
  abstract      = "Neural networks embed the geometric structure of a data
                   manifold lying in a high-dimensional space into latent
                   representations. Ideally, the distribution of the data
                   points in the latent space should depend only on the task,
                   the data, the loss, and other architecture-specific
                   constraints. However, factors such as the random weights
                   initialization, training hyperparameters, or other sources
                   of randomness in the training phase may induce incoherent
                   latent spaces that hinder any form of reuse. Nevertheless,
                   we empirically observe that, under the same data and
                   modeling choices, the angles between the encodings within
                   distinct latent spaces do not change. In this work, we
                   propose the latent similarity between each sample and a
                   fixed set of anchors as an alternative data representation,
                   demonstrating that it can enforce the desired invariances
                   without any additional training. We show how neural
                   architectures can leverage these relative representations to
                   guarantee, in practice, invariance to latent isometries and
                   rescalings, effectively enabling latent space communication:
                   from zero-shot model stitching to latent space comparison
                   between diverse settings. We extensively validate the
                   generalization capability of our approach on different
                   datasets, spanning various modalities (images, text,
                   graphs), tasks (e.g., classification, reconstruction) and
                   architectures (e.g., CNNs, GCNs, transformers).",
  month         =  sep,
  year          =  2022,
  url           = "http://arxiv.org/abs/2209.15430",
  archivePrefix = "arXiv",
  eprint        = "2209.15430",
  primaryClass  = "cs.LG",
  arxivid       = "2209.15430"
}

@ARTICLE{Norelli2024-ui,
  title   = "{ASIF}: Coupled data turns unimodal models to multimodal without
             training",
  author  = "Norelli, Antonio and Fumero, Marco and Maiorca, Valentino and
             Moschella, Luca and Rodola, Emanuele and Locatello, Francesco",
  journal = "Adv. Neural Inf. Process. Syst.",
  volume  =  36,
  year    =  2024,
  url     = "https://proceedings.neurips.cc/paper_files/paper/2023/hash/3186591903d9db31770ad131adb5ceb4-Abstract-Conference.html",
  issn    = "1049-5258"
}

@ARTICLE{Zhang2024-eq,
  title         = "{MM-LLMs}: Recent Advances in {MultiModal} Large Language
                   Models",
  author        = "Zhang, Duzhen and Yu, Yahan and Li, Chenxing and Dong,
                   Jiahua and Su, Dan and Chu, Chenhui and Yu, Dong",
  abstract      = "In the past year, MultiModal Large Language Models (MM-LLMs)
                   have undergone substantial advancements, augmenting
                   off-the-shelf LLMs to support MM inputs or outputs via
                   cost-effective training strategies. The resulting models not
                   only preserve the inherent reasoning and decision-making
                   capabilities of LLMs but also empower a diverse range of MM
                   tasks. In this paper, we provide a comprehensive survey
                   aimed at facilitating further research of MM-LLMs.
                   Initially, we outline general design formulations for model
                   architecture and training pipeline. Subsequently, we
                   introduce a taxonomy encompassing $122$ MM-LLMs, each
                   characterized by its specific formulations. Furthermore, we
                   review the performance of selected MM-LLMs on mainstream
                   benchmarks and summarize key training recipes to enhance the
                   potency of MM-LLMs. Finally, we explore promising directions
                   for MM-LLMs while concurrently maintaining a real-time
                   tracking website for the latest developments in the field.
                   We hope that this survey contributes to the ongoing
                   advancement of the MM-LLMs domain.",
  month         =  jan,
  year          =  2024,
  url           = "http://arxiv.org/abs/2401.13601",
  keywords      = "WP3",
  archivePrefix = "arXiv",
  eprint        = "2401.13601",
  primaryClass  = "cs.CL",
  arxivid       = "2401.13601"
}

@ARTICLE{Groeneveld2024-id,
  title         = "{OLMo}: Accelerating the Science of Language Models",
  author        = "Groeneveld, Dirk and Beltagy, Iz and Walsh, Pete and Bhagia,
                   Akshita and Kinney, Rodney and Tafjord, Oyvind and Jha,
                   Ananya Harsh and Ivison, Hamish and Magnusson, Ian and Wang,
                   Yizhong and Arora, Shane and Atkinson, David and Authur,
                   Russell and Chandu, Khyathi Raghavi and Cohan, Arman and
                   Dumas, Jennifer and Elazar, Yanai and Gu, Yuling and Hessel,
                   Jack and Khot, Tushar and Merrill, William and Morrison,
                   Jacob and Muennighoff, Niklas and Naik, Aakanksha and Nam,
                   Crystal and Peters, Matthew E and Pyatkin, Valentina and
                   Ravichander, Abhilasha and Schwenk, Dustin and Shah, Saurabh
                   and Smith, Will and Strubell, Emma and Subramani, Nishant
                   and Wortsman, Mitchell and Dasigi, Pradeep and Lambert,
                   Nathan and Richardson, Kyle and Zettlemoyer, Luke and Dodge,
                   Jesse and Lo, Kyle and Soldaini, Luca and Smith, Noah A and
                   Hajishirzi, Hannaneh",
  abstract      = "Language models (LMs) have become ubiquitous in both NLP
                   research and in commercial product offerings. As their
                   commercial importance has surged, the most powerful models
                   have become closed off, gated behind proprietary interfaces,
                   with important details of their training data,
                   architectures, and development undisclosed. Given the
                   importance of these details in scientifically studying these
                   models, including their biases and potential risks, we
                   believe it is essential for the research community to have
                   access to powerful, truly open LMs. To this end, this
                   technical report details the first release of OLMo, a
                   state-of-the-art, truly Open Language Model and its
                   framework to build and study the science of language
                   modeling. Unlike most prior efforts that have only released
                   model weights and inference code, we release OLMo and the
                   whole framework, including training data and training and
                   evaluation code. We hope this release will empower and
                   strengthen the open research community and inspire a new
                   wave of innovation.",
  month         =  feb,
  year          =  2024,
  url           = "http://arxiv.org/abs/2402.00838",
  keywords      = "WP3;Models",
  archivePrefix = "arXiv",
  eprint        = "2402.00838",
  primaryClass  = "cs.CL",
  arxivid       = "2402.00838"
}

@ARTICLE{Guo2024-qb,
  title         = "Vision Superalignment: {Weak-to-Strong} Generalization for
                   Vision Foundation Models",
  author        = "Guo, Jianyuan and Chen, Hanting and Wang, Chengcheng and
                   Han, Kai and Xu, Chang and Wang, Yunhe",
  abstract      = "Recent advancements in large language models have sparked
                   interest in their extraordinary and near-superhuman
                   capabilities, leading researchers to explore methods for
                   evaluating and optimizing these abilities, which is called
                   superalignment. In this context, our paper delves into the
                   realm of vision foundation models, focusing on the concept
                   of weak-to-strong generalization, which involves using a
                   weaker model to supervise a stronger one, aiming to enhance
                   the latter's capabilities beyond the former's limits. We
                   introduce a novel and adaptively adjustable loss function
                   for weak-to-strong supervision. Our comprehensive
                   experiments span various scenarios, including few-shot
                   learning, transfer learning, noisy label learning, and
                   common knowledge distillation settings. The results are
                   striking: our approach not only exceeds the performance
                   benchmarks set by strong-to-strong generalization but also
                   surpasses the outcomes of fine-tuning strong models with
                   whole datasets. This compelling evidence underscores the
                   significant potential of weak-to-strong generalization,
                   showcasing its capability to substantially elevate the
                   performance of vision foundation models. The code is
                   available at
                   https://github.com/ggjy/vision\_weak\_to\_strong.",
  month         =  feb,
  year          =  2024,
  url           = "http://arxiv.org/abs/2402.03749",
  archivePrefix = "arXiv",
  eprint        = "2402.03749",
  primaryClass  = "cs.CV",
  arxivid       = "2402.03749"
}

@ARTICLE{Gandhi2023-uk,
  title         = "{Distil-Whisper}: Robust Knowledge Distillation via
                   {Large-Scale} Pseudo Labelling",
  author        = "Gandhi, Sanchit and von Platen, Patrick and Rush, Alexander
                   M",
  abstract      = "As the size of pre-trained speech recognition models
                   increases, running these large models in low-latency or
                   resource-constrained environments becomes challenging. In
                   this work, we leverage pseudo-labelling to assemble a
                   large-scale open-source dataset which we use to distill the
                   Whisper model into a smaller variant, called Distil-Whisper.
                   Using a simple word error rate (WER) heuristic, we select
                   only the highest quality pseudo-labels for training. The
                   distilled model is 5.8 times faster with 51\% fewer
                   parameters, while performing to within 1\% WER on
                   out-of-distribution test data in a zero-shot transfer
                   setting. Distil-Whisper maintains the robustness of the
                   Whisper model to difficult acoustic conditions, while being
                   less prone to hallucination errors on long-form audio.
                   Distil-Whisper is designed to be paired with Whisper for
                   speculative decoding, yielding a 2 times speed-up while
                   mathematically ensuring the same outputs as the original
                   model. To facilitate further research in this domain, we
                   make our training code, inference code and models publicly
                   accessible.",
  month         =  nov,
  year          =  2023,
  url           = "http://arxiv.org/abs/2311.00430",
  keywords      = "WP3;WP4",
  archivePrefix = "arXiv",
  eprint        = "2311.00430",
  primaryClass  = "cs.CL",
  arxivid       = "2311.00430"
}

@ARTICLE{Nguyen2024-iu,
  title         = "{SpiRit-LM}: Interleaved Spoken and Written Language Model",
  author        = "Nguyen, Tu Anh and Muller, Benjamin and Yu, Bokai and
                   Costa-jussa, Marta R and Elbayad, Maha and Popuri, Sravya
                   and Duquenne, Paul-Ambroise and Algayres, Robin and
                   Mavlyutov, Ruslan and Gat, Itai and Synnaeve, Gabriel and
                   Pino, Juan and Sagot, Benoit and Dupoux, Emmanuel",
  abstract      = "We introduce SPIRIT-LM, a foundation multimodal language
                   model that freely mixes text and speech. Our model is based
                   on a pretrained text language model that we extend to the
                   speech modality by continuously training it on text and
                   speech units. Speech and text sequences are concatenated as
                   a single set of tokens, and trained with a word-level
                   interleaving method using a small automatically-curated
                   speech-text parallel corpus. SPIRIT-LM comes in two
                   versions: a BASE version that uses speech semantic units and
                   an EXPRESSIVE version that models expressivity using pitch
                   and style units in addition to the semantic units. For both
                   versions, the text is encoded with subword BPE tokens. The
                   resulting model displays both the semantic abilities of text
                   models and the expressive abilities of speech models.
                   Additionally, we demonstrate that SPIRIT-LM is able to learn
                   new tasks in a few-shot fashion across modalities (i.e. ASR,
                   TTS, Speech Classification).",
  month         =  feb,
  year          =  2024,
  url           = "http://arxiv.org/abs/2402.05755",
  keywords      = "Models;WP3;WP4",
  archivePrefix = "arXiv",
  eprint        = "2402.05755",
  primaryClass  = "cs.CL",
  arxivid       = "2402.05755"
}

@ARTICLE{Fan2024-vn,
  title         = "{MouSi}: {Poly-Visual-Expert} {Vision-Language} Models",
  author        = "Fan, Xiaoran and Ji, Tao and Jiang, Changhao and Li, Shuo
                   and Jin, Senjie and Song, Sirui and Wang, Junke and Hong,
                   Boyang and Chen, Lu and Zheng, Guodong and Zhang, Ming and
                   Huang, Caishuang and Zheng, Rui and Xi, Zhiheng and Zhou,
                   Yuhao and Dou, Shihan and Ye, Junjie and Yan, Hang and Gui,
                   Tao and Zhang, Qi and Qiu, Xipeng and Huang, Xuanjing and
                   Wu, Zuxuan and Jiang, Yu-Gang",
  abstract      = "Current large vision-language models (VLMs) often encounter
                   challenges such as insufficient capabilities of a single
                   visual component and excessively long visual tokens. These
                   issues can limit the model's effectiveness in accurately
                   interpreting complex visual information and over-lengthy
                   contextual information. Addressing these challenges is
                   crucial for enhancing the performance and applicability of
                   VLMs. This paper proposes the use of ensemble experts
                   technique to synergizes the capabilities of individual
                   visual encoders, including those skilled in image-text
                   matching, OCR, image segmentation, etc. This technique
                   introduces a fusion network to unify the processing of
                   outputs from different visual experts, while bridging the
                   gap between image encoders and pre-trained LLMs. In
                   addition, we explore different positional encoding schemes
                   to alleviate the waste of positional encoding caused by
                   lengthy image feature sequences, effectively addressing the
                   issue of position overflow and length limitations. For
                   instance, in our implementation, this technique
                   significantly reduces the positional occupancy in models
                   like SAM, from a substantial 4096 to a more efficient and
                   manageable 64 or even down to 1. Experimental results
                   demonstrate that VLMs with multiple experts exhibit
                   consistently superior performance over isolated visual
                   encoders and mark a significant performance boost as more
                   experts are integrated. We have open-sourced the training
                   code used in this report. All of these resources can be
                   found on our project website.",
  month         =  jan,
  year          =  2024,
  url           = "http://arxiv.org/abs/2401.17221",
  keywords      = "Models;WP3",
  archivePrefix = "arXiv",
  eprint        = "2401.17221",
  primaryClass  = "cs.CV",
  arxivid       = "2401.17221"
}

@ARTICLE{Lajszczak2024-vz,
  title         = "{BASE} {TTS}: Lessons from building a billion-parameter
                   {Text-to-Speech} model on {100K} hours of data",
  author        = "{\L}ajszczak, Mateusz and C{\'a}mbara, Guillermo and Li,
                   Yang and Beyhan, Fatih and van Korlaar, Arent and Yang, Fan
                   and Joly, Arnaud and Mart{\'\i}n-Cortinas, {\'A}lvaro and
                   Abbas, Ammar and Michalski, Adam and Moinet, Alexis and
                   Karlapati, Sri and Muszy{\'n}ska, Ewa and Guo, Haohan and
                   Putrycz, Bartosz and Gambino, Soledad L{\'o}pez and Yoo,
                   Kayeon and Sokolova, Elena and Drugman, Thomas",
  abstract      = "We introduce a text-to-speech (TTS) model called BASE TTS,
                   which stands for $\textbf\{B\}$ig $\textbf\{A\}$daptive
                   $\textbf\{S\}$treamable TTS with $\textbf\{E\}$mergent
                   abilities. BASE TTS is the largest TTS model to-date,
                   trained on 100K hours of public domain speech data,
                   achieving a new state-of-the-art in speech naturalness. It
                   deploys a 1-billion-parameter autoregressive Transformer
                   that converts raw texts into discrete codes
                   (``speechcodes'') followed by a convolution-based decoder
                   which converts these speechcodes into waveforms in an
                   incremental, streamable manner. Further, our speechcodes are
                   built using a novel speech tokenization technique that
                   features speaker ID disentanglement and compression with
                   byte-pair encoding. Echoing the widely-reported ``emergent
                   abilities'' of large language models when trained on
                   increasing volume of data, we show that BASE TTS variants
                   built with 10K+ hours and 500M+ parameters begin to
                   demonstrate natural prosody on textually complex sentences.
                   We design and share a specialized dataset to measure these
                   emergent abilities for text-to-speech. We showcase
                   state-of-the-art naturalness of BASE TTS by evaluating
                   against baselines that include publicly available
                   large-scale text-to-speech systems: YourTTS, Bark and
                   TortoiseTTS. Audio samples generated by the model can be
                   heard at https://amazon-ltts-paper.com/.",
  month         =  feb,
  year          =  2024,
  url           = "http://arxiv.org/abs/2402.08093",
  keywords      = "Models;WP3",
  archivePrefix = "arXiv",
  eprint        = "2402.08093",
  primaryClass  = "cs.LG",
  arxivid       = "2402.08093"
}

@ARTICLE{Ustun2024-gv,
  title         = "Aya Model: An Instruction Finetuned {Open-Access}
                   Multilingual Language Model",
  author        = "{\"U}st{\"u}n, Ahmet and Aryabumi, Viraat and Yong,
                   Zheng-Xin and Ko, Wei-Yin and D'souza, Daniel and Onilude,
                   Gbemileke and Bhandari, Neel and Singh, Shivalika and Ooi,
                   Hui-Lee and Kayid, Amr and Vargus, Freddie and Blunsom, Phil
                   and Longpre, Shayne and Muennighoff, Niklas and Fadaee,
                   Marzieh and Kreutzer, Julia and Hooker, Sara",
  abstract      = "Recent breakthroughs in large language models (LLMs) have
                   centered around a handful of data-rich languages. What does
                   it take to broaden access to breakthroughs beyond
                   first-class citizen languages? Our work introduces Aya, a
                   massively multilingual generative language model that
                   follows instructions in 101 languages of which over 50\% are
                   considered as lower-resourced. Aya outperforms mT0 and
                   BLOOMZ on the majority of tasks while covering double the
                   number of languages. We introduce extensive new evaluation
                   suites that broaden the state-of-art for multilingual eval
                   across 99 languages -- including discriminative and
                   generative tasks, human evaluation, and simulated win rates
                   that cover both held-out tasks and in-distribution
                   performance. Furthermore, we conduct detailed investigations
                   on the optimal finetuning mixture composition, data pruning,
                   as well as the toxicity, bias, and safety of our models. We
                   open-source our instruction datasets and our model at
                   https://hf.co/CohereForAI/aya-101",
  month         =  feb,
  year          =  2024,
  url           = "http://arxiv.org/abs/2402.07827",
  keywords      = "Models;WP3",
  archivePrefix = "arXiv",
  eprint        = "2402.07827",
  primaryClass  = "cs.CL",
  arxivid       = "2402.07827"
}

@ARTICLE{Kusupati2022-db,
  title         = "Matryoshka Representation Learning",
  author        = "Kusupati, Aditya and Bhatt, Gantavya and Rege, Aniket and
                   Wallingford, Matthew and Sinha, Aditya and Ramanujan, Vivek
                   and Howard-Snyder, William and Chen, Kaifeng and Kakade,
                   Sham and Jain, Prateek and Farhadi, Ali",
  abstract      = "Learned representations are a central component in modern ML
                   systems, serving a multitude of downstream tasks. When
                   training such representations, it is often the case that
                   computational and statistical constraints for each
                   downstream task are unknown. In this context rigid, fixed
                   capacity representations can be either over or
                   under-accommodating to the task at hand. This leads us to
                   ask: can we design a flexible representation that can adapt
                   to multiple downstream tasks with varying computational
                   resources? Our main contribution is Matryoshka
                   Representation Learning (MRL) which encodes information at
                   different granularities and allows a single embedding to
                   adapt to the computational constraints of downstream tasks.
                   MRL minimally modifies existing representation learning
                   pipelines and imposes no additional cost during inference
                   and deployment. MRL learns coarse-to-fine representations
                   that are at least as accurate and rich as independently
                   trained low-dimensional representations. The flexibility
                   within the learned Matryoshka Representations offer: (a) up
                   to 14x smaller embedding size for ImageNet-1K classification
                   at the same level of accuracy; (b) up to 14x real-world
                   speed-ups for large-scale retrieval on ImageNet-1K and 4K;
                   and (c) up to 2\% accuracy improvements for long-tail
                   few-shot classification, all while being as robust as the
                   original representations. Finally, we show that MRL extends
                   seamlessly to web-scale datasets (ImageNet, JFT) across
                   various modalities -- vision (ViT, ResNet), vision +
                   language (ALIGN) and language (BERT). MRL code and
                   pretrained models are open-sourced at
                   https://github.com/RAIVNLab/MRL.",
  month         =  may,
  year          =  2022,
  url           = "http://arxiv.org/abs/2205.13147",
  keywords      = "WP4",
  archivePrefix = "arXiv",
  eprint        = "2205.13147",
  primaryClass  = "cs.LG",
  arxivid       = "2205.13147"
}

@ARTICLE{Maaz2024-vd,
  title         = "{PALO}: A Polyglot Large Multimodal Model for {5B} People",
  author        = "Maaz, Muhammad and Rasheed, Hanoona and Shaker, Abdelrahman
                   and Khan, Salman and Cholakal, Hisham and Anwer, Rao M and
                   Baldwin, Tim and Felsberg, Michael and Khan, Fahad S",
  abstract      = "In pursuit of more inclusive Vision-Language Models (VLMs),
                   this study introduces a Large Multilingual Multimodal Model
                   called \textbackslashtextsc\{Palo\}.
                   \textbackslashtextsc\{Palo\} offers visual reasoning
                   capabilities in 10 major languages, including English,
                   Chinese, Hindi, Spanish, French, Arabic, Bengali, Russian,
                   Urdu, and Japanese, that span a total of $\sim$5B people
                   (65\% of the world population). Our approach involves a
                   semi-automated translation approach to adapt the multimodal
                   instruction dataset from English to the target languages
                   using a fine-tuned Large Language Model, thereby ensuring
                   high linguistic fidelity while allowing scalability due to
                   minimal manual effort. The incorporation of diverse
                   instruction sets helps us boost overall performance across
                   multiple languages especially those that are
                   underrepresented like Hindi, Arabic, Bengali, and Urdu. The
                   resulting models are trained across three scales (1.7B, 7B
                   and 13B parameters) to show the generalization and
                   scalability where we observe substantial improvements
                   compared to strong baselines. We also propose the first
                   multilingual multimodal benchmark for the forthcoming
                   approaches to evaluate their vision-language reasoning
                   capabilities across languages. Code:
                   https://github.com/mbzuai-oryx/PALO.",
  month         =  feb,
  year          =  2024,
  url           = "http://arxiv.org/abs/2402.14818",
  keywords      = "Models;WP3",
  archivePrefix = "arXiv",
  eprint        = "2402.14818",
  primaryClass  = "cs.CL",
  arxivid       = "2402.14818"
}

@ARTICLE{Zhan2024-aq,
  title         = "{AnyGPT}: Unified Multimodal {LLM} with Discrete Sequence
                   Modeling",
  author        = "Zhan, Jun and Dai, Junqi and Ye, Jiasheng and Zhou, Yunhua
                   and Zhang, Dong and Liu, Zhigeng and Zhang, Xin and Yuan,
                   Ruibin and Zhang, Ge and Li, Linyang and Yan, Hang and Fu,
                   Jie and Gui, Tao and Sun, Tianxiang and Jiang, Yugang and
                   Qiu, Xipeng",
  abstract      = "We introduce AnyGPT, an any-to-any multimodal language model
                   that utilizes discrete representations for the unified
                   processing of various modalities, including speech, text,
                   images, and music. AnyGPT can be trained stably without any
                   alterations to the current large language model (LLM)
                   architecture or training paradigms. Instead, it relies
                   exclusively on data-level preprocessing, facilitating the
                   seamless integration of new modalities into LLMs, akin to
                   the incorporation of new languages. We build a multimodal
                   text-centric dataset for multimodal alignment pre-training.
                   Utilizing generative models, we synthesize the first
                   large-scale any-to-any multimodal instruction dataset. It
                   consists of 108k samples of multi-turn conversations that
                   intricately interweave various modalities, thus equipping
                   the model to handle arbitrary combinations of multimodal
                   inputs and outputs. Experimental results demonstrate that
                   AnyGPT is capable of facilitating any-to-any multimodal
                   conversation while achieving performance comparable to
                   specialized models across all modalities, proving that
                   discrete representations can effectively and conveniently
                   unify multiple modalities within a language model. Demos are
                   shown in https://junzhan2000.github.io/AnyGPT.github.io/",
  month         =  feb,
  year          =  2024,
  url           = "http://arxiv.org/abs/2402.12226",
  keywords      = "Models;WP3",
  archivePrefix = "arXiv",
  eprint        = "2402.12226",
  primaryClass  = "cs.CL",
  arxivid       = "2402.12226"
}

@ARTICLE{Han2023-oa,
  title         = "{OneLLM}: One Framework to Align All Modalities with
                   Language",
  author        = "Han, Jiaming and Gong, Kaixiong and Zhang, Yiyuan and Wang,
                   Jiaqi and Zhang, Kaipeng and Lin, Dahua and Qiao, Yu and
                   Gao, Peng and Yue, Xiangyu",
  abstract      = "Multimodal large language models (MLLMs) have gained
                   significant attention due to their strong multimodal
                   understanding capability. However, existing works rely
                   heavily on modality-specific encoders, which usually differ
                   in architecture and are limited to common modalities. In
                   this paper, we present OneLLM, an MLLM that aligns eight
                   modalities to language using a unified framework. We achieve
                   this through a unified multimodal encoder and a progressive
                   multimodal alignment pipeline. In detail, we first train an
                   image projection module to connect a vision encoder with
                   LLM. Then, we build a universal projection module (UPM) by
                   mixing multiple image projection modules and dynamic
                   routing. Finally, we progressively align more modalities to
                   LLM with the UPM. To fully leverage the potential of OneLLM
                   in following instructions, we also curated a comprehensive
                   multimodal instruction dataset, including 2M items from
                   image, audio, video, point cloud, depth/normal map, IMU and
                   fMRI brain activity. OneLLM is evaluated on 25 diverse
                   benchmarks, encompassing tasks such as multimodal
                   captioning, question answering and reasoning, where it
                   delivers excellent performance. Code, data, model and online
                   demo are available at https://github.com/csuhan/OneLLM",
  month         =  dec,
  year          =  2023,
  url           = "http://arxiv.org/abs/2312.03700",
  keywords      = "WP3;Models",
  archivePrefix = "arXiv",
  eprint        = "2312.03700",
  primaryClass  = "cs.CV",
  arxivid       = "2312.03700"
}
