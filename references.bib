@MISC{Pandey2024-fy,
  title        = "{BharatGPT} Aims to Become Meta of Indic {LLMs}",
  booktitle    = "Analytics India Magazine",
  author       = "Pandey, Mohit",
  abstract     = "BharatGPT researchers know the power of open source and have
                  plans to release a bunch of models initially for the
                  developer community.",
  month        =  jan,
  year         =  2024,
  url          = "https://analyticsindiamag.com/bharatgpt-aims-to-become-meta-of-indic-llms/",
  howpublished = "\url{https://analyticsindiamag.com/bharatgpt-aims-to-become-meta-of-indic-llms/}",
  note         = "Accessed: 2024-2-27",
  language     = "en"
}

@ARTICLE{Wang2023-gz,
  title         = "Large-scale {Multi-Modal} Pre-trained Models: A
                   Comprehensive Survey",
  author        = "Wang, Xiao and Chen, Guangyao and Qian, Guangwu and Gao,
                   Pengcheng and Wei, Xiao-Yong and Wang, Yaowei and Tian,
                   Yonghong and Gao, Wen",
  abstract      = "With the urgent demand for generalized deep models, many
                   pre-trained big models are proposed, such as BERT, ViT, GPT,
                   etc. Inspired by the success of these models in single
                   domains (like computer vision and natural language
                   processing), the multi-modal pre-trained big models have
                   also drawn more and more attention in recent years. In this
                   work, we give a comprehensive survey of these models and
                   hope this paper could provide new insights and helps fresh
                   researchers to track the most cutting-edge works.
                   Specifically, we firstly introduce the background of
                   multi-modal pre-training by reviewing the conventional deep
                   learning, pre-training works in natural language process,
                   computer vision, and speech. Then, we introduce the task
                   definition, key challenges, and advantages of multi-modal
                   pre-training models (MM-PTMs), and discuss the MM-PTMs with
                   a focus on data, objectives, network architectures, and
                   knowledge enhanced pre-training. After that, we introduce
                   the downstream tasks used for the validation of large-scale
                   MM-PTMs, including generative, classification, and
                   regression tasks. We also give visualization and analysis of
                   the model parameters and results on representative
                   downstream tasks. Finally, we point out possible research
                   directions for this topic that may benefit future works. In
                   addition, we maintain a continuously updated paper list for
                   large-scale pre-trained multi-modal big models:
                   https://github.com/wangxiao5791509/MultiModal\_BigModels\_Survey",
  month         =  feb,
  year          =  2023,
  url           = "http://arxiv.org/abs/2302.10035",
  keywords      = "WP3",
  archivePrefix = "arXiv",
  eprint        = "2302.10035",
  primaryClass  = "cs.CV",
  arxivid       = "2302.10035"
}

@ARTICLE{Thompson2024-gz,
  title         = "A Shocking Amount of the Web is Machine Translated: Insights
                   from {Multi-Way} Parallelism",
  author        = "Thompson, Brian and Dhaliwal, Mehak Preet and Frisch, Peter
                   and Domhan, Tobias and Federico, Marcello",
  abstract      = "We show that content on the web is often translated into
                   many languages, and the low quality of these multi-way
                   translations indicates they were likely created using
                   Machine Translation (MT). Multi-way parallel, machine
                   generated content not only dominates the translations in
                   lower resource languages; it also constitutes a large
                   fraction of the total web content in those languages. We
                   also find evidence of a selection bias in the type of
                   content which is translated into many languages, consistent
                   with low quality English content being translated en masse
                   into many lower resource languages, via MT. Our work raises
                   serious concerns about training models such as multilingual
                   large language models on both monolingual and bilingual data
                   scraped from the web.",
  month         =  jan,
  year          =  2024,
  url           = "http://arxiv.org/abs/2401.05749",
  archivePrefix = "arXiv",
  eprint        = "2401.05749",
  primaryClass  = "cs.CL",
  arxivid       = "2401.05749"
}

@ARTICLE{Hassid2023-uv,
  title         = "Textually Pretrained Speech Language Models",
  author        = "Hassid, Michael and Remez, Tal and Nguyen, Tu Anh and Gat,
                   Itai and Conneau, Alexis and Kreuk, Felix and Copet, Jade
                   and Defossez, Alexandre and Synnaeve, Gabriel and Dupoux,
                   Emmanuel and Schwartz, Roy and Adi, Yossi",
  abstract      = "Speech language models (SpeechLMs) process and generate
                   acoustic data only, without textual supervision. In this
                   work, we propose TWIST, a method for training SpeechLMs
                   using a warm-start from a pretrained textual language
                   models. We show using both automatic and human evaluations
                   that TWIST outperforms a cold-start SpeechLM across the
                   board. We empirically analyze the effect of different model
                   design choices such as the speech tokenizer, the pretrained
                   textual model, and the dataset size. We find that model and
                   dataset scale both play an important role in constructing
                   better-performing SpeechLMs. Based on our observations, we
                   present the largest (to the best of our knowledge) SpeechLM
                   both in terms of number of parameters and training data. We
                   additionally introduce two spoken versions of the StoryCloze
                   textual benchmark to further improve model evaluation and
                   advance future research in the field. We make speech
                   samples, code and models publicly available:
                   https://pages.cs.huji.ac.il/adiyoss-lab/twist/ .",
  month         =  may,
  year          =  2023,
  url           = "http://arxiv.org/abs/2305.13009",
  keywords      = "WP3",
  archivePrefix = "arXiv",
  eprint        = "2305.13009",
  primaryClass  = "cs.CL",
  arxivid       = "2305.13009"
}

@ARTICLE{Huang2023-di,
  title         = "{AudioGPT}: Understanding and Generating Speech, Music,
                   Sound, and Talking Head",
  author        = "Huang, Rongjie and Li, Mingze and Yang, Dongchao and Shi,
                   Jiatong and Chang, Xuankai and Ye, Zhenhui and Wu, Yuning
                   and Hong, Zhiqing and Huang, Jiawei and Liu, Jinglin and
                   Ren, Yi and Zhao, Zhou and Watanabe, Shinji",
  abstract      = "Large language models (LLMs) have exhibited remarkable
                   capabilities across a variety of domains and tasks,
                   challenging our understanding of learning and cognition.
                   Despite the recent success, current LLMs are not capable of
                   processing complex audio information or conducting spoken
                   conversations (like Siri or Alexa). In this work, we propose
                   a multi-modal AI system named AudioGPT, which complements
                   LLMs (i.e., ChatGPT) with 1) foundation models to process
                   complex audio information and solve numerous understanding
                   and generation tasks; and 2) the input/output interface
                   (ASR, TTS) to support spoken dialogue. With an increasing
                   demand to evaluate multi-modal LLMs of human intention
                   understanding and cooperation with foundation models, we
                   outline the principles and processes and test AudioGPT in
                   terms of consistency, capability, and robustness.
                   Experimental results demonstrate the capabilities of
                   AudioGPT in solving AI tasks with speech, music, sound, and
                   talking head understanding and generation in multi-round
                   dialogues, which empower humans to create rich and diverse
                   audio content with unprecedented ease. Our system is
                   publicly available at
                   \textbackslashurl\{https://github.com/AIGC-Audio/AudioGPT\}.",
  month         =  apr,
  year          =  2023,
  url           = "http://arxiv.org/abs/2304.12995",
  keywords      = "Models;WP3",
  archivePrefix = "arXiv",
  eprint        = "2304.12995",
  primaryClass  = "cs.CL",
  arxivid       = "2304.12995"
}

@MISC{noauthor_undated-xw,
  title        = "Accelerating Generative {AI} with {PyTorch} {IV}: Seamless
                  {M4T}, fast",
  booktitle    = "{PyTorch}",
  abstract     = "This post is the fourth part of a multi-series blog focused
                  on how to accelerate generative AI models with pure, native
                  PyTorch. To skip to the code, check out our github
                  (seamless\_communication, fairseq2). We are excited to share
                  a breadth of newly released PyTorch performance features
                  alongside practical examples to see how far we can push
                  PyTorch native performance. In part one, we showed how to
                  accelerate Segment Anything over 8x using only pure, native
                  PyTorch. In part two, we showed how to accelerate Llama-7B by
                  almost 10x using only native PyTorch optimizations. In part
                  three, we showed how to accelerate text-to-image diffusion
                  models up to 3x using only native Pytorch optimizations.",
  url          = "https://pytorch.org/blog/accelerating-generative-ai-4/",
  howpublished = "\url{https://pytorch.org/blog/accelerating-generative-ai-4/}",
  note         = "Accessed: 2024-2-27",
  language     = "en"
}

@ARTICLE{Moschella2022-uh,
  title         = "Relative representations enable zero-shot latent space
                   communication",
  author        = "Moschella, Luca and Maiorca, Valentino and Fumero, Marco and
                   Norelli, Antonio and Locatello, Francesco and Rodol{\`a},
                   Emanuele",
  abstract      = "Neural networks embed the geometric structure of a data
                   manifold lying in a high-dimensional space into latent
                   representations. Ideally, the distribution of the data
                   points in the latent space should depend only on the task,
                   the data, the loss, and other architecture-specific
                   constraints. However, factors such as the random weights
                   initialization, training hyperparameters, or other sources
                   of randomness in the training phase may induce incoherent
                   latent spaces that hinder any form of reuse. Nevertheless,
                   we empirically observe that, under the same data and
                   modeling choices, the angles between the encodings within
                   distinct latent spaces do not change. In this work, we
                   propose the latent similarity between each sample and a
                   fixed set of anchors as an alternative data representation,
                   demonstrating that it can enforce the desired invariances
                   without any additional training. We show how neural
                   architectures can leverage these relative representations to
                   guarantee, in practice, invariance to latent isometries and
                   rescalings, effectively enabling latent space communication:
                   from zero-shot model stitching to latent space comparison
                   between diverse settings. We extensively validate the
                   generalization capability of our approach on different
                   datasets, spanning various modalities (images, text,
                   graphs), tasks (e.g., classification, reconstruction) and
                   architectures (e.g., CNNs, GCNs, transformers).",
  month         =  sep,
  year          =  2022,
  url           = "http://arxiv.org/abs/2209.15430",
  archivePrefix = "arXiv",
  eprint        = "2209.15430",
  primaryClass  = "cs.LG",
  arxivid       = "2209.15430"
}

@ARTICLE{Norelli2024-ui,
  title   = "{ASIF}: Coupled data turns unimodal models to multimodal without
             training",
  author  = "Norelli, Antonio and Fumero, Marco and Maiorca, Valentino and
             Moschella, Luca and Rodola, Emanuele and Locatello, Francesco",
  journal = "Adv. Neural Inf. Process. Syst.",
  volume  =  36,
  year    =  2024,
  url     = "https://proceedings.neurips.cc/paper_files/paper/2023/hash/3186591903d9db31770ad131adb5ceb4-Abstract-Conference.html",
  issn    = "1049-5258"
}

@ARTICLE{Zhang2024-eq,
  title         = "{MM-LLMs}: Recent Advances in {MultiModal} Large Language
                   Models",
  author        = "Zhang, Duzhen and Yu, Yahan and Li, Chenxing and Dong,
                   Jiahua and Su, Dan and Chu, Chenhui and Yu, Dong",
  abstract      = "In the past year, MultiModal Large Language Models (MM-LLMs)
                   have undergone substantial advancements, augmenting
                   off-the-shelf LLMs to support MM inputs or outputs via
                   cost-effective training strategies. The resulting models not
                   only preserve the inherent reasoning and decision-making
                   capabilities of LLMs but also empower a diverse range of MM
                   tasks. In this paper, we provide a comprehensive survey
                   aimed at facilitating further research of MM-LLMs.
                   Initially, we outline general design formulations for model
                   architecture and training pipeline. Subsequently, we
                   introduce a taxonomy encompassing $122$ MM-LLMs, each
                   characterized by its specific formulations. Furthermore, we
                   review the performance of selected MM-LLMs on mainstream
                   benchmarks and summarize key training recipes to enhance the
                   potency of MM-LLMs. Finally, we explore promising directions
                   for MM-LLMs while concurrently maintaining a real-time
                   tracking website for the latest developments in the field.
                   We hope that this survey contributes to the ongoing
                   advancement of the MM-LLMs domain.",
  month         =  jan,
  year          =  2024,
  url           = "http://arxiv.org/abs/2401.13601",
  keywords      = "WP3",
  archivePrefix = "arXiv",
  eprint        = "2401.13601",
  primaryClass  = "cs.CL",
  arxivid       = "2401.13601"
}

@ARTICLE{Groeneveld2024-id,
  title         = "{OLMo}: Accelerating the Science of Language Models",
  author        = "Groeneveld, Dirk and Beltagy, Iz and Walsh, Pete and Bhagia,
                   Akshita and Kinney, Rodney and Tafjord, Oyvind and Jha,
                   Ananya Harsh and Ivison, Hamish and Magnusson, Ian and Wang,
                   Yizhong and Arora, Shane and Atkinson, David and Authur,
                   Russell and Chandu, Khyathi Raghavi and Cohan, Arman and
                   Dumas, Jennifer and Elazar, Yanai and Gu, Yuling and Hessel,
                   Jack and Khot, Tushar and Merrill, William and Morrison,
                   Jacob and Muennighoff, Niklas and Naik, Aakanksha and Nam,
                   Crystal and Peters, Matthew E and Pyatkin, Valentina and
                   Ravichander, Abhilasha and Schwenk, Dustin and Shah, Saurabh
                   and Smith, Will and Strubell, Emma and Subramani, Nishant
                   and Wortsman, Mitchell and Dasigi, Pradeep and Lambert,
                   Nathan and Richardson, Kyle and Zettlemoyer, Luke and Dodge,
                   Jesse and Lo, Kyle and Soldaini, Luca and Smith, Noah A and
                   Hajishirzi, Hannaneh",
  abstract      = "Language models (LMs) have become ubiquitous in both NLP
                   research and in commercial product offerings. As their
                   commercial importance has surged, the most powerful models
                   have become closed off, gated behind proprietary interfaces,
                   with important details of their training data,
                   architectures, and development undisclosed. Given the
                   importance of these details in scientifically studying these
                   models, including their biases and potential risks, we
                   believe it is essential for the research community to have
                   access to powerful, truly open LMs. To this end, this
                   technical report details the first release of OLMo, a
                   state-of-the-art, truly Open Language Model and its
                   framework to build and study the science of language
                   modeling. Unlike most prior efforts that have only released
                   model weights and inference code, we release OLMo and the
                   whole framework, including training data and training and
                   evaluation code. We hope this release will empower and
                   strengthen the open research community and inspire a new
                   wave of innovation.",
  month         =  feb,
  year          =  2024,
  url           = "http://arxiv.org/abs/2402.00838",
  keywords      = "WP3",
  archivePrefix = "arXiv",
  eprint        = "2402.00838",
  primaryClass  = "cs.CL",
  arxivid       = "2402.00838"
}
