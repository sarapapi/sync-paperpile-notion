@ARTICLE{Wang2023-fe,
  title         = "Large-scale {Multi-Modal} Pre-trained Models: A
                   Comprehensive Survey",
  author        = "Wang, Xiao and Chen, Guangyao and Qian, Guangwu and Gao,
                   Pengcheng and Wei, Xiao-Yong and Wang, Yaowei and Tian,
                   Yonghong and Gao, Wen",
  abstract      = "With the urgent demand for generalized deep models, many
                   pre-trained big models are proposed, such as BERT, ViT, GPT,
                   etc. Inspired by the success of these models in single
                   domains (like computer vision and natural language
                   processing), the multi-modal pre-trained big models have
                   also drawn more and more attention in recent years. In this
                   work, we give a comprehensive survey of these models and
                   hope this paper could provide new insights and helps fresh
                   researchers to track the most cutting-edge works.
                   Specifically, we firstly introduce the background of
                   multi-modal pre-training by reviewing the conventional deep
                   learning, pre-training works in natural language process,
                   computer vision, and speech. Then, we introduce the task
                   definition, key challenges, and advantages of multi-modal
                   pre-training models (MM-PTMs), and discuss the MM-PTMs with
                   a focus on data, objectives, network architectures, and
                   knowledge enhanced pre-training. After that, we introduce
                   the downstream tasks used for the validation of large-scale
                   MM-PTMs, including generative, classification, and
                   regression tasks. We also give visualization and analysis of
                   the model parameters and results on representative
                   downstream tasks. Finally, we point out possible research
                   directions for this topic that may benefit future works. In
                   addition, we maintain a continuously updated paper list for
                   large-scale pre-trained multi-modal big models:
                   https://github.com/wangxiao5791509/MultiModal\_BigModels\_Survey",
  month         =  feb,
  year          =  2023,
  keywords      = "Model",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2302.10035"
}

@ARTICLE{Kusupati2022-nx,
  title         = "Matryoshka Representation Learning",
  author        = "Kusupati, Aditya and Bhatt, Gantavya and Rege, Aniket and
                   Wallingford, Matthew and Sinha, Aditya and Ramanujan, Vivek
                   and Howard-Snyder, William and Chen, Kaifeng and Kakade,
                   Sham and Jain, Prateek and Farhadi, Ali",
  abstract      = "Learned representations are a central component in modern ML
                   systems, serving a multitude of downstream tasks. When
                   training such representations, it is often the case that
                   computational and statistical constraints for each
                   downstream task are unknown. In this context rigid, fixed
                   capacity representations can be either over or
                   under-accommodating to the task at hand. This leads us to
                   ask: can we design a flexible representation that can adapt
                   to multiple downstream tasks with varying computational
                   resources? Our main contribution is Matryoshka
                   Representation Learning (MRL) which encodes information at
                   different granularities and allows a single embedding to
                   adapt to the computational constraints of downstream tasks.
                   MRL minimally modifies existing representation learning
                   pipelines and imposes no additional cost during inference
                   and deployment. MRL learns coarse-to-fine representations
                   that are at least as accurate and rich as independently
                   trained low-dimensional representations. The flexibility
                   within the learned Matryoshka Representations offer: (a) up
                   to 14x smaller embedding size for ImageNet-1K classification
                   at the same level of accuracy; (b) up to 14x real-world
                   speed-ups for large-scale retrieval on ImageNet-1K and 4K;
                   and (c) up to 2\% accuracy improvements for long-tail
                   few-shot classification, all while being as robust as the
                   original representations. Finally, we show that MRL extends
                   seamlessly to web-scale datasets (ImageNet, JFT) across
                   various modalities -- vision (ViT, ResNet), vision +
                   language (ALIGN) and language (BERT). MRL code and
                   pretrained models are open-sourced at
                   https://github.com/RAIVNLab/MRL.",
  month         =  may,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2205.13147"
}
